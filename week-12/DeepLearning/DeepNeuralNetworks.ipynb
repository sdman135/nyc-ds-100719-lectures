{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting Off\n",
    "\n",
    "How does sklearn utilize numpy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to build a Deep Neural Network with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model building steps:\n",
    "\n",
    "1. Specify Architecture\n",
    "\n",
    "2. Compile\n",
    "\n",
    "3. Fit \n",
    "\n",
    "4. Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify the architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling a model \n",
    "\n",
    "- Specify the optimizer\n",
    "    - Many options and mathematically complex\n",
    "    - “Adam” is usually a good choice \n",
    "- Loss function\n",
    "    - “mean_squared_error” common for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-03T18:56:07.550315Z",
     "start_time": "2020-01-03T18:56:07.532129Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"#instantiate the model\\nmodel = Sequential()\\n#create a hidden layer and the input layer\\nmodel.add(Dense(100, activation='relu', input_shape = (n_cols,)))\\n#add one hidden layer\\nmodel.add(Dense(100, activation='relu'))\\n#add the final layer\\nmodel.add(Dense(1))\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''#instantiate the model\n",
    "model = Sequential()\n",
    "#create a hidden layer and the input layer\n",
    "model.add(Dense(100, activation='relu', input_shape = (n_cols,)))\n",
    "#add one hidden layer\n",
    "model.add(Dense(100, activation='relu'))\n",
    "#add the final layer\n",
    "model.add(Dense(1))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-03T18:56:07.562066Z",
     "start_time": "2020-01-03T18:56:07.553907Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"model.compile(optimizer='adam', loss='mean_squared_error')\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''model.compile(optimizer='adam', loss='mean_squared_error')'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting a model\n",
    "\n",
    "- Applying backpropagation and gradient descent with your data to update the weights\n",
    "- Scaling data before fi!ing can ease optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-03T18:56:07.571734Z",
     "start_time": "2020-01-03T18:56:07.564877Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model.fit(features, # Features\\n                      target, # Target\\n                      epochs=15, # Number of epochs\\n                      verbose=2, # Some output\\n                      batch_size=100, # Number of observations per batch\\n                      validation_data=(X_test, y_test)) # Data for evaluation'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train neural network\n",
    "'''model.fit(features, # Features\n",
    "                      target, # Target\n",
    "                      epochs=15, # Number of epochs\n",
    "                      verbose=2, # Some output\n",
    "                      batch_size=100, # Number of observations per batch\n",
    "                      validation_data=(X_test, y_test)) # Data for evaluation'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-03T18:56:07.580647Z",
     "start_time": "2020-01-03T18:56:07.574903Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model.predict(features)'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''model.predict(features)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applied:Create a Regression NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-03T18:56:11.371289Z",
     "start_time": "2020-01-03T18:56:07.584559Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-03T18:56:17.049693Z",
     "start_time": "2020-01-03T18:56:11.374316Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/learn-co-students/nyc-mhtn-ds-042219-lectures/master/Module_4/kc_feat_engineering_project_revamp/kc_housing_data_for_feat_engineering_lab.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-03T18:56:17.226578Z",
     "start_time": "2020-01-03T18:56:17.052301Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>condition</th>\n",
       "      <th>...</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "      <th>yr_old</th>\n",
       "      <th>year_sold</th>\n",
       "      <th>since_sold</th>\n",
       "      <th>price_log</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7129300520</th>\n",
       "      <td>2014-10-13</td>\n",
       "      <td>221900.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1180</td>\n",
       "      <td>5650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>98178</td>\n",
       "      <td>47.5112</td>\n",
       "      <td>-122.257</td>\n",
       "      <td>1340</td>\n",
       "      <td>5650</td>\n",
       "      <td>62</td>\n",
       "      <td>2014</td>\n",
       "      <td>3</td>\n",
       "      <td>12.309982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6414100192</th>\n",
       "      <td>2014-12-09</td>\n",
       "      <td>538000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2570</td>\n",
       "      <td>7242</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1991</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.7210</td>\n",
       "      <td>-122.319</td>\n",
       "      <td>1690</td>\n",
       "      <td>7639</td>\n",
       "      <td>66</td>\n",
       "      <td>2014</td>\n",
       "      <td>3</td>\n",
       "      <td>13.195614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5631500400</th>\n",
       "      <td>2015-02-25</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>770</td>\n",
       "      <td>10000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>98028</td>\n",
       "      <td>47.7379</td>\n",
       "      <td>-122.233</td>\n",
       "      <td>2720</td>\n",
       "      <td>8062</td>\n",
       "      <td>84</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>12.100712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2487200875</th>\n",
       "      <td>2014-12-09</td>\n",
       "      <td>604000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1960</td>\n",
       "      <td>5000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>98136</td>\n",
       "      <td>47.5208</td>\n",
       "      <td>-122.393</td>\n",
       "      <td>1360</td>\n",
       "      <td>5000</td>\n",
       "      <td>52</td>\n",
       "      <td>2014</td>\n",
       "      <td>3</td>\n",
       "      <td>13.311329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1954400510</th>\n",
       "      <td>2015-02-18</td>\n",
       "      <td>510000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1680</td>\n",
       "      <td>8080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6168</td>\n",
       "      <td>-122.045</td>\n",
       "      <td>1800</td>\n",
       "      <td>7503</td>\n",
       "      <td>30</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>13.142166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7237550310</th>\n",
       "      <td>2014-05-12</td>\n",
       "      <td>1225000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.50</td>\n",
       "      <td>5420</td>\n",
       "      <td>101930</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>98053</td>\n",
       "      <td>47.6561</td>\n",
       "      <td>-122.005</td>\n",
       "      <td>4760</td>\n",
       "      <td>101930</td>\n",
       "      <td>16</td>\n",
       "      <td>2014</td>\n",
       "      <td>3</td>\n",
       "      <td>14.018451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1321400060</th>\n",
       "      <td>2014-06-27</td>\n",
       "      <td>257500.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>1715</td>\n",
       "      <td>6819</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>98003</td>\n",
       "      <td>47.3097</td>\n",
       "      <td>-122.327</td>\n",
       "      <td>2238</td>\n",
       "      <td>6819</td>\n",
       "      <td>22</td>\n",
       "      <td>2014</td>\n",
       "      <td>3</td>\n",
       "      <td>12.458775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008000270</th>\n",
       "      <td>2015-01-15</td>\n",
       "      <td>291850.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1060</td>\n",
       "      <td>9711</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>98198</td>\n",
       "      <td>47.4095</td>\n",
       "      <td>-122.315</td>\n",
       "      <td>1650</td>\n",
       "      <td>9711</td>\n",
       "      <td>54</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>12.583995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2414600126</th>\n",
       "      <td>2015-04-15</td>\n",
       "      <td>229500.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1780</td>\n",
       "      <td>7470</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>98146</td>\n",
       "      <td>47.5123</td>\n",
       "      <td>-122.337</td>\n",
       "      <td>1780</td>\n",
       "      <td>8113</td>\n",
       "      <td>57</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>12.343658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3793500160</th>\n",
       "      <td>2015-03-12</td>\n",
       "      <td>323000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1890</td>\n",
       "      <td>6560</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>98038</td>\n",
       "      <td>47.3684</td>\n",
       "      <td>-122.031</td>\n",
       "      <td>2390</td>\n",
       "      <td>7570</td>\n",
       "      <td>14</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>12.685408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1736800520</th>\n",
       "      <td>2015-04-03</td>\n",
       "      <td>662500.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>3560</td>\n",
       "      <td>9796</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>98007</td>\n",
       "      <td>47.6007</td>\n",
       "      <td>-122.145</td>\n",
       "      <td>2210</td>\n",
       "      <td>8925</td>\n",
       "      <td>52</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>13.403776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9212900260</th>\n",
       "      <td>2014-05-27</td>\n",
       "      <td>468000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1160</td>\n",
       "      <td>6000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>98115</td>\n",
       "      <td>47.6900</td>\n",
       "      <td>-122.292</td>\n",
       "      <td>1330</td>\n",
       "      <td>6000</td>\n",
       "      <td>75</td>\n",
       "      <td>2014</td>\n",
       "      <td>3</td>\n",
       "      <td>13.056224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114101516</th>\n",
       "      <td>2014-05-28</td>\n",
       "      <td>310000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1430</td>\n",
       "      <td>19901</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>98028</td>\n",
       "      <td>47.7558</td>\n",
       "      <td>-122.229</td>\n",
       "      <td>1780</td>\n",
       "      <td>12697</td>\n",
       "      <td>90</td>\n",
       "      <td>2014</td>\n",
       "      <td>3</td>\n",
       "      <td>12.644328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6054650070</th>\n",
       "      <td>2014-10-07</td>\n",
       "      <td>400000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1370</td>\n",
       "      <td>9680</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6127</td>\n",
       "      <td>-122.045</td>\n",
       "      <td>1370</td>\n",
       "      <td>10208</td>\n",
       "      <td>40</td>\n",
       "      <td>2014</td>\n",
       "      <td>3</td>\n",
       "      <td>12.899220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1175000570</th>\n",
       "      <td>2015-03-12</td>\n",
       "      <td>530000.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1810</td>\n",
       "      <td>4850</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>98107</td>\n",
       "      <td>47.6700</td>\n",
       "      <td>-122.394</td>\n",
       "      <td>1360</td>\n",
       "      <td>4850</td>\n",
       "      <td>117</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>13.180632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9297300055</th>\n",
       "      <td>2015-01-24</td>\n",
       "      <td>650000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>2950</td>\n",
       "      <td>5000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>98126</td>\n",
       "      <td>47.5714</td>\n",
       "      <td>-122.375</td>\n",
       "      <td>2140</td>\n",
       "      <td>4000</td>\n",
       "      <td>38</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>13.384728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1875500060</th>\n",
       "      <td>2014-07-31</td>\n",
       "      <td>395000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1890</td>\n",
       "      <td>14040</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>98019</td>\n",
       "      <td>47.7277</td>\n",
       "      <td>-121.962</td>\n",
       "      <td>1890</td>\n",
       "      <td>14018</td>\n",
       "      <td>23</td>\n",
       "      <td>2014</td>\n",
       "      <td>3</td>\n",
       "      <td>12.886641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6865200140</th>\n",
       "      <td>2014-05-29</td>\n",
       "      <td>485000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1600</td>\n",
       "      <td>4300</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>98103</td>\n",
       "      <td>47.6648</td>\n",
       "      <td>-122.343</td>\n",
       "      <td>1610</td>\n",
       "      <td>4300</td>\n",
       "      <td>101</td>\n",
       "      <td>2014</td>\n",
       "      <td>3</td>\n",
       "      <td>13.091904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16000397</th>\n",
       "      <td>2014-12-05</td>\n",
       "      <td>189000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1200</td>\n",
       "      <td>9850</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>98002</td>\n",
       "      <td>47.3089</td>\n",
       "      <td>-122.210</td>\n",
       "      <td>1060</td>\n",
       "      <td>5095</td>\n",
       "      <td>96</td>\n",
       "      <td>2014</td>\n",
       "      <td>3</td>\n",
       "      <td>12.149502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7983200060</th>\n",
       "      <td>2015-04-24</td>\n",
       "      <td>230000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1250</td>\n",
       "      <td>9774</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>98003</td>\n",
       "      <td>47.3343</td>\n",
       "      <td>-122.306</td>\n",
       "      <td>1280</td>\n",
       "      <td>8850</td>\n",
       "      <td>48</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>12.345835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6300500875</th>\n",
       "      <td>2014-05-14</td>\n",
       "      <td>385000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1620</td>\n",
       "      <td>4980</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>98133</td>\n",
       "      <td>47.7025</td>\n",
       "      <td>-122.341</td>\n",
       "      <td>1400</td>\n",
       "      <td>4980</td>\n",
       "      <td>70</td>\n",
       "      <td>2014</td>\n",
       "      <td>3</td>\n",
       "      <td>12.860999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2524049179</th>\n",
       "      <td>2014-08-26</td>\n",
       "      <td>2000000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.75</td>\n",
       "      <td>3050</td>\n",
       "      <td>44867</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>98040</td>\n",
       "      <td>47.5316</td>\n",
       "      <td>-122.233</td>\n",
       "      <td>4110</td>\n",
       "      <td>20336</td>\n",
       "      <td>49</td>\n",
       "      <td>2014</td>\n",
       "      <td>3</td>\n",
       "      <td>14.508658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7137970340</th>\n",
       "      <td>2014-07-03</td>\n",
       "      <td>285000.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2270</td>\n",
       "      <td>6300</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>98092</td>\n",
       "      <td>47.3266</td>\n",
       "      <td>-122.169</td>\n",
       "      <td>2240</td>\n",
       "      <td>7005</td>\n",
       "      <td>22</td>\n",
       "      <td>2014</td>\n",
       "      <td>3</td>\n",
       "      <td>12.560244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8091400200</th>\n",
       "      <td>2014-05-16</td>\n",
       "      <td>252700.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1070</td>\n",
       "      <td>9643</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>98030</td>\n",
       "      <td>47.3533</td>\n",
       "      <td>-122.166</td>\n",
       "      <td>1220</td>\n",
       "      <td>8386</td>\n",
       "      <td>32</td>\n",
       "      <td>2014</td>\n",
       "      <td>3</td>\n",
       "      <td>12.439958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3814700200</th>\n",
       "      <td>2014-11-20</td>\n",
       "      <td>329000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2450</td>\n",
       "      <td>6500</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>98030</td>\n",
       "      <td>47.3739</td>\n",
       "      <td>-122.172</td>\n",
       "      <td>2200</td>\n",
       "      <td>6865</td>\n",
       "      <td>32</td>\n",
       "      <td>2014</td>\n",
       "      <td>3</td>\n",
       "      <td>12.703813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1202000200</th>\n",
       "      <td>2014-11-03</td>\n",
       "      <td>233000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1710</td>\n",
       "      <td>4697</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>98002</td>\n",
       "      <td>47.3048</td>\n",
       "      <td>-122.218</td>\n",
       "      <td>1030</td>\n",
       "      <td>4705</td>\n",
       "      <td>76</td>\n",
       "      <td>2014</td>\n",
       "      <td>3</td>\n",
       "      <td>12.358794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1794500383</th>\n",
       "      <td>2014-06-26</td>\n",
       "      <td>937000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.75</td>\n",
       "      <td>2450</td>\n",
       "      <td>2691</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>98119</td>\n",
       "      <td>47.6386</td>\n",
       "      <td>-122.360</td>\n",
       "      <td>1760</td>\n",
       "      <td>3573</td>\n",
       "      <td>102</td>\n",
       "      <td>2014</td>\n",
       "      <td>3</td>\n",
       "      <td>13.750439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3303700376</th>\n",
       "      <td>2014-12-01</td>\n",
       "      <td>667000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1400</td>\n",
       "      <td>1581</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>98112</td>\n",
       "      <td>47.6221</td>\n",
       "      <td>-122.314</td>\n",
       "      <td>1860</td>\n",
       "      <td>3861</td>\n",
       "      <td>108</td>\n",
       "      <td>2014</td>\n",
       "      <td>3</td>\n",
       "      <td>13.410545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5101402488</th>\n",
       "      <td>2014-06-24</td>\n",
       "      <td>438000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1520</td>\n",
       "      <td>6380</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>98115</td>\n",
       "      <td>47.6950</td>\n",
       "      <td>-122.304</td>\n",
       "      <td>1520</td>\n",
       "      <td>6235</td>\n",
       "      <td>69</td>\n",
       "      <td>2014</td>\n",
       "      <td>3</td>\n",
       "      <td>12.989974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1873100390</th>\n",
       "      <td>2015-03-02</td>\n",
       "      <td>719000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2570</td>\n",
       "      <td>7173</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>98052</td>\n",
       "      <td>47.7073</td>\n",
       "      <td>-122.110</td>\n",
       "      <td>2630</td>\n",
       "      <td>6026</td>\n",
       "      <td>12</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>13.485617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025049203</th>\n",
       "      <td>2014-06-10</td>\n",
       "      <td>399950.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>710</td>\n",
       "      <td>1157</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>98102</td>\n",
       "      <td>47.6413</td>\n",
       "      <td>-122.329</td>\n",
       "      <td>1370</td>\n",
       "      <td>1173</td>\n",
       "      <td>74</td>\n",
       "      <td>2014</td>\n",
       "      <td>3</td>\n",
       "      <td>12.899095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952006823</th>\n",
       "      <td>2014-12-02</td>\n",
       "      <td>380000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1260</td>\n",
       "      <td>900</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>98116</td>\n",
       "      <td>47.5621</td>\n",
       "      <td>-122.384</td>\n",
       "      <td>1310</td>\n",
       "      <td>1415</td>\n",
       "      <td>10</td>\n",
       "      <td>2014</td>\n",
       "      <td>3</td>\n",
       "      <td>12.847927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3832050760</th>\n",
       "      <td>2014-08-28</td>\n",
       "      <td>270000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1870</td>\n",
       "      <td>5000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>98042</td>\n",
       "      <td>47.3339</td>\n",
       "      <td>-122.055</td>\n",
       "      <td>2170</td>\n",
       "      <td>5399</td>\n",
       "      <td>8</td>\n",
       "      <td>2014</td>\n",
       "      <td>3</td>\n",
       "      <td>12.506177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2767604724</th>\n",
       "      <td>2014-10-15</td>\n",
       "      <td>505000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1430</td>\n",
       "      <td>1201</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>98107</td>\n",
       "      <td>47.6707</td>\n",
       "      <td>-122.381</td>\n",
       "      <td>1430</td>\n",
       "      <td>1249</td>\n",
       "      <td>8</td>\n",
       "      <td>2014</td>\n",
       "      <td>3</td>\n",
       "      <td>13.132314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6632300207</th>\n",
       "      <td>2015-03-05</td>\n",
       "      <td>385000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1520</td>\n",
       "      <td>1488</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.7337</td>\n",
       "      <td>-122.309</td>\n",
       "      <td>1520</td>\n",
       "      <td>1497</td>\n",
       "      <td>11</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>12.860999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2767600688</th>\n",
       "      <td>2014-11-13</td>\n",
       "      <td>414500.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1210</td>\n",
       "      <td>1278</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>98117</td>\n",
       "      <td>47.6756</td>\n",
       "      <td>-122.375</td>\n",
       "      <td>1210</td>\n",
       "      <td>1118</td>\n",
       "      <td>10</td>\n",
       "      <td>2014</td>\n",
       "      <td>3</td>\n",
       "      <td>12.934828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7570050450</th>\n",
       "      <td>2014-09-10</td>\n",
       "      <td>347500.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2540</td>\n",
       "      <td>4760</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>98038</td>\n",
       "      <td>47.3452</td>\n",
       "      <td>-122.022</td>\n",
       "      <td>2540</td>\n",
       "      <td>4571</td>\n",
       "      <td>7</td>\n",
       "      <td>2014</td>\n",
       "      <td>3</td>\n",
       "      <td>12.758520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7430200100</th>\n",
       "      <td>2014-05-14</td>\n",
       "      <td>1222500.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.50</td>\n",
       "      <td>4910</td>\n",
       "      <td>9444</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6502</td>\n",
       "      <td>-122.066</td>\n",
       "      <td>4560</td>\n",
       "      <td>11063</td>\n",
       "      <td>10</td>\n",
       "      <td>2014</td>\n",
       "      <td>3</td>\n",
       "      <td>14.016409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4140940150</th>\n",
       "      <td>2014-10-02</td>\n",
       "      <td>572000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.75</td>\n",
       "      <td>2770</td>\n",
       "      <td>3852</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>98178</td>\n",
       "      <td>47.5001</td>\n",
       "      <td>-122.232</td>\n",
       "      <td>1810</td>\n",
       "      <td>5641</td>\n",
       "      <td>3</td>\n",
       "      <td>2014</td>\n",
       "      <td>3</td>\n",
       "      <td>13.256894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1931300412</th>\n",
       "      <td>2015-04-16</td>\n",
       "      <td>475000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>1190</td>\n",
       "      <td>1200</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>98103</td>\n",
       "      <td>47.6542</td>\n",
       "      <td>-122.346</td>\n",
       "      <td>1180</td>\n",
       "      <td>1224</td>\n",
       "      <td>9</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>13.071070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8672200110</th>\n",
       "      <td>2015-03-17</td>\n",
       "      <td>1088000.0</td>\n",
       "      <td>5</td>\n",
       "      <td>3.75</td>\n",
       "      <td>4170</td>\n",
       "      <td>8142</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>98056</td>\n",
       "      <td>47.5354</td>\n",
       "      <td>-122.181</td>\n",
       "      <td>3030</td>\n",
       "      <td>7980</td>\n",
       "      <td>11</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>13.899852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5087900040</th>\n",
       "      <td>2014-10-17</td>\n",
       "      <td>350000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.75</td>\n",
       "      <td>2500</td>\n",
       "      <td>5995</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>98042</td>\n",
       "      <td>47.3749</td>\n",
       "      <td>-122.107</td>\n",
       "      <td>2530</td>\n",
       "      <td>5988</td>\n",
       "      <td>9</td>\n",
       "      <td>2014</td>\n",
       "      <td>3</td>\n",
       "      <td>12.765688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1972201967</th>\n",
       "      <td>2014-10-31</td>\n",
       "      <td>520000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.25</td>\n",
       "      <td>1530</td>\n",
       "      <td>981</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>98103</td>\n",
       "      <td>47.6533</td>\n",
       "      <td>-122.346</td>\n",
       "      <td>1530</td>\n",
       "      <td>1282</td>\n",
       "      <td>11</td>\n",
       "      <td>2014</td>\n",
       "      <td>3</td>\n",
       "      <td>13.161584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7502800100</th>\n",
       "      <td>2014-08-13</td>\n",
       "      <td>679950.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2.75</td>\n",
       "      <td>3600</td>\n",
       "      <td>9437</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>98059</td>\n",
       "      <td>47.4822</td>\n",
       "      <td>-122.131</td>\n",
       "      <td>3550</td>\n",
       "      <td>9421</td>\n",
       "      <td>3</td>\n",
       "      <td>2014</td>\n",
       "      <td>3</td>\n",
       "      <td>13.429775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191100405</th>\n",
       "      <td>2015-04-21</td>\n",
       "      <td>1575000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.25</td>\n",
       "      <td>3410</td>\n",
       "      <td>10125</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>98040</td>\n",
       "      <td>47.5653</td>\n",
       "      <td>-122.223</td>\n",
       "      <td>2290</td>\n",
       "      <td>10125</td>\n",
       "      <td>10</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>14.269766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8956200760</th>\n",
       "      <td>2014-10-13</td>\n",
       "      <td>541800.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.50</td>\n",
       "      <td>3118</td>\n",
       "      <td>7866</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>98001</td>\n",
       "      <td>47.2931</td>\n",
       "      <td>-122.264</td>\n",
       "      <td>2673</td>\n",
       "      <td>6500</td>\n",
       "      <td>3</td>\n",
       "      <td>2014</td>\n",
       "      <td>3</td>\n",
       "      <td>13.202652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7202300110</th>\n",
       "      <td>2014-09-15</td>\n",
       "      <td>810000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3990</td>\n",
       "      <td>7838</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>98053</td>\n",
       "      <td>47.6857</td>\n",
       "      <td>-122.046</td>\n",
       "      <td>3370</td>\n",
       "      <td>6814</td>\n",
       "      <td>14</td>\n",
       "      <td>2014</td>\n",
       "      <td>3</td>\n",
       "      <td>13.604790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249000205</th>\n",
       "      <td>2014-10-15</td>\n",
       "      <td>1537000.0</td>\n",
       "      <td>5</td>\n",
       "      <td>3.75</td>\n",
       "      <td>4470</td>\n",
       "      <td>8088</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>98004</td>\n",
       "      <td>47.6321</td>\n",
       "      <td>-122.200</td>\n",
       "      <td>2780</td>\n",
       "      <td>8964</td>\n",
       "      <td>9</td>\n",
       "      <td>2014</td>\n",
       "      <td>3</td>\n",
       "      <td>14.245343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5100403806</th>\n",
       "      <td>2015-04-07</td>\n",
       "      <td>467000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1425</td>\n",
       "      <td>1179</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.6963</td>\n",
       "      <td>-122.318</td>\n",
       "      <td>1285</td>\n",
       "      <td>1253</td>\n",
       "      <td>9</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>13.054085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>844000965</th>\n",
       "      <td>2014-06-26</td>\n",
       "      <td>224000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1500</td>\n",
       "      <td>11968</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>98010</td>\n",
       "      <td>47.3095</td>\n",
       "      <td>-122.002</td>\n",
       "      <td>1320</td>\n",
       "      <td>11303</td>\n",
       "      <td>3</td>\n",
       "      <td>2014</td>\n",
       "      <td>3</td>\n",
       "      <td>12.319401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7852140040</th>\n",
       "      <td>2014-08-25</td>\n",
       "      <td>507250.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2270</td>\n",
       "      <td>5536</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>98065</td>\n",
       "      <td>47.5389</td>\n",
       "      <td>-121.881</td>\n",
       "      <td>2270</td>\n",
       "      <td>5731</td>\n",
       "      <td>14</td>\n",
       "      <td>2014</td>\n",
       "      <td>3</td>\n",
       "      <td>13.136759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9834201367</th>\n",
       "      <td>2015-01-26</td>\n",
       "      <td>429000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1490</td>\n",
       "      <td>1126</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>98144</td>\n",
       "      <td>47.5699</td>\n",
       "      <td>-122.288</td>\n",
       "      <td>1400</td>\n",
       "      <td>1230</td>\n",
       "      <td>3</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>12.969212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3448900210</th>\n",
       "      <td>2014-10-14</td>\n",
       "      <td>610685.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2520</td>\n",
       "      <td>6023</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>98056</td>\n",
       "      <td>47.5137</td>\n",
       "      <td>-122.167</td>\n",
       "      <td>2520</td>\n",
       "      <td>6023</td>\n",
       "      <td>3</td>\n",
       "      <td>2014</td>\n",
       "      <td>3</td>\n",
       "      <td>13.322337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7936000429</th>\n",
       "      <td>2015-03-26</td>\n",
       "      <td>1007500.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.50</td>\n",
       "      <td>3510</td>\n",
       "      <td>7200</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>98136</td>\n",
       "      <td>47.5537</td>\n",
       "      <td>-122.398</td>\n",
       "      <td>2050</td>\n",
       "      <td>6200</td>\n",
       "      <td>8</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>13.822983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997800021</th>\n",
       "      <td>2015-02-19</td>\n",
       "      <td>475000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1310</td>\n",
       "      <td>1294</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>98116</td>\n",
       "      <td>47.5773</td>\n",
       "      <td>-122.409</td>\n",
       "      <td>1330</td>\n",
       "      <td>1265</td>\n",
       "      <td>9</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>13.071070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263000018</th>\n",
       "      <td>2014-05-21</td>\n",
       "      <td>360000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1530</td>\n",
       "      <td>1131</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>98103</td>\n",
       "      <td>47.6993</td>\n",
       "      <td>-122.346</td>\n",
       "      <td>1530</td>\n",
       "      <td>1509</td>\n",
       "      <td>8</td>\n",
       "      <td>2014</td>\n",
       "      <td>3</td>\n",
       "      <td>12.793859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6600060120</th>\n",
       "      <td>2015-02-23</td>\n",
       "      <td>400000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2310</td>\n",
       "      <td>5813</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>98146</td>\n",
       "      <td>47.5107</td>\n",
       "      <td>-122.362</td>\n",
       "      <td>1830</td>\n",
       "      <td>7200</td>\n",
       "      <td>3</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>12.899220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1523300141</th>\n",
       "      <td>2014-06-23</td>\n",
       "      <td>402101.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1020</td>\n",
       "      <td>1350</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>98144</td>\n",
       "      <td>47.5944</td>\n",
       "      <td>-122.299</td>\n",
       "      <td>1020</td>\n",
       "      <td>2007</td>\n",
       "      <td>8</td>\n",
       "      <td>2014</td>\n",
       "      <td>3</td>\n",
       "      <td>12.904459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291310100</th>\n",
       "      <td>2015-01-16</td>\n",
       "      <td>400000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1600</td>\n",
       "      <td>2388</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>98027</td>\n",
       "      <td>47.5345</td>\n",
       "      <td>-122.069</td>\n",
       "      <td>1410</td>\n",
       "      <td>1287</td>\n",
       "      <td>13</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>12.899220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1523300157</th>\n",
       "      <td>2014-10-15</td>\n",
       "      <td>325000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1020</td>\n",
       "      <td>1076</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>98144</td>\n",
       "      <td>47.5941</td>\n",
       "      <td>-122.299</td>\n",
       "      <td>1020</td>\n",
       "      <td>1357</td>\n",
       "      <td>9</td>\n",
       "      <td>2014</td>\n",
       "      <td>3</td>\n",
       "      <td>12.691580</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21600 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date      price  bedrooms  bathrooms  sqft_living  sqft_lot  \\\n",
       "id                                                                              \n",
       "7129300520  2014-10-13   221900.0         3       1.00         1180      5650   \n",
       "6414100192  2014-12-09   538000.0         3       2.25         2570      7242   \n",
       "5631500400  2015-02-25   180000.0         2       1.00          770     10000   \n",
       "2487200875  2014-12-09   604000.0         4       3.00         1960      5000   \n",
       "1954400510  2015-02-18   510000.0         3       2.00         1680      8080   \n",
       "7237550310  2014-05-12  1225000.0         4       4.50         5420    101930   \n",
       "1321400060  2014-06-27   257500.0         3       2.25         1715      6819   \n",
       "2008000270  2015-01-15   291850.0         3       1.50         1060      9711   \n",
       "2414600126  2015-04-15   229500.0         3       1.00         1780      7470   \n",
       "3793500160  2015-03-12   323000.0         3       2.50         1890      6560   \n",
       "1736800520  2015-04-03   662500.0         3       2.50         3560      9796   \n",
       "9212900260  2014-05-27   468000.0         2       1.00         1160      6000   \n",
       "114101516   2014-05-28   310000.0         3       1.00         1430     19901   \n",
       "6054650070  2014-10-07   400000.0         3       1.75         1370      9680   \n",
       "1175000570  2015-03-12   530000.0         5       2.00         1810      4850   \n",
       "9297300055  2015-01-24   650000.0         4       3.00         2950      5000   \n",
       "1875500060  2014-07-31   395000.0         3       2.00         1890     14040   \n",
       "6865200140  2014-05-29   485000.0         4       1.00         1600      4300   \n",
       "16000397    2014-12-05   189000.0         2       1.00         1200      9850   \n",
       "7983200060  2015-04-24   230000.0         3       1.00         1250      9774   \n",
       "6300500875  2014-05-14   385000.0         4       1.75         1620      4980   \n",
       "2524049179  2014-08-26  2000000.0         3       2.75         3050     44867   \n",
       "7137970340  2014-07-03   285000.0         5       2.50         2270      6300   \n",
       "8091400200  2014-05-16   252700.0         2       1.50         1070      9643   \n",
       "3814700200  2014-11-20   329000.0         3       2.25         2450      6500   \n",
       "1202000200  2014-11-03   233000.0         3       2.00         1710      4697   \n",
       "1794500383  2014-06-26   937000.0         3       1.75         2450      2691   \n",
       "3303700376  2014-12-01   667000.0         3       1.00         1400      1581   \n",
       "5101402488  2014-06-24   438000.0         3       1.75         1520      6380   \n",
       "1873100390  2015-03-02   719000.0         4       2.50         2570      7173   \n",
       "...                ...        ...       ...        ...          ...       ...   \n",
       "2025049203  2014-06-10   399950.0         2       1.00          710      1157   \n",
       "952006823   2014-12-02   380000.0         3       2.50         1260       900   \n",
       "3832050760  2014-08-28   270000.0         3       2.50         1870      5000   \n",
       "2767604724  2014-10-15   505000.0         2       2.50         1430      1201   \n",
       "6632300207  2015-03-05   385000.0         3       2.50         1520      1488   \n",
       "2767600688  2014-11-13   414500.0         2       1.50         1210      1278   \n",
       "7570050450  2014-09-10   347500.0         3       2.50         2540      4760   \n",
       "7430200100  2014-05-14  1222500.0         4       3.50         4910      9444   \n",
       "4140940150  2014-10-02   572000.0         4       2.75         2770      3852   \n",
       "1931300412  2015-04-16   475000.0         3       2.25         1190      1200   \n",
       "8672200110  2015-03-17  1088000.0         5       3.75         4170      8142   \n",
       "5087900040  2014-10-17   350000.0         4       2.75         2500      5995   \n",
       "1972201967  2014-10-31   520000.0         2       2.25         1530       981   \n",
       "7502800100  2014-08-13   679950.0         5       2.75         3600      9437   \n",
       "191100405   2015-04-21  1575000.0         4       3.25         3410     10125   \n",
       "8956200760  2014-10-13   541800.0         4       2.50         3118      7866   \n",
       "7202300110  2014-09-15   810000.0         4       3.00         3990      7838   \n",
       "249000205   2014-10-15  1537000.0         5       3.75         4470      8088   \n",
       "5100403806  2015-04-07   467000.0         3       2.50         1425      1179   \n",
       "844000965   2014-06-26   224000.0         3       1.75         1500     11968   \n",
       "7852140040  2014-08-25   507250.0         3       2.50         2270      5536   \n",
       "9834201367  2015-01-26   429000.0         3       2.00         1490      1126   \n",
       "3448900210  2014-10-14   610685.0         4       2.50         2520      6023   \n",
       "7936000429  2015-03-26  1007500.0         4       3.50         3510      7200   \n",
       "2997800021  2015-02-19   475000.0         3       2.50         1310      1294   \n",
       "263000018   2014-05-21   360000.0         3       2.50         1530      1131   \n",
       "6600060120  2015-02-23   400000.0         4       2.50         2310      5813   \n",
       "1523300141  2014-06-23   402101.0         2       0.75         1020      1350   \n",
       "291310100   2015-01-16   400000.0         3       2.50         1600      2388   \n",
       "1523300157  2014-10-15   325000.0         2       0.75         1020      1076   \n",
       "\n",
       "            floors  waterfront  view  condition  ...  yr_renovated  zipcode  \\\n",
       "id                                               ...                          \n",
       "7129300520     1.0           0     0          3  ...             0    98178   \n",
       "6414100192     2.0           0     0          3  ...          1991    98125   \n",
       "5631500400     1.0           0     0          3  ...             0    98028   \n",
       "2487200875     1.0           0     0          5  ...             0    98136   \n",
       "1954400510     1.0           0     0          3  ...             0    98074   \n",
       "7237550310     1.0           0     0          3  ...             0    98053   \n",
       "1321400060     2.0           0     0          3  ...             0    98003   \n",
       "2008000270     1.0           0     0          3  ...             0    98198   \n",
       "2414600126     1.0           0     0          3  ...             0    98146   \n",
       "3793500160     2.0           0     0          3  ...             0    98038   \n",
       "1736800520     1.0           0     0          3  ...             0    98007   \n",
       "9212900260     1.0           0     0          4  ...             0    98115   \n",
       "114101516      1.5           0     0          4  ...             0    98028   \n",
       "6054650070     1.0           0     0          4  ...             0    98074   \n",
       "1175000570     1.5           0     0          3  ...             0    98107   \n",
       "9297300055     2.0           0     3          3  ...             0    98126   \n",
       "1875500060     2.0           0     0          3  ...             0    98019   \n",
       "6865200140     1.5           0     0          4  ...             0    98103   \n",
       "16000397       1.0           0     0          4  ...             0    98002   \n",
       "7983200060     1.0           0     0          4  ...             0    98003   \n",
       "6300500875     1.0           0     0          4  ...             0    98133   \n",
       "2524049179     1.0           0     4          3  ...             0    98040   \n",
       "7137970340     2.0           0     0          3  ...             0    98092   \n",
       "8091400200     1.0           0     0          3  ...             0    98030   \n",
       "3814700200     2.0           0     0          4  ...             0    98030   \n",
       "1202000200     1.5           0     0          5  ...             0    98002   \n",
       "1794500383     2.0           0     0          3  ...             0    98119   \n",
       "3303700376     1.5           0     0          5  ...             0    98112   \n",
       "5101402488     1.0           0     0          3  ...             0    98115   \n",
       "1873100390     2.0           0     0          3  ...             0    98052   \n",
       "...            ...         ...   ...        ...  ...           ...      ...   \n",
       "2025049203     2.0           0     0          4  ...             0    98102   \n",
       "952006823      2.0           0     0          3  ...             0    98116   \n",
       "3832050760     2.0           0     0          3  ...             0    98042   \n",
       "2767604724     3.0           0     0          3  ...             0    98107   \n",
       "6632300207     3.0           0     0          3  ...             0    98125   \n",
       "2767600688     2.0           0     0          3  ...             0    98117   \n",
       "7570050450     2.0           0     0          3  ...             0    98038   \n",
       "7430200100     1.5           0     0          3  ...             0    98074   \n",
       "4140940150     2.0           0     0          3  ...             0    98178   \n",
       "1931300412     3.0           0     0          3  ...             0    98103   \n",
       "8672200110     2.0           0     2          3  ...             0    98056   \n",
       "5087900040     2.0           0     0          3  ...             0    98042   \n",
       "1972201967     3.0           0     0          3  ...             0    98103   \n",
       "7502800100     2.0           0     0          3  ...             0    98059   \n",
       "191100405      2.0           0     0          3  ...             0    98040   \n",
       "8956200760     2.0           0     2          3  ...             0    98001   \n",
       "7202300110     2.0           0     0          3  ...             0    98053   \n",
       "249000205      2.0           0     0          3  ...             0    98004   \n",
       "5100403806     3.0           0     0          3  ...             0    98125   \n",
       "844000965      1.0           0     0          3  ...             0    98010   \n",
       "7852140040     2.0           0     0          3  ...             0    98065   \n",
       "9834201367     3.0           0     0          3  ...             0    98144   \n",
       "3448900210     2.0           0     0          3  ...             0    98056   \n",
       "7936000429     2.0           0     0          3  ...             0    98136   \n",
       "2997800021     2.0           0     0          3  ...             0    98116   \n",
       "263000018      3.0           0     0          3  ...             0    98103   \n",
       "6600060120     2.0           0     0          3  ...             0    98146   \n",
       "1523300141     2.0           0     0          3  ...             0    98144   \n",
       "291310100      2.0           0     0          3  ...             0    98027   \n",
       "1523300157     2.0           0     0          3  ...             0    98144   \n",
       "\n",
       "                lat     long  sqft_living15  sqft_lot15  yr_old  year_sold  \\\n",
       "id                                                                           \n",
       "7129300520  47.5112 -122.257           1340        5650      62       2014   \n",
       "6414100192  47.7210 -122.319           1690        7639      66       2014   \n",
       "5631500400  47.7379 -122.233           2720        8062      84       2015   \n",
       "2487200875  47.5208 -122.393           1360        5000      52       2014   \n",
       "1954400510  47.6168 -122.045           1800        7503      30       2015   \n",
       "7237550310  47.6561 -122.005           4760      101930      16       2014   \n",
       "1321400060  47.3097 -122.327           2238        6819      22       2014   \n",
       "2008000270  47.4095 -122.315           1650        9711      54       2015   \n",
       "2414600126  47.5123 -122.337           1780        8113      57       2015   \n",
       "3793500160  47.3684 -122.031           2390        7570      14       2015   \n",
       "1736800520  47.6007 -122.145           2210        8925      52       2015   \n",
       "9212900260  47.6900 -122.292           1330        6000      75       2014   \n",
       "114101516   47.7558 -122.229           1780       12697      90       2014   \n",
       "6054650070  47.6127 -122.045           1370       10208      40       2014   \n",
       "1175000570  47.6700 -122.394           1360        4850     117       2015   \n",
       "9297300055  47.5714 -122.375           2140        4000      38       2015   \n",
       "1875500060  47.7277 -121.962           1890       14018      23       2014   \n",
       "6865200140  47.6648 -122.343           1610        4300     101       2014   \n",
       "16000397    47.3089 -122.210           1060        5095      96       2014   \n",
       "7983200060  47.3343 -122.306           1280        8850      48       2015   \n",
       "6300500875  47.7025 -122.341           1400        4980      70       2014   \n",
       "2524049179  47.5316 -122.233           4110       20336      49       2014   \n",
       "7137970340  47.3266 -122.169           2240        7005      22       2014   \n",
       "8091400200  47.3533 -122.166           1220        8386      32       2014   \n",
       "3814700200  47.3739 -122.172           2200        6865      32       2014   \n",
       "1202000200  47.3048 -122.218           1030        4705      76       2014   \n",
       "1794500383  47.6386 -122.360           1760        3573     102       2014   \n",
       "3303700376  47.6221 -122.314           1860        3861     108       2014   \n",
       "5101402488  47.6950 -122.304           1520        6235      69       2014   \n",
       "1873100390  47.7073 -122.110           2630        6026      12       2015   \n",
       "...             ...      ...            ...         ...     ...        ...   \n",
       "2025049203  47.6413 -122.329           1370        1173      74       2014   \n",
       "952006823   47.5621 -122.384           1310        1415      10       2014   \n",
       "3832050760  47.3339 -122.055           2170        5399       8       2014   \n",
       "2767604724  47.6707 -122.381           1430        1249       8       2014   \n",
       "6632300207  47.7337 -122.309           1520        1497      11       2015   \n",
       "2767600688  47.6756 -122.375           1210        1118      10       2014   \n",
       "7570050450  47.3452 -122.022           2540        4571       7       2014   \n",
       "7430200100  47.6502 -122.066           4560       11063      10       2014   \n",
       "4140940150  47.5001 -122.232           1810        5641       3       2014   \n",
       "1931300412  47.6542 -122.346           1180        1224       9       2015   \n",
       "8672200110  47.5354 -122.181           3030        7980      11       2015   \n",
       "5087900040  47.3749 -122.107           2530        5988       9       2014   \n",
       "1972201967  47.6533 -122.346           1530        1282      11       2014   \n",
       "7502800100  47.4822 -122.131           3550        9421       3       2014   \n",
       "191100405   47.5653 -122.223           2290       10125      10       2015   \n",
       "8956200760  47.2931 -122.264           2673        6500       3       2014   \n",
       "7202300110  47.6857 -122.046           3370        6814      14       2014   \n",
       "249000205   47.6321 -122.200           2780        8964       9       2014   \n",
       "5100403806  47.6963 -122.318           1285        1253       9       2015   \n",
       "844000965   47.3095 -122.002           1320       11303       3       2014   \n",
       "7852140040  47.5389 -121.881           2270        5731      14       2014   \n",
       "9834201367  47.5699 -122.288           1400        1230       3       2015   \n",
       "3448900210  47.5137 -122.167           2520        6023       3       2014   \n",
       "7936000429  47.5537 -122.398           2050        6200       8       2015   \n",
       "2997800021  47.5773 -122.409           1330        1265       9       2015   \n",
       "263000018   47.6993 -122.346           1530        1509       8       2014   \n",
       "6600060120  47.5107 -122.362           1830        7200       3       2015   \n",
       "1523300141  47.5944 -122.299           1020        2007       8       2014   \n",
       "291310100   47.5345 -122.069           1410        1287      13       2015   \n",
       "1523300157  47.5941 -122.299           1020        1357       9       2014   \n",
       "\n",
       "            since_sold  price_log  \n",
       "id                                 \n",
       "7129300520           3  12.309982  \n",
       "6414100192           3  13.195614  \n",
       "5631500400           2  12.100712  \n",
       "2487200875           3  13.311329  \n",
       "1954400510           2  13.142166  \n",
       "7237550310           3  14.018451  \n",
       "1321400060           3  12.458775  \n",
       "2008000270           2  12.583995  \n",
       "2414600126           2  12.343658  \n",
       "3793500160           2  12.685408  \n",
       "1736800520           2  13.403776  \n",
       "9212900260           3  13.056224  \n",
       "114101516            3  12.644328  \n",
       "6054650070           3  12.899220  \n",
       "1175000570           2  13.180632  \n",
       "9297300055           2  13.384728  \n",
       "1875500060           3  12.886641  \n",
       "6865200140           3  13.091904  \n",
       "16000397             3  12.149502  \n",
       "7983200060           2  12.345835  \n",
       "6300500875           3  12.860999  \n",
       "2524049179           3  14.508658  \n",
       "7137970340           3  12.560244  \n",
       "8091400200           3  12.439958  \n",
       "3814700200           3  12.703813  \n",
       "1202000200           3  12.358794  \n",
       "1794500383           3  13.750439  \n",
       "3303700376           3  13.410545  \n",
       "5101402488           3  12.989974  \n",
       "1873100390           2  13.485617  \n",
       "...                ...        ...  \n",
       "2025049203           3  12.899095  \n",
       "952006823            3  12.847927  \n",
       "3832050760           3  12.506177  \n",
       "2767604724           3  13.132314  \n",
       "6632300207           2  12.860999  \n",
       "2767600688           3  12.934828  \n",
       "7570050450           3  12.758520  \n",
       "7430200100           3  14.016409  \n",
       "4140940150           3  13.256894  \n",
       "1931300412           2  13.071070  \n",
       "8672200110           2  13.899852  \n",
       "5087900040           3  12.765688  \n",
       "1972201967           3  13.161584  \n",
       "7502800100           3  13.429775  \n",
       "191100405            2  14.269766  \n",
       "8956200760           3  13.202652  \n",
       "7202300110           3  13.604790  \n",
       "249000205            3  14.245343  \n",
       "5100403806           2  13.054085  \n",
       "844000965            3  12.319401  \n",
       "7852140040           3  13.136759  \n",
       "9834201367           2  12.969212  \n",
       "3448900210           3  13.322337  \n",
       "7936000429           2  13.822983  \n",
       "2997800021           2  13.071070  \n",
       "263000018            3  12.793859  \n",
       "6600060120           2  12.899220  \n",
       "1523300141           3  12.904459  \n",
       "291310100            2  12.899220  \n",
       "1523300157           3  12.691580  \n",
       "\n",
       "[21600 rows x 24 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-03T18:56:17.233595Z",
     "start_time": "2020-01-03T18:56:17.229205Z"
    }
   },
   "outputs": [],
   "source": [
    "features = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot',\n",
    "       'floors', 'waterfront', 'view', 'condition', 'grade', 'sqft_above',\n",
    "       'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode', 'lat', 'long',\n",
    "       'sqft_living15', 'sqft_lot15', 'yr_old', 'since_sold',\n",
    "       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-03T18:56:17.244645Z",
     "start_time": "2020-01-03T18:56:17.236522Z"
    }
   },
   "outputs": [],
   "source": [
    "y = df['price']\n",
    "X = df[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-03T18:56:17.252089Z",
     "start_time": "2020-01-03T18:56:17.247061Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Store the number of columns in the predictors data to `n_cols`. This has been done for you.\n",
    "- Start by creating a `Sequential` model called `model`.\n",
    "- Use the `.add()` method on `model` to add a `Dense` layer.\n",
    "- Add 50 units, specify `activation='relu'`, and the `input_shape` parameter to be the tuple `(n_cols,)` which means it has `n_cols` items in each row of data, and any number of rows of data are acceptable as inputs.\n",
    "- Add another `Dense` layer. This should have 32 units and a 'relu' activation.\n",
    "- Finally, add an output layer, which is a `Dense` layer with a single node. Don't use any activation function here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-03T18:56:17.321333Z",
     "start_time": "2020-01-03T18:56:17.254059Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save the number of columns in predictors: n_cols\n",
    "n_cols = len(features)\n",
    "\n",
    "# Set up the model: model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the first layer\n",
    "model.add(Dense(100, activation='relu', input_shape = (n_cols,)))\n",
    "\n",
    "# Add the second layer\n",
    "model.add(Dense(64, activation='relu'))\n",
    "\n",
    "# Add the output layer\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Compile the model using `model.compile()`. Your `optimizer` should be `'adam'` and the `loss` should be `'mean_squared_error'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-03T18:56:17.363869Z",
     "start_time": "2020-01-03T18:56:17.324016Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss function: mean_squared_error\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Verify that model contains information from compiling\n",
    "print(\"Loss function: \" + model.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fit the `model`. Remember that the first argument is the predictive features (`predictors`), and the data to be predicted (`target`) is the second argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-03T18:56:20.266632Z",
     "start_time": "2020-01-03T18:56:17.367888Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      " - 0s - loss: 329763573229.0370\n",
      "Epoch 2/20\n",
      " - 0s - loss: 149784110573.0370\n",
      "Epoch 3/20\n",
      " - 0s - loss: 117761237807.4074\n",
      "Epoch 4/20\n",
      " - 0s - loss: 115596603354.0741\n",
      "Epoch 5/20\n",
      " - 0s - loss: 113847277568.0000\n",
      "Epoch 6/20\n",
      " - 0s - loss: 111823658211.5556\n",
      "Epoch 7/20\n",
      " - 0s - loss: 109617619247.4074\n",
      "Epoch 8/20\n",
      " - 0s - loss: 107068077549.0370\n",
      "Epoch 9/20\n",
      " - 0s - loss: 103703495186.9630\n",
      "Epoch 10/20\n",
      " - 0s - loss: 98568787816.2963\n",
      "Epoch 11/20\n",
      " - 0s - loss: 90359743658.6667\n",
      "Epoch 12/20\n",
      " - 0s - loss: 80346699946.6667\n",
      "Epoch 13/20\n",
      " - 0s - loss: 70839655670.5185\n",
      "Epoch 14/20\n",
      " - 0s - loss: 63975897315.5555\n",
      "Epoch 15/20\n",
      " - 0s - loss: 59696546000.5926\n",
      "Epoch 16/20\n",
      " - 0s - loss: 58136496222.8148\n",
      "Epoch 17/20\n",
      " - 0s - loss: 57253964856.8889\n",
      "Epoch 18/20\n",
      " - 0s - loss: 56701666417.7778\n",
      "Epoch 19/20\n",
      " - 0s - loss: 56547137991.1111\n",
      "Epoch 20/20\n",
      " - 0s - loss: 56206946493.6296\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x131835240>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(X, # Features\n",
    "                      y, # Target\n",
    "                      epochs=20, # Number of epochs\n",
    "                      verbose=2, # Some output\n",
    "                      batch_size=500, # Number of observations per batch\n",
    "                  \n",
    "         ) # Data for evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Models\n",
    "\n",
    "\n",
    "- ‘categorical_crossentropy’ loss function Similar to log loss: Lower is be!er\n",
    "- Add metrics = [‘accuracy’] to compile step for easy-to- understand diagnostics\n",
    "- Output layer has separate node for each possible outcome, and uses ‘so\"max’ activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-03T18:56:20.589383Z",
     "start_time": "2020-01-03T18:56:20.270686Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>youngin</th>\n",
       "      <th>male</th>\n",
       "      <th>Q</th>\n",
       "      <th>S</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Survived  Pclass   Age  SibSp  Parch     Fare  youngin  male  Q  \\\n",
       "PassengerId                                                                    \n",
       "1                   0       3  22.0      1      0   7.2500        0     1  0   \n",
       "2                   1       1  38.0      1      0  71.2833        0     0  0   \n",
       "3                   1       3  26.0      0      0   7.9250        0     0  0   \n",
       "4                   1       1  35.0      1      0  53.1000        0     0  0   \n",
       "5                   0       3  35.0      0      0   8.0500        0     1  0   \n",
       "\n",
       "             S  \n",
       "PassengerId     \n",
       "1            1  \n",
       "2            0  \n",
       "3            1  \n",
       "4            1  \n",
       "5            1  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/learn-co-students/nyc-mhtn-ds-042219-lectures/master/Module_4/cleaned_titanic.csv', index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-03T18:56:20.621493Z",
     "start_time": "2020-01-03T18:56:20.613731Z"
    }
   },
   "outputs": [],
   "source": [
    "predictors = df.drop(columns=['Survived'])\n",
    "n_cols = predictors.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Convert `df.Survived` to a categorical variable using the `to_categorical()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-03T18:56:20.664717Z",
     "start_time": "2020-01-03T18:56:20.635038Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "# Convert the target to categorical: target\n",
    "target = to_categorical(df.Survived)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-03T18:56:21.368398Z",
     "start_time": "2020-01-03T18:56:20.668384Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import `train_test_split` from `sklearn.model_selection`\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Split the data up in train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(predictors, target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-03T18:56:21.376152Z",
     "start_time": "2020-01-03T18:56:21.371206Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Specify a `Sequential` model called `model`.\n",
    "- Add a `Dense` layer with 32 nodes. Use `'relu'` as the `activation` and `(n_cols,)` as the `input_shape`.\n",
    "- Add the `Dense` output layer. Because there are two outcomes, it should have 2 units, and because it is a classification model, the `activation` should be `'softmax'`.\n",
    "- Compile the model, using `'sgd'` as the `optimizer`, `'categorical_crossentropy'` as the loss function, and `metrics=['accuracy']` to see the accuracy (what fraction of predictions were correct) at the end of each epoch.\n",
    "- Fit the model using the `X_train` and the `y_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-03T18:56:22.105838Z",
     "start_time": "2020-01-03T18:56:21.378724Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " - 0s - loss: 6.9449 - accuracy: 0.5485\n",
      "Epoch 2/50\n",
      " - 0s - loss: 5.4845 - accuracy: 0.6371\n",
      "Epoch 3/50\n",
      " - 0s - loss: 1.2480 - accuracy: 0.6624\n",
      "Epoch 4/50\n",
      " - 0s - loss: 3.8518 - accuracy: 0.5091\n",
      "Epoch 5/50\n",
      " - 0s - loss: 2.6403 - accuracy: 0.6034\n",
      "Epoch 6/50\n",
      " - 0s - loss: 2.6403 - accuracy: 0.4740\n",
      "Epoch 7/50\n",
      " - 0s - loss: 2.2551 - accuracy: 0.5795\n",
      "Epoch 8/50\n",
      " - 0s - loss: 1.4673 - accuracy: 0.6062\n",
      "Epoch 9/50\n",
      " - 0s - loss: 2.1811 - accuracy: 0.4895\n",
      "Epoch 10/50\n",
      " - 0s - loss: 1.2139 - accuracy: 0.6399\n",
      "Epoch 11/50\n",
      " - 0s - loss: 2.2739 - accuracy: 0.4993\n",
      "Epoch 12/50\n",
      " - 0s - loss: 1.5886 - accuracy: 0.5556\n",
      "Epoch 13/50\n",
      " - 0s - loss: 0.9790 - accuracy: 0.6695\n",
      "Epoch 14/50\n",
      " - 0s - loss: 0.8214 - accuracy: 0.6062\n",
      "Epoch 15/50\n",
      " - 0s - loss: 0.8078 - accuracy: 0.6526\n",
      "Epoch 16/50\n",
      " - 0s - loss: 2.0049 - accuracy: 0.5837\n",
      "Epoch 17/50\n",
      " - 0s - loss: 1.4503 - accuracy: 0.5373\n",
      "Epoch 18/50\n",
      " - 0s - loss: 0.8735 - accuracy: 0.6329\n",
      "Epoch 19/50\n",
      " - 0s - loss: 0.7224 - accuracy: 0.6793\n",
      "Epoch 20/50\n",
      " - 0s - loss: 0.9787 - accuracy: 0.6020\n",
      "Epoch 21/50\n",
      " - 0s - loss: 0.7333 - accuracy: 0.6751\n",
      "Epoch 22/50\n",
      " - 0s - loss: 1.1294 - accuracy: 0.5415\n",
      "Epoch 23/50\n",
      " - 0s - loss: 1.0578 - accuracy: 0.5738\n",
      "Epoch 24/50\n",
      " - 0s - loss: 0.7851 - accuracy: 0.6399\n",
      "Epoch 25/50\n",
      " - 0s - loss: 0.8051 - accuracy: 0.6076\n",
      "Epoch 26/50\n",
      " - 0s - loss: 0.6883 - accuracy: 0.6681\n",
      "Epoch 27/50\n",
      " - 0s - loss: 0.6568 - accuracy: 0.6498\n",
      "Epoch 28/50\n",
      " - 0s - loss: 0.7985 - accuracy: 0.6498\n",
      "Epoch 29/50\n",
      " - 0s - loss: 0.6540 - accuracy: 0.6793\n",
      "Epoch 30/50\n",
      " - 0s - loss: 0.7346 - accuracy: 0.6498\n",
      "Epoch 31/50\n",
      " - 0s - loss: 1.0558 - accuracy: 0.5359\n",
      "Epoch 32/50\n",
      " - 0s - loss: 0.6354 - accuracy: 0.6526\n",
      "Epoch 33/50\n",
      " - 0s - loss: 0.7308 - accuracy: 0.6399\n",
      "Epoch 34/50\n",
      " - 0s - loss: 0.9499 - accuracy: 0.6315\n",
      "Epoch 35/50\n",
      " - 0s - loss: 0.6383 - accuracy: 0.6765\n",
      "Epoch 36/50\n",
      " - 0s - loss: 0.6261 - accuracy: 0.6596\n",
      "Epoch 37/50\n",
      " - 0s - loss: 0.6296 - accuracy: 0.6610\n",
      "Epoch 38/50\n",
      " - 0s - loss: 0.7527 - accuracy: 0.6343\n",
      "Epoch 39/50\n",
      " - 0s - loss: 0.7639 - accuracy: 0.6132\n",
      "Epoch 40/50\n",
      " - 0s - loss: 0.6785 - accuracy: 0.6498\n",
      "Epoch 41/50\n",
      " - 0s - loss: 0.7029 - accuracy: 0.6315\n",
      "Epoch 42/50\n",
      " - 0s - loss: 0.6337 - accuracy: 0.6667\n",
      "Epoch 43/50\n",
      " - 0s - loss: 0.6527 - accuracy: 0.6737\n",
      "Epoch 44/50\n",
      " - 0s - loss: 0.6789 - accuracy: 0.6737\n",
      "Epoch 45/50\n",
      " - 0s - loss: 0.6790 - accuracy: 0.6357\n",
      "Epoch 46/50\n",
      " - 0s - loss: 0.6771 - accuracy: 0.6385\n",
      "Epoch 47/50\n",
      " - 0s - loss: 0.6335 - accuracy: 0.6709\n",
      "Epoch 48/50\n",
      " - 0s - loss: 0.6229 - accuracy: 0.6751\n",
      "Epoch 49/50\n",
      " - 0s - loss: 0.6447 - accuracy: 0.6723\n",
      "Epoch 50/50\n",
      " - 0s - loss: 0.6510 - accuracy: 0.6850\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1a3ac48ef0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "# Convert the target to categorical: target\n",
    "# target = to_categorical(df.Survived)\n",
    "\n",
    "# Set up the model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the first layer\n",
    "model.add(Dense(64, activation='relu', input_shape = (n_cols,)))\n",
    "\n",
    "# Add the output layer\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, # Features\n",
    "                      y_train, # Target\n",
    "                      epochs=50, # Number of epochs\n",
    "                      verbose=2, # Some output\n",
    "                      batch_size=500, # Number of observations per batch\n",
    "                  \n",
    "         ) # Data for evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving, reloading and using your Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-03T18:56:22.547433Z",
     "start_time": "2020-01-03T18:56:22.109364Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model.save('model_file.h5')\n",
    "my_model = load_model('model_file.h5')\n",
    "# predictions = my_model.predict('X')\n",
    "# probability_true = predictions[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create your predictions using the model's `.predict()` method on `X_test`.\n",
    "- Use NumPy indexing to find the column corresponding to predicted probabilities of survival being True. This is the second column (index `1`) of `predictions`. Store the result in `predicted_prob_true` and print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-03T18:56:22.613972Z",
     "start_time": "2020-01-03T18:56:22.551653Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.28371468 0.4985484  0.40151876 0.32448488 0.462737   0.28295693\n",
      " 0.37903813 0.36448216 0.63174355 0.5791533  0.48633012 0.38133258\n",
      " 0.08740138 0.3963328  0.29312903 0.38602608 0.6812434  0.35118353\n",
      " 0.4574453  0.3237263  0.48794344 0.5438829  0.14945662 0.45093986\n",
      " 0.32558694 0.44562876 0.29380697 0.29454157 0.47052938 0.33484027\n",
      " 0.48636836 0.29014668 0.32558694 0.4741305  0.33397758 0.52596533\n",
      " 0.47281647 0.975894   0.3703315  0.32558694 0.33108798 0.27757502\n",
      " 0.4019965  0.30384368 0.42687398 0.44349086 0.41039583 0.3136203\n",
      " 0.2844002  0.5144695  0.4598686  0.3419985  0.34302524 0.35756093\n",
      " 0.46482927 0.4779612  0.18784165 0.18585467 0.41056535 0.29396525\n",
      " 0.28278238 0.30109394 0.5214985  0.40969634 0.296232   0.41893595\n",
      " 0.10925518 0.5130992  0.6388358  0.5743594  0.35675547 0.8831805\n",
      " 0.38170087 0.30300134 0.2802833  0.36277673 0.37957564 0.20984338\n",
      " 0.46020773 0.45472762 0.11315465 0.4677662  0.39160234 0.29014668\n",
      " 0.38995567 0.31796512 0.15988544 0.4019965  0.3060989  0.32558694\n",
      " 0.25539008 0.33893713 0.33628818 0.23541905 0.4876616  0.41581807\n",
      " 0.3237263  0.24540575 0.4315688  0.31796512 0.30376598 0.7987287\n",
      " 0.46292195 0.25104702 0.07554252 0.15764552 0.26702717 0.50269204\n",
      " 0.37264872 0.36156538 0.39152035 0.54679734 0.4823094  0.5459319\n",
      " 0.30785218 0.35148364 0.13580845 0.11850853 0.2906302  0.39590445\n",
      " 0.4555135  0.57622    0.12005238 0.39197972 0.5027129  0.62749255\n",
      " 0.45093986 0.53670156 0.31978998 0.19797045 0.2760518  0.42026207\n",
      " 0.5646517  0.35112485 0.2759531  0.37698904 0.25366974 0.35879114\n",
      " 0.4013035  0.46049866 0.32305124 0.3237263  0.3161104  0.3260142\n",
      " 0.6314513  0.519043   0.05867776 0.58289    0.32227254 0.40892157\n",
      " 0.31892568 0.40055746 0.46526197 0.46375555 0.39118478 0.49588972\n",
      " 0.56048554 0.42900708 0.10461724 0.62159014 0.30275327 0.44969535\n",
      " 0.24610755 0.3266165  0.42025942 0.24817996 0.5264407  0.33628818\n",
      " 0.20897195 0.38754365 0.35879114 0.39916283 0.46139959 0.3879694\n",
      " 0.3550715  0.31928954 0.27692327 0.45120716]\n"
     ]
    }
   ],
   "source": [
    "# Calculate predictions: predictions\n",
    "predictions = my_model.predict(X_test)\n",
    "\n",
    "# Calculate predicted probability of survival: predicted_prob_true\n",
    "predicted_prob_true = predictions[:,1]\n",
    "\n",
    "# print predicted_prob_true\n",
    "print(predicted_prob_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify your model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-03T18:56:22.623129Z",
     "start_time": "2020-01-03T18:56:22.616844Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 64)                640       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 770\n",
      "Trainable params: 770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "my_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's play with Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Google Playground](https://developers.google.com/machine-learning/crash-course/introduction-to-neural-networks/playground-exercises)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **Number of Hidden Layers**\n",
    "\n",
    "*For many problems you can start with just one or two hidden layers it will work just fine. For more complex problems, you can gradually ramp up the number of hidden layers until your model starts to over fit. Very complex tasks, like image classification, will need dozens of layers.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Number of Neurons per layer**\n",
    "\n",
    "*The number of nuerons for the input and output layers are dependent on your data and the task. For hiddne layers, a common practice is to create a funnel with funnel with fewer and fewer neurons per layer.*\n",
    "\n",
    "*In general, you will get more bang for your buck by adding on more layers than adding more neurons.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **[Activation Functions](https://towardsdatascience.com/exploring-activation-functions-for-neural-networks-73498da59b02)**\n",
    "    - Linear\n",
    "    - Sigmoid\n",
    "    - Softmax\n",
    "    - Tanh\n",
    "    - ReLu\n",
    "    - elu\n",
    "    \n",
    "*In most cases you can use the ReLu activation function (or one of its variants) in the hidden layers. For the output layer, the softmax activation function is generally good for multiclass problems and the sigmouid function for binary classificatin problems. For regression tasks, you can simply use no activation function at all*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Selecting an optimizer](https://www.dlology.com/blog/quick-notes-on-how-to-choose-optimizer-in-keras/)\n",
    "    - Adam\n",
    "    - SGD\n",
    "    - RMSprop\n",
    "    - Adagrad\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Learning Rate**\n",
    "\n",
    "*If you set it too low, training will eventually converge, but it will do so slowly.*\n",
    "*If you set it too high, it might acutally diverge.*\n",
    "*If you set it slightly too high, it will converge at first but miss the local optima.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Regularization** \n",
    "    - **L1 and L2**\n",
    "    - **Dropout:**\n",
    "        \n",
    "        *Dropout is most popular techniqure for deep neural networks. It is a fairly simple algorithm where at every training step, every neuron has a probability fo being teporarily \"droppedout,\" meaning it will be completely ignored during this traing step, but it may be active during the next step.*\n",
    "    \n",
    "    - [Early Stopping](https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/)\n",
    "    \n",
    "    *Just interrupt training whne its performance on the validation set starts dropping*\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Paper on selecting hyperparameters](https://arxiv.org/pdf/1206.5533v2.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting a Model with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import  Modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-03T18:56:22.630695Z",
     "start_time": "2020-01-03T18:56:22.625197Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create first network with Keras\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import regularizers\n",
    "from keras.optimizers import SGD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model\n",
    "Models in Keras are defined as a sequence of layers.\n",
    "\n",
    "We create a Sequential model and add layers one at a time until we are happy with our network topology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-03T18:56:22.742643Z",
     "start_time": "2020-01-03T18:56:22.633430Z"
    }
   },
   "outputs": [],
   "source": [
    "network = Sequential()\n",
    "\n",
    "# Add a dropout layer for input layer\n",
    "network.add(Dropout(0.1, input_shape=(n_cols,)))\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(Dense(units=32, activation='relu'))\n",
    "# Add a dropout layer for previous hidden layer\n",
    "network.add(Dropout(0.1))\n",
    "# Add fully connected layer with a ReLU activation function and L2 regularization\n",
    "network.add(Dense(units=32, kernel_regularizer=regularizers.l2(0.05),activation='relu'))\n",
    "#Final Layer\n",
    "network.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Using GridSearchCV to tune Neural Networks](https://chrisalbon.com/deep_learning/keras/tuning_neural_network_hyperparameters/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-03T18:56:22.821562Z",
     "start_time": "2020-01-03T18:56:22.751777Z"
    }
   },
   "outputs": [],
   "source": [
    "network.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Keras Implementation of optimizers](https://keras.io/optimizers/)\n",
    "\n",
    "[Impact of Learning Rate on MOdel Performance](https://machinelearningmastery.com/understand-the-dynamics-of-learning-rate-on-deep-learning-neural-networks/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-03T18:56:22.834016Z",
     "start_time": "2020-01-03T18:56:22.828692Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set callback functions to early stop training and save the best model so far\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=5),\n",
    "             ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-03T18:56:46.782787Z",
     "start_time": "2020-01-03T18:56:22.837182Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 711 samples, validate on 178 samples\n",
      "Epoch 1/1000\n",
      " - 0s - loss: 3.8024 - accuracy: 0.3404 - val_loss: 2.6711 - val_accuracy: 0.2921\n",
      "Epoch 2/1000\n",
      " - 0s - loss: 2.7517 - accuracy: 0.4233 - val_loss: 2.1693 - val_accuracy: 0.6404\n",
      "Epoch 3/1000\n",
      " - 0s - loss: 2.3380 - accuracy: 0.5795 - val_loss: 2.0455 - val_accuracy: 0.6910\n",
      "Epoch 4/1000\n",
      " - 0s - loss: 2.2887 - accuracy: 0.6062 - val_loss: 1.9629 - val_accuracy: 0.7191\n",
      "Epoch 5/1000\n",
      " - 0s - loss: 2.1939 - accuracy: 0.6188 - val_loss: 1.8895 - val_accuracy: 0.7191\n",
      "Epoch 6/1000\n",
      " - 0s - loss: 2.0652 - accuracy: 0.6287 - val_loss: 1.8151 - val_accuracy: 0.7135\n",
      "Epoch 7/1000\n",
      " - 0s - loss: 1.9779 - accuracy: 0.6203 - val_loss: 1.7580 - val_accuracy: 0.6742\n",
      "Epoch 8/1000\n",
      " - 0s - loss: 1.9427 - accuracy: 0.5935 - val_loss: 1.7114 - val_accuracy: 0.6517\n",
      "Epoch 9/1000\n",
      " - 0s - loss: 1.8345 - accuracy: 0.6076 - val_loss: 1.6526 - val_accuracy: 0.6742\n",
      "Epoch 10/1000\n",
      " - 0s - loss: 1.8156 - accuracy: 0.6062 - val_loss: 1.6017 - val_accuracy: 0.6798\n",
      "Epoch 11/1000\n",
      " - 0s - loss: 1.6865 - accuracy: 0.6118 - val_loss: 1.5490 - val_accuracy: 0.7247\n",
      "Epoch 12/1000\n",
      " - 0s - loss: 1.6452 - accuracy: 0.6357 - val_loss: 1.4963 - val_accuracy: 0.6966\n",
      "Epoch 13/1000\n",
      " - 0s - loss: 1.6015 - accuracy: 0.6287 - val_loss: 1.4618 - val_accuracy: 0.6966\n",
      "Epoch 14/1000\n",
      " - 0s - loss: 1.5639 - accuracy: 0.6118 - val_loss: 1.4122 - val_accuracy: 0.6910\n",
      "Epoch 15/1000\n",
      " - 0s - loss: 1.4870 - accuracy: 0.6456 - val_loss: 1.3736 - val_accuracy: 0.6966\n",
      "Epoch 16/1000\n",
      " - 0s - loss: 1.4929 - accuracy: 0.6174 - val_loss: 1.3369 - val_accuracy: 0.6742\n",
      "Epoch 17/1000\n",
      " - 0s - loss: 1.4173 - accuracy: 0.6428 - val_loss: 1.3067 - val_accuracy: 0.6798\n",
      "Epoch 18/1000\n",
      " - 0s - loss: 1.3754 - accuracy: 0.6540 - val_loss: 1.2844 - val_accuracy: 0.6910\n",
      "Epoch 19/1000\n",
      " - 0s - loss: 1.3392 - accuracy: 0.6315 - val_loss: 1.2447 - val_accuracy: 0.6966\n",
      "Epoch 20/1000\n",
      " - 0s - loss: 1.3504 - accuracy: 0.6259 - val_loss: 1.2132 - val_accuracy: 0.7303\n",
      "Epoch 21/1000\n",
      " - 0s - loss: 1.2992 - accuracy: 0.6526 - val_loss: 1.1871 - val_accuracy: 0.7191\n",
      "Epoch 22/1000\n",
      " - 0s - loss: 1.2685 - accuracy: 0.6428 - val_loss: 1.1590 - val_accuracy: 0.6910\n",
      "Epoch 23/1000\n",
      " - 0s - loss: 1.2490 - accuracy: 0.6456 - val_loss: 1.1424 - val_accuracy: 0.7022\n",
      "Epoch 24/1000\n",
      " - 0s - loss: 1.2018 - accuracy: 0.6385 - val_loss: 1.1208 - val_accuracy: 0.6966\n",
      "Epoch 25/1000\n",
      " - 0s - loss: 1.2033 - accuracy: 0.6371 - val_loss: 1.1038 - val_accuracy: 0.6854\n",
      "Epoch 26/1000\n",
      " - 0s - loss: 1.1606 - accuracy: 0.6526 - val_loss: 1.0813 - val_accuracy: 0.7079\n",
      "Epoch 27/1000\n",
      " - 0s - loss: 1.1263 - accuracy: 0.6639 - val_loss: 1.0544 - val_accuracy: 0.7022\n",
      "Epoch 28/1000\n",
      " - 0s - loss: 1.1020 - accuracy: 0.6484 - val_loss: 1.0413 - val_accuracy: 0.6798\n",
      "Epoch 29/1000\n",
      " - 0s - loss: 1.1139 - accuracy: 0.6442 - val_loss: 1.0467 - val_accuracy: 0.6798\n",
      "Epoch 30/1000\n",
      " - 0s - loss: 1.0866 - accuracy: 0.6470 - val_loss: 1.0071 - val_accuracy: 0.6798\n",
      "Epoch 31/1000\n",
      " - 0s - loss: 1.0664 - accuracy: 0.6540 - val_loss: 0.9905 - val_accuracy: 0.6910\n",
      "Epoch 32/1000\n",
      " - 0s - loss: 1.0555 - accuracy: 0.6512 - val_loss: 0.9843 - val_accuracy: 0.6966\n",
      "Epoch 33/1000\n",
      " - 0s - loss: 1.0295 - accuracy: 0.6540 - val_loss: 0.9636 - val_accuracy: 0.7247\n",
      "Epoch 34/1000\n",
      " - 0s - loss: 1.0171 - accuracy: 0.6554 - val_loss: 0.9492 - val_accuracy: 0.6910\n",
      "Epoch 35/1000\n",
      " - 0s - loss: 1.0370 - accuracy: 0.6399 - val_loss: 0.9361 - val_accuracy: 0.7022\n",
      "Epoch 36/1000\n",
      " - 0s - loss: 1.0032 - accuracy: 0.6624 - val_loss: 0.9246 - val_accuracy: 0.7022\n",
      "Epoch 37/1000\n",
      " - 0s - loss: 0.9729 - accuracy: 0.6653 - val_loss: 0.9170 - val_accuracy: 0.7022\n",
      "Epoch 38/1000\n",
      " - 0s - loss: 0.9914 - accuracy: 0.6484 - val_loss: 0.9160 - val_accuracy: 0.7247\n",
      "Epoch 39/1000\n",
      " - 0s - loss: 0.9562 - accuracy: 0.6526 - val_loss: 0.9114 - val_accuracy: 0.7135\n",
      "Epoch 40/1000\n",
      " - 0s - loss: 0.9612 - accuracy: 0.6554 - val_loss: 0.8968 - val_accuracy: 0.6910\n",
      "Epoch 41/1000\n",
      " - 0s - loss: 0.9191 - accuracy: 0.6934 - val_loss: 0.8860 - val_accuracy: 0.7191\n",
      "Epoch 42/1000\n",
      " - 0s - loss: 0.9144 - accuracy: 0.6681 - val_loss: 0.8973 - val_accuracy: 0.6180\n",
      "Epoch 43/1000\n",
      " - 0s - loss: 0.9121 - accuracy: 0.6667 - val_loss: 0.8620 - val_accuracy: 0.6966\n",
      "Epoch 44/1000\n",
      " - 0s - loss: 0.9467 - accuracy: 0.6470 - val_loss: 0.8567 - val_accuracy: 0.6966\n",
      "Epoch 45/1000\n",
      " - 0s - loss: 0.9148 - accuracy: 0.6596 - val_loss: 0.8738 - val_accuracy: 0.6124\n",
      "Epoch 46/1000\n",
      " - 0s - loss: 0.9138 - accuracy: 0.6259 - val_loss: 0.8649 - val_accuracy: 0.6461\n",
      "Epoch 47/1000\n",
      " - 0s - loss: 0.9084 - accuracy: 0.6484 - val_loss: 0.8404 - val_accuracy: 0.6854\n",
      "Epoch 48/1000\n",
      " - 0s - loss: 0.8966 - accuracy: 0.6554 - val_loss: 0.8338 - val_accuracy: 0.6854\n",
      "Epoch 49/1000\n",
      " - 0s - loss: 0.9139 - accuracy: 0.6526 - val_loss: 0.8325 - val_accuracy: 0.6910\n",
      "Epoch 50/1000\n",
      " - 0s - loss: 0.8681 - accuracy: 0.6667 - val_loss: 0.8301 - val_accuracy: 0.7135\n",
      "Epoch 51/1000\n",
      " - 0s - loss: 0.8760 - accuracy: 0.6653 - val_loss: 0.8166 - val_accuracy: 0.6966\n",
      "Epoch 52/1000\n",
      " - 0s - loss: 0.8576 - accuracy: 0.6582 - val_loss: 0.8151 - val_accuracy: 0.6966\n",
      "Epoch 53/1000\n",
      " - 0s - loss: 0.8596 - accuracy: 0.6610 - val_loss: 0.8163 - val_accuracy: 0.6966\n",
      "Epoch 54/1000\n",
      " - 0s - loss: 0.8674 - accuracy: 0.6385 - val_loss: 0.8228 - val_accuracy: 0.6742\n",
      "Epoch 55/1000\n",
      " - 0s - loss: 0.8366 - accuracy: 0.6639 - val_loss: 0.8006 - val_accuracy: 0.6966\n",
      "Epoch 56/1000\n",
      " - 0s - loss: 0.8445 - accuracy: 0.6624 - val_loss: 0.7876 - val_accuracy: 0.7022\n",
      "Epoch 57/1000\n",
      " - 0s - loss: 0.8277 - accuracy: 0.6709 - val_loss: 0.7874 - val_accuracy: 0.7022\n",
      "Epoch 58/1000\n",
      " - 0s - loss: 0.8053 - accuracy: 0.6667 - val_loss: 0.7843 - val_accuracy: 0.7022\n",
      "Epoch 59/1000\n",
      " - 0s - loss: 0.8148 - accuracy: 0.6653 - val_loss: 0.7746 - val_accuracy: 0.7022\n",
      "Epoch 60/1000\n",
      " - 0s - loss: 0.8191 - accuracy: 0.6568 - val_loss: 0.7737 - val_accuracy: 0.7022\n",
      "Epoch 61/1000\n",
      " - 0s - loss: 0.8102 - accuracy: 0.6596 - val_loss: 0.7713 - val_accuracy: 0.7022\n",
      "Epoch 62/1000\n",
      " - 0s - loss: 0.7874 - accuracy: 0.6892 - val_loss: 0.7640 - val_accuracy: 0.6966\n",
      "Epoch 63/1000\n",
      " - 0s - loss: 0.8074 - accuracy: 0.6751 - val_loss: 0.7629 - val_accuracy: 0.6910\n",
      "Epoch 64/1000\n",
      " - 0s - loss: 0.8017 - accuracy: 0.6695 - val_loss: 0.7601 - val_accuracy: 0.6966\n",
      "Epoch 65/1000\n",
      " - 0s - loss: 0.7932 - accuracy: 0.6765 - val_loss: 0.7596 - val_accuracy: 0.6910\n",
      "Epoch 66/1000\n",
      " - 0s - loss: 0.8068 - accuracy: 0.6681 - val_loss: 0.7577 - val_accuracy: 0.6966\n",
      "Epoch 67/1000\n",
      " - 0s - loss: 0.7788 - accuracy: 0.6681 - val_loss: 0.7526 - val_accuracy: 0.7022\n",
      "Epoch 68/1000\n",
      " - 0s - loss: 0.8033 - accuracy: 0.6681 - val_loss: 0.7462 - val_accuracy: 0.6966\n",
      "Epoch 69/1000\n",
      " - 0s - loss: 0.7946 - accuracy: 0.6526 - val_loss: 0.7438 - val_accuracy: 0.6910\n",
      "Epoch 70/1000\n",
      " - 0s - loss: 0.7716 - accuracy: 0.6653 - val_loss: 0.7410 - val_accuracy: 0.7022\n",
      "Epoch 71/1000\n",
      " - 0s - loss: 0.7724 - accuracy: 0.6765 - val_loss: 0.7423 - val_accuracy: 0.7022\n",
      "Epoch 72/1000\n",
      " - 0s - loss: 0.7827 - accuracy: 0.6681 - val_loss: 0.7408 - val_accuracy: 0.6854\n",
      "Epoch 73/1000\n",
      " - 0s - loss: 0.7807 - accuracy: 0.6695 - val_loss: 0.7359 - val_accuracy: 0.7022\n",
      "Epoch 74/1000\n",
      " - 0s - loss: 0.7870 - accuracy: 0.6512 - val_loss: 0.7391 - val_accuracy: 0.7022\n",
      "Epoch 75/1000\n",
      " - 0s - loss: 0.7800 - accuracy: 0.6568 - val_loss: 0.7310 - val_accuracy: 0.7022\n",
      "Epoch 76/1000\n",
      " - 0s - loss: 0.7656 - accuracy: 0.6807 - val_loss: 0.7233 - val_accuracy: 0.6854\n",
      "Epoch 77/1000\n",
      " - 0s - loss: 0.7557 - accuracy: 0.6639 - val_loss: 0.7212 - val_accuracy: 0.7022\n",
      "Epoch 78/1000\n",
      " - 0s - loss: 0.7418 - accuracy: 0.6737 - val_loss: 0.7128 - val_accuracy: 0.6854\n",
      "Epoch 79/1000\n",
      " - 0s - loss: 0.7496 - accuracy: 0.6850 - val_loss: 0.7096 - val_accuracy: 0.7022\n",
      "Epoch 80/1000\n",
      " - 0s - loss: 0.7568 - accuracy: 0.6765 - val_loss: 0.7140 - val_accuracy: 0.6966\n",
      "Epoch 81/1000\n",
      " - 0s - loss: 0.7432 - accuracy: 0.6934 - val_loss: 0.7180 - val_accuracy: 0.6854\n",
      "Epoch 82/1000\n",
      " - 0s - loss: 0.7468 - accuracy: 0.6695 - val_loss: 0.7104 - val_accuracy: 0.6910\n",
      "Epoch 83/1000\n",
      " - 0s - loss: 0.7365 - accuracy: 0.6850 - val_loss: 0.7017 - val_accuracy: 0.6910\n",
      "Epoch 84/1000\n",
      " - 0s - loss: 0.7338 - accuracy: 0.6864 - val_loss: 0.7033 - val_accuracy: 0.6966\n",
      "Epoch 85/1000\n",
      " - 0s - loss: 0.7604 - accuracy: 0.6709 - val_loss: 0.6993 - val_accuracy: 0.6910\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/1000\n",
      " - 0s - loss: 0.7338 - accuracy: 0.6793 - val_loss: 0.7031 - val_accuracy: 0.6966\n",
      "Epoch 87/1000\n",
      " - 0s - loss: 0.7389 - accuracy: 0.6793 - val_loss: 0.7010 - val_accuracy: 0.6910\n",
      "Epoch 88/1000\n",
      " - 0s - loss: 0.7283 - accuracy: 0.6892 - val_loss: 0.7017 - val_accuracy: 0.7022\n",
      "Epoch 89/1000\n",
      " - 0s - loss: 0.7074 - accuracy: 0.6779 - val_loss: 0.6977 - val_accuracy: 0.7022\n",
      "Epoch 90/1000\n",
      " - 0s - loss: 0.7272 - accuracy: 0.6737 - val_loss: 0.6985 - val_accuracy: 0.7079\n",
      "Epoch 91/1000\n",
      " - 0s - loss: 0.7183 - accuracy: 0.6737 - val_loss: 0.7057 - val_accuracy: 0.6910\n",
      "Epoch 92/1000\n",
      " - 0s - loss: 0.7196 - accuracy: 0.6610 - val_loss: 0.6957 - val_accuracy: 0.7022\n",
      "Epoch 93/1000\n",
      " - 0s - loss: 0.7234 - accuracy: 0.6695 - val_loss: 0.6888 - val_accuracy: 0.7022\n",
      "Epoch 94/1000\n",
      " - 0s - loss: 0.7044 - accuracy: 0.6835 - val_loss: 0.6843 - val_accuracy: 0.6966\n",
      "Epoch 95/1000\n",
      " - 0s - loss: 0.7091 - accuracy: 0.6835 - val_loss: 0.6771 - val_accuracy: 0.6966\n",
      "Epoch 96/1000\n",
      " - 0s - loss: 0.7159 - accuracy: 0.6709 - val_loss: 0.6771 - val_accuracy: 0.6910\n",
      "Epoch 97/1000\n",
      " - 0s - loss: 0.7210 - accuracy: 0.6667 - val_loss: 0.6794 - val_accuracy: 0.6910\n",
      "Epoch 98/1000\n",
      " - 0s - loss: 0.7032 - accuracy: 0.6596 - val_loss: 0.6749 - val_accuracy: 0.6854\n",
      "Epoch 99/1000\n",
      " - 0s - loss: 0.7339 - accuracy: 0.6667 - val_loss: 0.6760 - val_accuracy: 0.6910\n",
      "Epoch 100/1000\n",
      " - 0s - loss: 0.7065 - accuracy: 0.6695 - val_loss: 0.6738 - val_accuracy: 0.6966\n",
      "Epoch 101/1000\n",
      " - 0s - loss: 0.7034 - accuracy: 0.6709 - val_loss: 0.6758 - val_accuracy: 0.7022\n",
      "Epoch 102/1000\n",
      " - 0s - loss: 0.6960 - accuracy: 0.6751 - val_loss: 0.6649 - val_accuracy: 0.7022\n",
      "Epoch 103/1000\n",
      " - 0s - loss: 0.7032 - accuracy: 0.6765 - val_loss: 0.6628 - val_accuracy: 0.6966\n",
      "Epoch 104/1000\n",
      " - 0s - loss: 0.7010 - accuracy: 0.6793 - val_loss: 0.6704 - val_accuracy: 0.7022\n",
      "Epoch 105/1000\n",
      " - 0s - loss: 0.7125 - accuracy: 0.6498 - val_loss: 0.6784 - val_accuracy: 0.6910\n",
      "Epoch 106/1000\n",
      " - 0s - loss: 0.6869 - accuracy: 0.6821 - val_loss: 0.6678 - val_accuracy: 0.6910\n",
      "Epoch 107/1000\n",
      " - 0s - loss: 0.6883 - accuracy: 0.6737 - val_loss: 0.6631 - val_accuracy: 0.6910\n",
      "Epoch 108/1000\n",
      " - 0s - loss: 0.6938 - accuracy: 0.6807 - val_loss: 0.6592 - val_accuracy: 0.6910\n",
      "Epoch 109/1000\n",
      " - 0s - loss: 0.7002 - accuracy: 0.6681 - val_loss: 0.6600 - val_accuracy: 0.6910\n",
      "Epoch 110/1000\n",
      " - 0s - loss: 0.6845 - accuracy: 0.6793 - val_loss: 0.6568 - val_accuracy: 0.6854\n",
      "Epoch 111/1000\n",
      " - 0s - loss: 0.6875 - accuracy: 0.6723 - val_loss: 0.6528 - val_accuracy: 0.6966\n",
      "Epoch 112/1000\n",
      " - 0s - loss: 0.6904 - accuracy: 0.6878 - val_loss: 0.6486 - val_accuracy: 0.6966\n",
      "Epoch 113/1000\n",
      " - 0s - loss: 0.6866 - accuracy: 0.6751 - val_loss: 0.6476 - val_accuracy: 0.6854\n",
      "Epoch 114/1000\n",
      " - 0s - loss: 0.6832 - accuracy: 0.6709 - val_loss: 0.6483 - val_accuracy: 0.7022\n",
      "Epoch 115/1000\n",
      " - 0s - loss: 0.6818 - accuracy: 0.6779 - val_loss: 0.6477 - val_accuracy: 0.6854\n",
      "Epoch 116/1000\n",
      " - 0s - loss: 0.6853 - accuracy: 0.6765 - val_loss: 0.6461 - val_accuracy: 0.6966\n",
      "Epoch 117/1000\n",
      " - 0s - loss: 0.6755 - accuracy: 0.6793 - val_loss: 0.6454 - val_accuracy: 0.6910\n",
      "Epoch 118/1000\n",
      " - 0s - loss: 0.6766 - accuracy: 0.6850 - val_loss: 0.6447 - val_accuracy: 0.6910\n",
      "Epoch 119/1000\n",
      " - 0s - loss: 0.6756 - accuracy: 0.6920 - val_loss: 0.6451 - val_accuracy: 0.6910\n",
      "Epoch 120/1000\n",
      " - 0s - loss: 0.6831 - accuracy: 0.6723 - val_loss: 0.6547 - val_accuracy: 0.6854\n",
      "Epoch 121/1000\n",
      " - 0s - loss: 0.6598 - accuracy: 0.6906 - val_loss: 0.6480 - val_accuracy: 0.6966\n",
      "Epoch 122/1000\n",
      " - 0s - loss: 0.6825 - accuracy: 0.6751 - val_loss: 0.6395 - val_accuracy: 0.7022\n",
      "Epoch 123/1000\n",
      " - 0s - loss: 0.6610 - accuracy: 0.6934 - val_loss: 0.6361 - val_accuracy: 0.6854\n",
      "Epoch 124/1000\n",
      " - 0s - loss: 0.6722 - accuracy: 0.6878 - val_loss: 0.6384 - val_accuracy: 0.6910\n",
      "Epoch 125/1000\n",
      " - 0s - loss: 0.6630 - accuracy: 0.6737 - val_loss: 0.6379 - val_accuracy: 0.6966\n",
      "Epoch 126/1000\n",
      " - 0s - loss: 0.6531 - accuracy: 0.6779 - val_loss: 0.6449 - val_accuracy: 0.6854\n",
      "Epoch 127/1000\n",
      " - 0s - loss: 0.6758 - accuracy: 0.6751 - val_loss: 0.6485 - val_accuracy: 0.6910\n",
      "Epoch 128/1000\n",
      " - 0s - loss: 0.6631 - accuracy: 0.6807 - val_loss: 0.6361 - val_accuracy: 0.6910\n",
      "Epoch 129/1000\n",
      " - 0s - loss: 0.6647 - accuracy: 0.6793 - val_loss: 0.6344 - val_accuracy: 0.6966\n",
      "Epoch 130/1000\n",
      " - 0s - loss: 0.6533 - accuracy: 0.6751 - val_loss: 0.6293 - val_accuracy: 0.6910\n",
      "Epoch 131/1000\n",
      " - 0s - loss: 0.6564 - accuracy: 0.6892 - val_loss: 0.6321 - val_accuracy: 0.6966\n",
      "Epoch 132/1000\n",
      " - 0s - loss: 0.6517 - accuracy: 0.6835 - val_loss: 0.6283 - val_accuracy: 0.6854\n",
      "Epoch 133/1000\n",
      " - 0s - loss: 0.6553 - accuracy: 0.6906 - val_loss: 0.6272 - val_accuracy: 0.6910\n",
      "Epoch 134/1000\n",
      " - 0s - loss: 0.6533 - accuracy: 0.6835 - val_loss: 0.6310 - val_accuracy: 0.6966\n",
      "Epoch 135/1000\n",
      " - 0s - loss: 0.6538 - accuracy: 0.6779 - val_loss: 0.6291 - val_accuracy: 0.6966\n",
      "Epoch 136/1000\n",
      " - 0s - loss: 0.6475 - accuracy: 0.7032 - val_loss: 0.6273 - val_accuracy: 0.6966\n",
      "Epoch 137/1000\n",
      " - 0s - loss: 0.6314 - accuracy: 0.6962 - val_loss: 0.6215 - val_accuracy: 0.7022\n",
      "Epoch 138/1000\n",
      " - 0s - loss: 0.6418 - accuracy: 0.6920 - val_loss: 0.6185 - val_accuracy: 0.6910\n",
      "Epoch 139/1000\n",
      " - 0s - loss: 0.6485 - accuracy: 0.6934 - val_loss: 0.6189 - val_accuracy: 0.6966\n",
      "Epoch 140/1000\n",
      " - 0s - loss: 0.6461 - accuracy: 0.6920 - val_loss: 0.6218 - val_accuracy: 0.6966\n",
      "Epoch 141/1000\n",
      " - 0s - loss: 0.6546 - accuracy: 0.6681 - val_loss: 0.6376 - val_accuracy: 0.6685\n",
      "Epoch 142/1000\n",
      " - 0s - loss: 0.6513 - accuracy: 0.6948 - val_loss: 0.6288 - val_accuracy: 0.6966\n",
      "Epoch 143/1000\n",
      " - 0s - loss: 0.6335 - accuracy: 0.6892 - val_loss: 0.6169 - val_accuracy: 0.6966\n",
      "Epoch 144/1000\n",
      " - 0s - loss: 0.6311 - accuracy: 0.6807 - val_loss: 0.6125 - val_accuracy: 0.6966\n",
      "Epoch 145/1000\n",
      " - 0s - loss: 0.6314 - accuracy: 0.7046 - val_loss: 0.6112 - val_accuracy: 0.7022\n",
      "Epoch 146/1000\n",
      " - 0s - loss: 0.6434 - accuracy: 0.6807 - val_loss: 0.6224 - val_accuracy: 0.6910\n",
      "Epoch 147/1000\n",
      " - 0s - loss: 0.6347 - accuracy: 0.6807 - val_loss: 0.6272 - val_accuracy: 0.6910\n",
      "Epoch 148/1000\n",
      " - 0s - loss: 0.6181 - accuracy: 0.7032 - val_loss: 0.6146 - val_accuracy: 0.7135\n",
      "Epoch 149/1000\n",
      " - 0s - loss: 0.6384 - accuracy: 0.6779 - val_loss: 0.6172 - val_accuracy: 0.6966\n",
      "Epoch 150/1000\n",
      " - 0s - loss: 0.6394 - accuracy: 0.6765 - val_loss: 0.6168 - val_accuracy: 0.7022\n",
      "Epoch 151/1000\n",
      " - 0s - loss: 0.6334 - accuracy: 0.6835 - val_loss: 0.6124 - val_accuracy: 0.7135\n",
      "Epoch 152/1000\n",
      " - 0s - loss: 0.6092 - accuracy: 0.7117 - val_loss: 0.6064 - val_accuracy: 0.7135\n",
      "Epoch 153/1000\n",
      " - 0s - loss: 0.6171 - accuracy: 0.7032 - val_loss: 0.6046 - val_accuracy: 0.7135\n",
      "Epoch 154/1000\n",
      " - 0s - loss: 0.6333 - accuracy: 0.6962 - val_loss: 0.6049 - val_accuracy: 0.7022\n",
      "Epoch 155/1000\n",
      " - 0s - loss: 0.6312 - accuracy: 0.6892 - val_loss: 0.6071 - val_accuracy: 0.7135\n",
      "Epoch 156/1000\n",
      " - 0s - loss: 0.6307 - accuracy: 0.6892 - val_loss: 0.6046 - val_accuracy: 0.6910\n",
      "Epoch 157/1000\n",
      " - 0s - loss: 0.6290 - accuracy: 0.6779 - val_loss: 0.5997 - val_accuracy: 0.7191\n",
      "Epoch 158/1000\n",
      " - 0s - loss: 0.6412 - accuracy: 0.6878 - val_loss: 0.6058 - val_accuracy: 0.7135\n",
      "Epoch 159/1000\n",
      " - 0s - loss: 0.6419 - accuracy: 0.6723 - val_loss: 0.5944 - val_accuracy: 0.7022\n",
      "Epoch 160/1000\n",
      " - 0s - loss: 0.6142 - accuracy: 0.6962 - val_loss: 0.5949 - val_accuracy: 0.7191\n",
      "Epoch 161/1000\n",
      " - 0s - loss: 0.6352 - accuracy: 0.6850 - val_loss: 0.5946 - val_accuracy: 0.7135\n",
      "Epoch 162/1000\n",
      " - 0s - loss: 0.6126 - accuracy: 0.7018 - val_loss: 0.5928 - val_accuracy: 0.7022\n",
      "Epoch 163/1000\n",
      " - 0s - loss: 0.6132 - accuracy: 0.7018 - val_loss: 0.5923 - val_accuracy: 0.7079\n",
      "Epoch 164/1000\n",
      " - 0s - loss: 0.6145 - accuracy: 0.6934 - val_loss: 0.5964 - val_accuracy: 0.7247\n",
      "Epoch 165/1000\n",
      " - 0s - loss: 0.6168 - accuracy: 0.6990 - val_loss: 0.5890 - val_accuracy: 0.7022\n",
      "Epoch 166/1000\n",
      " - 0s - loss: 0.6237 - accuracy: 0.6793 - val_loss: 0.5878 - val_accuracy: 0.7079\n",
      "Epoch 167/1000\n",
      " - 0s - loss: 0.6196 - accuracy: 0.6821 - val_loss: 0.5949 - val_accuracy: 0.7079\n",
      "Epoch 168/1000\n",
      " - 0s - loss: 0.6171 - accuracy: 0.7131 - val_loss: 0.5913 - val_accuracy: 0.7247\n",
      "Epoch 169/1000\n",
      " - 0s - loss: 0.6206 - accuracy: 0.6990 - val_loss: 0.5914 - val_accuracy: 0.7079\n",
      "Epoch 170/1000\n",
      " - 0s - loss: 0.6192 - accuracy: 0.6850 - val_loss: 0.5920 - val_accuracy: 0.7022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 171/1000\n",
      " - 0s - loss: 0.6056 - accuracy: 0.7046 - val_loss: 0.5895 - val_accuracy: 0.7303\n",
      "Epoch 172/1000\n",
      " - 0s - loss: 0.6178 - accuracy: 0.6934 - val_loss: 0.5876 - val_accuracy: 0.7079\n",
      "Epoch 173/1000\n",
      " - 0s - loss: 0.5955 - accuracy: 0.7145 - val_loss: 0.5868 - val_accuracy: 0.7135\n",
      "Epoch 174/1000\n",
      " - 0s - loss: 0.5959 - accuracy: 0.7046 - val_loss: 0.5787 - val_accuracy: 0.7135\n",
      "Epoch 175/1000\n",
      " - 0s - loss: 0.5977 - accuracy: 0.6990 - val_loss: 0.5769 - val_accuracy: 0.7079\n",
      "Epoch 176/1000\n",
      " - 0s - loss: 0.5794 - accuracy: 0.7342 - val_loss: 0.5774 - val_accuracy: 0.7303\n",
      "Epoch 177/1000\n",
      " - 0s - loss: 0.6009 - accuracy: 0.7004 - val_loss: 0.5774 - val_accuracy: 0.7360\n",
      "Epoch 178/1000\n",
      " - 0s - loss: 0.6307 - accuracy: 0.6850 - val_loss: 0.5741 - val_accuracy: 0.7303\n",
      "Epoch 179/1000\n",
      " - 0s - loss: 0.6184 - accuracy: 0.7032 - val_loss: 0.5773 - val_accuracy: 0.7416\n",
      "Epoch 180/1000\n",
      " - 0s - loss: 0.6078 - accuracy: 0.7286 - val_loss: 0.5751 - val_accuracy: 0.7416\n",
      "Epoch 181/1000\n",
      " - 0s - loss: 0.5920 - accuracy: 0.7229 - val_loss: 0.5776 - val_accuracy: 0.7079\n",
      "Epoch 182/1000\n",
      " - 0s - loss: 0.5852 - accuracy: 0.7173 - val_loss: 0.5842 - val_accuracy: 0.7360\n",
      "Epoch 183/1000\n",
      " - 0s - loss: 0.6079 - accuracy: 0.7159 - val_loss: 0.5864 - val_accuracy: 0.7303\n",
      "Epoch 184/1000\n",
      " - 0s - loss: 0.5962 - accuracy: 0.7046 - val_loss: 0.5846 - val_accuracy: 0.7079\n",
      "Epoch 185/1000\n",
      " - 0s - loss: 0.5909 - accuracy: 0.7257 - val_loss: 0.5680 - val_accuracy: 0.7360\n",
      "Epoch 186/1000\n",
      " - 0s - loss: 0.5876 - accuracy: 0.7187 - val_loss: 0.5639 - val_accuracy: 0.7135\n",
      "Epoch 187/1000\n",
      " - 0s - loss: 0.5974 - accuracy: 0.7173 - val_loss: 0.5676 - val_accuracy: 0.7247\n",
      "Epoch 188/1000\n",
      " - 0s - loss: 0.5815 - accuracy: 0.7342 - val_loss: 0.5695 - val_accuracy: 0.7416\n",
      "Epoch 189/1000\n",
      " - 0s - loss: 0.5946 - accuracy: 0.7215 - val_loss: 0.5674 - val_accuracy: 0.7247\n",
      "Epoch 190/1000\n",
      " - 0s - loss: 0.5819 - accuracy: 0.7229 - val_loss: 0.5635 - val_accuracy: 0.7360\n",
      "Epoch 191/1000\n",
      " - 0s - loss: 0.5938 - accuracy: 0.7131 - val_loss: 0.5590 - val_accuracy: 0.7303\n",
      "Epoch 192/1000\n",
      " - 0s - loss: 0.5980 - accuracy: 0.7004 - val_loss: 0.5575 - val_accuracy: 0.7360\n",
      "Epoch 193/1000\n",
      " - 0s - loss: 0.5755 - accuracy: 0.7257 - val_loss: 0.5583 - val_accuracy: 0.7360\n",
      "Epoch 194/1000\n",
      " - 0s - loss: 0.5806 - accuracy: 0.7300 - val_loss: 0.5643 - val_accuracy: 0.7416\n",
      "Epoch 195/1000\n",
      " - 0s - loss: 0.5891 - accuracy: 0.7342 - val_loss: 0.5576 - val_accuracy: 0.7247\n",
      "Epoch 196/1000\n",
      " - 0s - loss: 0.5659 - accuracy: 0.7398 - val_loss: 0.5555 - val_accuracy: 0.7247\n",
      "Epoch 197/1000\n",
      " - 0s - loss: 0.5895 - accuracy: 0.7187 - val_loss: 0.5621 - val_accuracy: 0.7416\n",
      "Epoch 198/1000\n",
      " - 0s - loss: 0.5832 - accuracy: 0.7356 - val_loss: 0.5571 - val_accuracy: 0.7247\n",
      "Epoch 199/1000\n",
      " - 0s - loss: 0.5796 - accuracy: 0.7356 - val_loss: 0.5542 - val_accuracy: 0.7303\n",
      "Epoch 200/1000\n",
      " - 0s - loss: 0.5696 - accuracy: 0.7286 - val_loss: 0.5654 - val_accuracy: 0.7472\n",
      "Epoch 201/1000\n",
      " - 0s - loss: 0.6066 - accuracy: 0.7117 - val_loss: 0.5554 - val_accuracy: 0.7528\n",
      "Epoch 202/1000\n",
      " - 0s - loss: 0.5922 - accuracy: 0.7187 - val_loss: 0.5816 - val_accuracy: 0.7640\n",
      "Epoch 203/1000\n",
      " - 0s - loss: 0.5807 - accuracy: 0.7187 - val_loss: 0.5730 - val_accuracy: 0.7472\n",
      "Epoch 204/1000\n",
      " - 0s - loss: 0.5851 - accuracy: 0.7356 - val_loss: 0.5648 - val_accuracy: 0.7472\n",
      "Epoch 205/1000\n",
      " - 0s - loss: 0.5944 - accuracy: 0.7300 - val_loss: 0.5556 - val_accuracy: 0.7472\n",
      "Epoch 206/1000\n",
      " - 0s - loss: 0.5809 - accuracy: 0.7271 - val_loss: 0.5658 - val_accuracy: 0.7079\n",
      "Epoch 207/1000\n",
      " - 0s - loss: 0.5681 - accuracy: 0.7454 - val_loss: 0.5550 - val_accuracy: 0.7584\n",
      "Epoch 208/1000\n",
      " - 0s - loss: 0.5798 - accuracy: 0.7398 - val_loss: 0.5484 - val_accuracy: 0.7472\n",
      "Epoch 209/1000\n",
      " - 0s - loss: 0.5695 - accuracy: 0.7328 - val_loss: 0.5479 - val_accuracy: 0.7528\n",
      "Epoch 210/1000\n",
      " - 0s - loss: 0.5677 - accuracy: 0.7370 - val_loss: 0.5489 - val_accuracy: 0.7584\n",
      "Epoch 211/1000\n",
      " - 0s - loss: 0.5635 - accuracy: 0.7412 - val_loss: 0.5478 - val_accuracy: 0.7697\n",
      "Epoch 212/1000\n",
      " - 0s - loss: 0.5747 - accuracy: 0.7539 - val_loss: 0.5427 - val_accuracy: 0.7472\n",
      "Epoch 213/1000\n",
      " - 0s - loss: 0.5771 - accuracy: 0.7356 - val_loss: 0.5459 - val_accuracy: 0.7640\n",
      "Epoch 214/1000\n",
      " - 0s - loss: 0.5720 - accuracy: 0.7229 - val_loss: 0.5452 - val_accuracy: 0.7584\n",
      "Epoch 215/1000\n",
      " - 0s - loss: 0.5731 - accuracy: 0.7257 - val_loss: 0.5441 - val_accuracy: 0.7584\n",
      "Epoch 216/1000\n",
      " - 0s - loss: 0.5920 - accuracy: 0.7328 - val_loss: 0.5455 - val_accuracy: 0.7584\n",
      "Epoch 217/1000\n",
      " - 0s - loss: 0.5696 - accuracy: 0.7511 - val_loss: 0.5452 - val_accuracy: 0.7584\n",
      "Epoch 218/1000\n",
      " - 0s - loss: 0.5490 - accuracy: 0.7496 - val_loss: 0.5495 - val_accuracy: 0.7472\n",
      "Epoch 219/1000\n",
      " - 0s - loss: 0.5640 - accuracy: 0.7356 - val_loss: 0.5557 - val_accuracy: 0.7472\n",
      "Epoch 220/1000\n",
      " - 0s - loss: 0.5679 - accuracy: 0.7440 - val_loss: 0.5573 - val_accuracy: 0.7584\n",
      "Epoch 221/1000\n",
      " - 0s - loss: 0.5661 - accuracy: 0.7356 - val_loss: 0.5534 - val_accuracy: 0.7528\n",
      "Epoch 222/1000\n",
      " - 0s - loss: 0.5505 - accuracy: 0.7623 - val_loss: 0.5609 - val_accuracy: 0.7247\n",
      "Epoch 223/1000\n",
      " - 0s - loss: 0.5751 - accuracy: 0.7243 - val_loss: 0.5536 - val_accuracy: 0.7528\n",
      "Epoch 224/1000\n",
      " - 0s - loss: 0.5909 - accuracy: 0.7271 - val_loss: 0.5574 - val_accuracy: 0.7472\n",
      "Epoch 225/1000\n",
      " - 0s - loss: 0.5615 - accuracy: 0.7468 - val_loss: 0.5710 - val_accuracy: 0.7247\n",
      "Epoch 226/1000\n",
      " - 0s - loss: 0.5920 - accuracy: 0.7159 - val_loss: 0.5575 - val_accuracy: 0.7360\n",
      "Epoch 227/1000\n",
      " - 0s - loss: 0.5654 - accuracy: 0.7426 - val_loss: 0.5560 - val_accuracy: 0.7584\n",
      "Epoch 228/1000\n",
      " - 0s - loss: 0.5762 - accuracy: 0.7356 - val_loss: 0.5433 - val_accuracy: 0.7360\n",
      "Epoch 229/1000\n",
      " - 0s - loss: 0.5736 - accuracy: 0.7314 - val_loss: 0.5426 - val_accuracy: 0.7472\n",
      "Epoch 230/1000\n",
      " - 0s - loss: 0.5702 - accuracy: 0.7370 - val_loss: 0.5474 - val_accuracy: 0.7640\n",
      "Epoch 231/1000\n",
      " - 0s - loss: 0.5590 - accuracy: 0.7454 - val_loss: 0.5432 - val_accuracy: 0.7809\n",
      "Epoch 232/1000\n",
      " - 0s - loss: 0.5698 - accuracy: 0.7440 - val_loss: 0.5401 - val_accuracy: 0.7528\n",
      "Epoch 233/1000\n",
      " - 0s - loss: 0.5602 - accuracy: 0.7257 - val_loss: 0.5426 - val_accuracy: 0.7640\n",
      "Epoch 234/1000\n",
      " - 0s - loss: 0.5773 - accuracy: 0.7342 - val_loss: 0.5477 - val_accuracy: 0.7584\n",
      "Epoch 235/1000\n",
      " - 0s - loss: 0.5729 - accuracy: 0.7356 - val_loss: 0.5422 - val_accuracy: 0.7528\n",
      "Epoch 236/1000\n",
      " - 0s - loss: 0.5732 - accuracy: 0.7328 - val_loss: 0.5574 - val_accuracy: 0.7584\n",
      "Epoch 237/1000\n",
      " - 0s - loss: 0.5652 - accuracy: 0.7496 - val_loss: 0.5386 - val_accuracy: 0.7528\n",
      "Epoch 238/1000\n",
      " - 0s - loss: 0.5647 - accuracy: 0.7482 - val_loss: 0.5404 - val_accuracy: 0.7472\n",
      "Epoch 239/1000\n",
      " - 0s - loss: 0.5627 - accuracy: 0.7412 - val_loss: 0.5378 - val_accuracy: 0.7753\n",
      "Epoch 240/1000\n",
      " - 0s - loss: 0.5530 - accuracy: 0.7257 - val_loss: 0.5386 - val_accuracy: 0.7528\n",
      "Epoch 241/1000\n",
      " - 0s - loss: 0.5507 - accuracy: 0.7581 - val_loss: 0.5429 - val_accuracy: 0.7640\n",
      "Epoch 242/1000\n",
      " - 0s - loss: 0.5350 - accuracy: 0.7778 - val_loss: 0.5352 - val_accuracy: 0.7528\n",
      "Epoch 243/1000\n",
      " - 0s - loss: 0.5671 - accuracy: 0.7342 - val_loss: 0.5442 - val_accuracy: 0.7416\n",
      "Epoch 244/1000\n",
      " - 0s - loss: 0.5560 - accuracy: 0.7511 - val_loss: 0.5436 - val_accuracy: 0.7584\n",
      "Epoch 245/1000\n",
      " - 0s - loss: 0.5547 - accuracy: 0.7595 - val_loss: 0.5430 - val_accuracy: 0.7809\n",
      "Epoch 246/1000\n",
      " - 0s - loss: 0.5452 - accuracy: 0.7539 - val_loss: 0.5300 - val_accuracy: 0.7584\n",
      "Epoch 247/1000\n",
      " - 0s - loss: 0.5553 - accuracy: 0.7412 - val_loss: 0.5252 - val_accuracy: 0.7472\n",
      "Epoch 248/1000\n",
      " - 0s - loss: 0.5509 - accuracy: 0.7328 - val_loss: 0.5369 - val_accuracy: 0.7753\n",
      "Epoch 249/1000\n",
      " - 0s - loss: 0.5295 - accuracy: 0.7707 - val_loss: 0.5248 - val_accuracy: 0.7584\n",
      "Epoch 250/1000\n",
      " - 0s - loss: 0.5547 - accuracy: 0.7370 - val_loss: 0.5254 - val_accuracy: 0.7753\n",
      "Epoch 251/1000\n",
      " - 0s - loss: 0.5380 - accuracy: 0.7623 - val_loss: 0.5271 - val_accuracy: 0.7753\n",
      "Epoch 252/1000\n",
      " - 0s - loss: 0.5340 - accuracy: 0.7525 - val_loss: 0.5413 - val_accuracy: 0.7584\n",
      "Epoch 253/1000\n",
      " - 0s - loss: 0.5384 - accuracy: 0.7567 - val_loss: 0.5378 - val_accuracy: 0.7753\n",
      "Epoch 254/1000\n",
      " - 0s - loss: 0.5277 - accuracy: 0.7623 - val_loss: 0.5283 - val_accuracy: 0.7809\n",
      "Epoch 255/1000\n",
      " - 0s - loss: 0.5399 - accuracy: 0.7511 - val_loss: 0.5182 - val_accuracy: 0.7528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 256/1000\n",
      " - 0s - loss: 0.5528 - accuracy: 0.7440 - val_loss: 0.5254 - val_accuracy: 0.7697\n",
      "Epoch 257/1000\n",
      " - 0s - loss: 0.5356 - accuracy: 0.7370 - val_loss: 0.5166 - val_accuracy: 0.7921\n",
      "Epoch 258/1000\n",
      " - 0s - loss: 0.5323 - accuracy: 0.7665 - val_loss: 0.5131 - val_accuracy: 0.7809\n",
      "Epoch 259/1000\n",
      " - 0s - loss: 0.5533 - accuracy: 0.7454 - val_loss: 0.5137 - val_accuracy: 0.7640\n",
      "Epoch 260/1000\n",
      " - 0s - loss: 0.5557 - accuracy: 0.7412 - val_loss: 0.5225 - val_accuracy: 0.7753\n",
      "Epoch 261/1000\n",
      " - 0s - loss: 0.5497 - accuracy: 0.7567 - val_loss: 0.5256 - val_accuracy: 0.7697\n",
      "Epoch 262/1000\n",
      " - 0s - loss: 0.5342 - accuracy: 0.7539 - val_loss: 0.5354 - val_accuracy: 0.7753\n",
      "Epoch 263/1000\n",
      " - 0s - loss: 0.5546 - accuracy: 0.7440 - val_loss: 0.5329 - val_accuracy: 0.7640\n",
      "Epoch 264/1000\n",
      " - 0s - loss: 0.5483 - accuracy: 0.7567 - val_loss: 0.5299 - val_accuracy: 0.7753\n",
      "Epoch 265/1000\n",
      " - 0s - loss: 0.5365 - accuracy: 0.7539 - val_loss: 0.5374 - val_accuracy: 0.7472\n",
      "Epoch 266/1000\n",
      " - 0s - loss: 0.5496 - accuracy: 0.7567 - val_loss: 0.5279 - val_accuracy: 0.7753\n",
      "Epoch 267/1000\n",
      " - 0s - loss: 0.5531 - accuracy: 0.7398 - val_loss: 0.5152 - val_accuracy: 0.7978\n",
      "Epoch 268/1000\n",
      " - 0s - loss: 0.5446 - accuracy: 0.7482 - val_loss: 0.5118 - val_accuracy: 0.7865\n",
      "Epoch 269/1000\n",
      " - 0s - loss: 0.5481 - accuracy: 0.7567 - val_loss: 0.5180 - val_accuracy: 0.7865\n",
      "Epoch 270/1000\n",
      " - 0s - loss: 0.5564 - accuracy: 0.7609 - val_loss: 0.5156 - val_accuracy: 0.7865\n",
      "Epoch 271/1000\n",
      " - 0s - loss: 0.5460 - accuracy: 0.7581 - val_loss: 0.5109 - val_accuracy: 0.7640\n",
      "Epoch 272/1000\n",
      " - 0s - loss: 0.5350 - accuracy: 0.7370 - val_loss: 0.5112 - val_accuracy: 0.7753\n",
      "Epoch 273/1000\n",
      " - 0s - loss: 0.5341 - accuracy: 0.7539 - val_loss: 0.5091 - val_accuracy: 0.7697\n",
      "Epoch 274/1000\n",
      " - 0s - loss: 0.5286 - accuracy: 0.7539 - val_loss: 0.5141 - val_accuracy: 0.7809\n",
      "Epoch 275/1000\n",
      " - 0s - loss: 0.5331 - accuracy: 0.7665 - val_loss: 0.5172 - val_accuracy: 0.7921\n",
      "Epoch 276/1000\n",
      " - 0s - loss: 0.5397 - accuracy: 0.7609 - val_loss: 0.5109 - val_accuracy: 0.7640\n",
      "Epoch 277/1000\n",
      " - 0s - loss: 0.5465 - accuracy: 0.7454 - val_loss: 0.5119 - val_accuracy: 0.8034\n",
      "Epoch 278/1000\n",
      " - 0s - loss: 0.5303 - accuracy: 0.7525 - val_loss: 0.5151 - val_accuracy: 0.7809\n",
      "Epoch 279/1000\n",
      " - 0s - loss: 0.5430 - accuracy: 0.7482 - val_loss: 0.5111 - val_accuracy: 0.7753\n",
      "Epoch 280/1000\n",
      " - 0s - loss: 0.5518 - accuracy: 0.7511 - val_loss: 0.5073 - val_accuracy: 0.8034\n",
      "Epoch 281/1000\n",
      " - 0s - loss: 0.5410 - accuracy: 0.7482 - val_loss: 0.5028 - val_accuracy: 0.8034\n",
      "Epoch 282/1000\n",
      " - 0s - loss: 0.5328 - accuracy: 0.7482 - val_loss: 0.5044 - val_accuracy: 0.7865\n",
      "Epoch 283/1000\n",
      " - 0s - loss: 0.5299 - accuracy: 0.7651 - val_loss: 0.5063 - val_accuracy: 0.7865\n",
      "Epoch 284/1000\n",
      " - 0s - loss: 0.5207 - accuracy: 0.7736 - val_loss: 0.5083 - val_accuracy: 0.7697\n",
      "Epoch 285/1000\n",
      " - 0s - loss: 0.5291 - accuracy: 0.7665 - val_loss: 0.5030 - val_accuracy: 0.7697\n",
      "Epoch 286/1000\n",
      " - 0s - loss: 0.5254 - accuracy: 0.7736 - val_loss: 0.5012 - val_accuracy: 0.7921\n",
      "Epoch 287/1000\n",
      " - 0s - loss: 0.5230 - accuracy: 0.7722 - val_loss: 0.5016 - val_accuracy: 0.7809\n",
      "Epoch 288/1000\n",
      " - 0s - loss: 0.5485 - accuracy: 0.7496 - val_loss: 0.5016 - val_accuracy: 0.8034\n",
      "Epoch 289/1000\n",
      " - 0s - loss: 0.5502 - accuracy: 0.7511 - val_loss: 0.5018 - val_accuracy: 0.8034\n",
      "Epoch 290/1000\n",
      " - 0s - loss: 0.5482 - accuracy: 0.7539 - val_loss: 0.5039 - val_accuracy: 0.8146\n",
      "Epoch 291/1000\n",
      " - 0s - loss: 0.5356 - accuracy: 0.7679 - val_loss: 0.5032 - val_accuracy: 0.7921\n",
      "Epoch 292/1000\n",
      " - 0s - loss: 0.5236 - accuracy: 0.7750 - val_loss: 0.5055 - val_accuracy: 0.7640\n",
      "Epoch 293/1000\n",
      " - 0s - loss: 0.5376 - accuracy: 0.7482 - val_loss: 0.5032 - val_accuracy: 0.7809\n",
      "Epoch 294/1000\n",
      " - 0s - loss: 0.5317 - accuracy: 0.7496 - val_loss: 0.5039 - val_accuracy: 0.7753\n",
      "Epoch 295/1000\n",
      " - 0s - loss: 0.5397 - accuracy: 0.7511 - val_loss: 0.4994 - val_accuracy: 0.7753\n",
      "Epoch 296/1000\n",
      " - 0s - loss: 0.5155 - accuracy: 0.7707 - val_loss: 0.5021 - val_accuracy: 0.7753\n",
      "Epoch 297/1000\n",
      " - 0s - loss: 0.5294 - accuracy: 0.7454 - val_loss: 0.4982 - val_accuracy: 0.7640\n",
      "Epoch 298/1000\n",
      " - 0s - loss: 0.5160 - accuracy: 0.7482 - val_loss: 0.4982 - val_accuracy: 0.8146\n",
      "Epoch 299/1000\n",
      " - 0s - loss: 0.5414 - accuracy: 0.7581 - val_loss: 0.4972 - val_accuracy: 0.8146\n",
      "Epoch 300/1000\n",
      " - 0s - loss: 0.5506 - accuracy: 0.7398 - val_loss: 0.4934 - val_accuracy: 0.8034\n",
      "Epoch 301/1000\n",
      " - 0s - loss: 0.5199 - accuracy: 0.7679 - val_loss: 0.4942 - val_accuracy: 0.7921\n",
      "Epoch 302/1000\n",
      " - 0s - loss: 0.5298 - accuracy: 0.7525 - val_loss: 0.5015 - val_accuracy: 0.7921\n",
      "Epoch 303/1000\n",
      " - 0s - loss: 0.5407 - accuracy: 0.7271 - val_loss: 0.4981 - val_accuracy: 0.7921\n",
      "Epoch 304/1000\n",
      " - 0s - loss: 0.5466 - accuracy: 0.7496 - val_loss: 0.5245 - val_accuracy: 0.7697\n",
      "Epoch 305/1000\n",
      " - 0s - loss: 0.5279 - accuracy: 0.7539 - val_loss: 0.5104 - val_accuracy: 0.8090\n",
      "Epoch 306/1000\n",
      " - 0s - loss: 0.5356 - accuracy: 0.7679 - val_loss: 0.4955 - val_accuracy: 0.8090\n",
      "Epoch 307/1000\n",
      " - 0s - loss: 0.5271 - accuracy: 0.7581 - val_loss: 0.4951 - val_accuracy: 0.8034\n",
      "Epoch 308/1000\n",
      " - 0s - loss: 0.5086 - accuracy: 0.7848 - val_loss: 0.5000 - val_accuracy: 0.7978\n",
      "Epoch 309/1000\n",
      " - 0s - loss: 0.5290 - accuracy: 0.7581 - val_loss: 0.4951 - val_accuracy: 0.7978\n",
      "Epoch 310/1000\n",
      " - 0s - loss: 0.5148 - accuracy: 0.7736 - val_loss: 0.4935 - val_accuracy: 0.7921\n",
      "Epoch 311/1000\n",
      " - 0s - loss: 0.5379 - accuracy: 0.7243 - val_loss: 0.4979 - val_accuracy: 0.7865\n",
      "Epoch 312/1000\n",
      " - 0s - loss: 0.5352 - accuracy: 0.7693 - val_loss: 0.5172 - val_accuracy: 0.7809\n",
      "Epoch 313/1000\n",
      " - 0s - loss: 0.5231 - accuracy: 0.7792 - val_loss: 0.5006 - val_accuracy: 0.7809\n",
      "Epoch 314/1000\n",
      " - 0s - loss: 0.5203 - accuracy: 0.7651 - val_loss: 0.4988 - val_accuracy: 0.7865\n",
      "Epoch 315/1000\n",
      " - 0s - loss: 0.5266 - accuracy: 0.7581 - val_loss: 0.5050 - val_accuracy: 0.7865\n",
      "Epoch 316/1000\n",
      " - 0s - loss: 0.5422 - accuracy: 0.7567 - val_loss: 0.4966 - val_accuracy: 0.8090\n",
      "Epoch 317/1000\n",
      " - 0s - loss: 0.5599 - accuracy: 0.7581 - val_loss: 0.4966 - val_accuracy: 0.8202\n",
      "Epoch 318/1000\n",
      " - 0s - loss: 0.5378 - accuracy: 0.7426 - val_loss: 0.5108 - val_accuracy: 0.7809\n",
      "Epoch 319/1000\n",
      " - 0s - loss: 0.5316 - accuracy: 0.7539 - val_loss: 0.4971 - val_accuracy: 0.8146\n",
      "Epoch 320/1000\n",
      " - 0s - loss: 0.5238 - accuracy: 0.7651 - val_loss: 0.5001 - val_accuracy: 0.8202\n",
      "Epoch 321/1000\n",
      " - 0s - loss: 0.5197 - accuracy: 0.7736 - val_loss: 0.4925 - val_accuracy: 0.8034\n",
      "Epoch 322/1000\n",
      " - 0s - loss: 0.5141 - accuracy: 0.7736 - val_loss: 0.4901 - val_accuracy: 0.8034\n",
      "Epoch 323/1000\n",
      " - 0s - loss: 0.5217 - accuracy: 0.7553 - val_loss: 0.4947 - val_accuracy: 0.8258\n",
      "Epoch 324/1000\n",
      " - 0s - loss: 0.5348 - accuracy: 0.7412 - val_loss: 0.5089 - val_accuracy: 0.8146\n",
      "Epoch 325/1000\n",
      " - 0s - loss: 0.5220 - accuracy: 0.7623 - val_loss: 0.5066 - val_accuracy: 0.7809\n",
      "Epoch 326/1000\n",
      " - 0s - loss: 0.5445 - accuracy: 0.7482 - val_loss: 0.5012 - val_accuracy: 0.7753\n",
      "Epoch 327/1000\n",
      " - 0s - loss: 0.5310 - accuracy: 0.7567 - val_loss: 0.4906 - val_accuracy: 0.8034\n",
      "Epoch 328/1000\n",
      " - 0s - loss: 0.5381 - accuracy: 0.7595 - val_loss: 0.5029 - val_accuracy: 0.8258\n",
      "Epoch 329/1000\n",
      " - 0s - loss: 0.5050 - accuracy: 0.7904 - val_loss: 0.4974 - val_accuracy: 0.8258\n",
      "Epoch 330/1000\n",
      " - 0s - loss: 0.5192 - accuracy: 0.7651 - val_loss: 0.5003 - val_accuracy: 0.7921\n",
      "Epoch 331/1000\n",
      " - 0s - loss: 0.5239 - accuracy: 0.7440 - val_loss: 0.4923 - val_accuracy: 0.7865\n",
      "Epoch 332/1000\n",
      " - 0s - loss: 0.5368 - accuracy: 0.7539 - val_loss: 0.4915 - val_accuracy: 0.8090\n",
      "Epoch 333/1000\n",
      " - 0s - loss: 0.5129 - accuracy: 0.7637 - val_loss: 0.4927 - val_accuracy: 0.7921\n",
      "Epoch 334/1000\n",
      " - 0s - loss: 0.5265 - accuracy: 0.7440 - val_loss: 0.4927 - val_accuracy: 0.8202\n",
      "Epoch 335/1000\n",
      " - 0s - loss: 0.5208 - accuracy: 0.7511 - val_loss: 0.5032 - val_accuracy: 0.7978\n",
      "Epoch 336/1000\n",
      " - 0s - loss: 0.5420 - accuracy: 0.7539 - val_loss: 0.4892 - val_accuracy: 0.8090\n",
      "Epoch 337/1000\n",
      " - 0s - loss: 0.5327 - accuracy: 0.7707 - val_loss: 0.4921 - val_accuracy: 0.7865\n",
      "Epoch 338/1000\n",
      " - 0s - loss: 0.5344 - accuracy: 0.7665 - val_loss: 0.5069 - val_accuracy: 0.7753\n",
      "Epoch 339/1000\n",
      " - 0s - loss: 0.5360 - accuracy: 0.7454 - val_loss: 0.4950 - val_accuracy: 0.8315\n",
      "Epoch 340/1000\n",
      " - 0s - loss: 0.5297 - accuracy: 0.7567 - val_loss: 0.4988 - val_accuracy: 0.7921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 341/1000\n",
      " - 0s - loss: 0.5105 - accuracy: 0.7764 - val_loss: 0.4909 - val_accuracy: 0.8258\n",
      "Epoch 342/1000\n",
      " - 0s - loss: 0.5245 - accuracy: 0.7834 - val_loss: 0.5022 - val_accuracy: 0.8034\n",
      "Epoch 343/1000\n",
      " - 0s - loss: 0.5324 - accuracy: 0.7511 - val_loss: 0.4958 - val_accuracy: 0.7809\n",
      "Epoch 344/1000\n",
      " - 0s - loss: 0.5020 - accuracy: 0.7778 - val_loss: 0.4980 - val_accuracy: 0.7809\n",
      "Epoch 345/1000\n",
      " - 0s - loss: 0.5199 - accuracy: 0.7707 - val_loss: 0.4913 - val_accuracy: 0.8034\n",
      "Epoch 346/1000\n",
      " - 0s - loss: 0.5177 - accuracy: 0.7736 - val_loss: 0.4942 - val_accuracy: 0.8258\n",
      "Epoch 347/1000\n",
      " - 0s - loss: 0.5060 - accuracy: 0.7679 - val_loss: 0.5071 - val_accuracy: 0.7865\n",
      "Epoch 348/1000\n",
      " - 0s - loss: 0.5144 - accuracy: 0.7750 - val_loss: 0.4933 - val_accuracy: 0.7978\n",
      "Epoch 349/1000\n",
      " - 0s - loss: 0.5157 - accuracy: 0.7637 - val_loss: 0.4890 - val_accuracy: 0.8258\n",
      "Epoch 350/1000\n",
      " - 0s - loss: 0.5025 - accuracy: 0.7834 - val_loss: 0.4869 - val_accuracy: 0.8090\n",
      "Epoch 351/1000\n",
      " - 0s - loss: 0.5114 - accuracy: 0.7820 - val_loss: 0.4963 - val_accuracy: 0.7978\n",
      "Epoch 352/1000\n",
      " - 0s - loss: 0.5283 - accuracy: 0.7553 - val_loss: 0.4971 - val_accuracy: 0.7921\n",
      "Epoch 353/1000\n",
      " - 0s - loss: 0.4970 - accuracy: 0.7778 - val_loss: 0.4904 - val_accuracy: 0.8315\n",
      "Epoch 354/1000\n",
      " - 0s - loss: 0.5325 - accuracy: 0.7595 - val_loss: 0.4901 - val_accuracy: 0.8315\n",
      "Epoch 355/1000\n",
      " - 0s - loss: 0.4969 - accuracy: 0.7665 - val_loss: 0.4920 - val_accuracy: 0.8146\n",
      "Epoch 356/1000\n",
      " - 0s - loss: 0.5318 - accuracy: 0.7567 - val_loss: 0.4911 - val_accuracy: 0.8034\n",
      "Epoch 357/1000\n",
      " - 0s - loss: 0.5174 - accuracy: 0.7637 - val_loss: 0.4900 - val_accuracy: 0.8258\n",
      "Epoch 358/1000\n",
      " - 0s - loss: 0.5149 - accuracy: 0.7693 - val_loss: 0.4892 - val_accuracy: 0.8258\n",
      "Epoch 359/1000\n",
      " - 0s - loss: 0.5027 - accuracy: 0.7848 - val_loss: 0.4905 - val_accuracy: 0.7978\n",
      "Epoch 360/1000\n",
      " - 0s - loss: 0.5189 - accuracy: 0.7553 - val_loss: 0.4955 - val_accuracy: 0.7921\n",
      "Epoch 361/1000\n",
      " - 0s - loss: 0.5266 - accuracy: 0.7398 - val_loss: 0.4822 - val_accuracy: 0.8034\n",
      "Epoch 362/1000\n",
      " - 0s - loss: 0.5112 - accuracy: 0.7707 - val_loss: 0.4834 - val_accuracy: 0.8034\n",
      "Epoch 363/1000\n",
      " - 0s - loss: 0.5214 - accuracy: 0.7553 - val_loss: 0.4845 - val_accuracy: 0.7921\n",
      "Epoch 364/1000\n",
      " - 0s - loss: 0.4959 - accuracy: 0.7736 - val_loss: 0.4910 - val_accuracy: 0.8034\n",
      "Epoch 365/1000\n",
      " - 0s - loss: 0.5194 - accuracy: 0.7651 - val_loss: 0.4904 - val_accuracy: 0.8090\n",
      "Epoch 366/1000\n",
      " - 0s - loss: 0.5022 - accuracy: 0.7679 - val_loss: 0.4912 - val_accuracy: 0.8034\n",
      "Epoch 367/1000\n",
      " - 0s - loss: 0.5123 - accuracy: 0.7707 - val_loss: 0.4883 - val_accuracy: 0.8202\n",
      "Epoch 368/1000\n",
      " - 0s - loss: 0.5011 - accuracy: 0.7764 - val_loss: 0.4856 - val_accuracy: 0.8146\n",
      "Epoch 369/1000\n",
      " - 0s - loss: 0.5166 - accuracy: 0.7539 - val_loss: 0.4834 - val_accuracy: 0.8258\n",
      "Epoch 370/1000\n",
      " - 0s - loss: 0.4933 - accuracy: 0.7890 - val_loss: 0.4818 - val_accuracy: 0.8202\n",
      "Epoch 371/1000\n",
      " - 0s - loss: 0.5220 - accuracy: 0.7511 - val_loss: 0.4874 - val_accuracy: 0.8371\n",
      "Epoch 372/1000\n",
      " - 0s - loss: 0.5251 - accuracy: 0.7637 - val_loss: 0.4877 - val_accuracy: 0.8202\n",
      "Epoch 373/1000\n",
      " - 0s - loss: 0.5002 - accuracy: 0.7778 - val_loss: 0.4829 - val_accuracy: 0.8202\n",
      "Epoch 374/1000\n",
      " - 0s - loss: 0.4952 - accuracy: 0.7904 - val_loss: 0.4819 - val_accuracy: 0.8090\n",
      "Epoch 375/1000\n",
      " - 0s - loss: 0.4999 - accuracy: 0.7778 - val_loss: 0.4869 - val_accuracy: 0.8090\n",
      "Epoch 376/1000\n",
      " - 0s - loss: 0.5294 - accuracy: 0.7468 - val_loss: 0.4900 - val_accuracy: 0.8090\n",
      "Epoch 377/1000\n",
      " - 0s - loss: 0.5007 - accuracy: 0.7595 - val_loss: 0.4884 - val_accuracy: 0.7809\n",
      "Epoch 378/1000\n",
      " - 0s - loss: 0.5320 - accuracy: 0.7539 - val_loss: 0.4937 - val_accuracy: 0.7978\n",
      "Epoch 379/1000\n",
      " - 0s - loss: 0.4912 - accuracy: 0.7792 - val_loss: 0.4911 - val_accuracy: 0.8202\n",
      "Epoch 380/1000\n",
      " - 0s - loss: 0.5277 - accuracy: 0.7553 - val_loss: 0.4916 - val_accuracy: 0.8090\n",
      "Epoch 381/1000\n",
      " - 0s - loss: 0.5141 - accuracy: 0.7511 - val_loss: 0.5061 - val_accuracy: 0.7640\n",
      "Epoch 382/1000\n",
      " - 0s - loss: 0.5111 - accuracy: 0.7511 - val_loss: 0.4931 - val_accuracy: 0.7697\n",
      "Epoch 383/1000\n",
      " - 0s - loss: 0.5222 - accuracy: 0.7539 - val_loss: 0.4860 - val_accuracy: 0.8034\n",
      "Epoch 384/1000\n",
      " - 0s - loss: 0.4890 - accuracy: 0.7693 - val_loss: 0.4902 - val_accuracy: 0.7809\n",
      "Epoch 385/1000\n",
      " - 0s - loss: 0.4992 - accuracy: 0.7890 - val_loss: 0.4868 - val_accuracy: 0.7809\n",
      "Epoch 386/1000\n",
      " - 0s - loss: 0.5020 - accuracy: 0.7722 - val_loss: 0.4799 - val_accuracy: 0.8258\n",
      "Epoch 387/1000\n",
      " - 0s - loss: 0.5123 - accuracy: 0.7792 - val_loss: 0.4816 - val_accuracy: 0.8202\n",
      "Epoch 388/1000\n",
      " - 0s - loss: 0.4913 - accuracy: 0.7707 - val_loss: 0.4846 - val_accuracy: 0.8258\n",
      "Epoch 389/1000\n",
      " - 0s - loss: 0.5008 - accuracy: 0.7722 - val_loss: 0.4820 - val_accuracy: 0.8258\n",
      "Epoch 390/1000\n",
      " - 0s - loss: 0.5261 - accuracy: 0.7595 - val_loss: 0.4847 - val_accuracy: 0.8258\n",
      "Epoch 391/1000\n",
      " - 0s - loss: 0.5057 - accuracy: 0.7834 - val_loss: 0.4809 - val_accuracy: 0.8202\n",
      "Epoch 392/1000\n",
      " - 0s - loss: 0.5134 - accuracy: 0.7595 - val_loss: 0.4856 - val_accuracy: 0.7809\n",
      "Epoch 393/1000\n",
      " - 0s - loss: 0.5047 - accuracy: 0.7609 - val_loss: 0.4826 - val_accuracy: 0.8146\n",
      "Epoch 394/1000\n",
      " - 0s - loss: 0.5236 - accuracy: 0.7595 - val_loss: 0.4869 - val_accuracy: 0.7921\n",
      "Epoch 395/1000\n",
      " - 0s - loss: 0.5083 - accuracy: 0.7722 - val_loss: 0.4842 - val_accuracy: 0.8090\n",
      "Epoch 396/1000\n",
      " - 0s - loss: 0.4913 - accuracy: 0.7862 - val_loss: 0.4839 - val_accuracy: 0.7921\n",
      "Epoch 397/1000\n",
      " - 0s - loss: 0.4846 - accuracy: 0.7736 - val_loss: 0.4875 - val_accuracy: 0.7978\n",
      "Epoch 398/1000\n",
      " - 0s - loss: 0.4994 - accuracy: 0.7665 - val_loss: 0.4926 - val_accuracy: 0.7921\n",
      "Epoch 399/1000\n",
      " - 0s - loss: 0.5065 - accuracy: 0.7707 - val_loss: 0.4841 - val_accuracy: 0.8202\n",
      "Epoch 400/1000\n",
      " - 0s - loss: 0.5030 - accuracy: 0.7693 - val_loss: 0.4844 - val_accuracy: 0.8202\n",
      "Epoch 401/1000\n",
      " - 0s - loss: 0.5158 - accuracy: 0.7679 - val_loss: 0.4992 - val_accuracy: 0.8090\n",
      "Epoch 402/1000\n",
      " - 0s - loss: 0.5186 - accuracy: 0.7539 - val_loss: 0.4868 - val_accuracy: 0.7865\n",
      "Epoch 403/1000\n",
      " - 0s - loss: 0.4917 - accuracy: 0.7736 - val_loss: 0.4835 - val_accuracy: 0.7809\n",
      "Epoch 404/1000\n",
      " - 0s - loss: 0.4879 - accuracy: 0.7736 - val_loss: 0.4806 - val_accuracy: 0.7978\n",
      "Epoch 405/1000\n",
      " - 0s - loss: 0.5157 - accuracy: 0.7665 - val_loss: 0.4809 - val_accuracy: 0.8090\n",
      "Epoch 406/1000\n",
      " - 0s - loss: 0.5160 - accuracy: 0.7623 - val_loss: 0.4790 - val_accuracy: 0.8146\n",
      "Epoch 407/1000\n",
      " - 0s - loss: 0.5081 - accuracy: 0.7778 - val_loss: 0.4785 - val_accuracy: 0.8258\n",
      "Epoch 408/1000\n",
      " - 0s - loss: 0.4976 - accuracy: 0.7904 - val_loss: 0.4798 - val_accuracy: 0.8090\n",
      "Epoch 409/1000\n",
      " - 0s - loss: 0.5047 - accuracy: 0.7778 - val_loss: 0.4950 - val_accuracy: 0.7865\n",
      "Epoch 410/1000\n",
      " - 0s - loss: 0.4963 - accuracy: 0.7707 - val_loss: 0.4817 - val_accuracy: 0.8258\n",
      "Epoch 411/1000\n",
      " - 0s - loss: 0.5120 - accuracy: 0.7651 - val_loss: 0.4866 - val_accuracy: 0.8034\n",
      "Epoch 412/1000\n",
      " - 0s - loss: 0.5072 - accuracy: 0.7693 - val_loss: 0.4832 - val_accuracy: 0.8315\n",
      "Epoch 413/1000\n",
      " - 0s - loss: 0.5099 - accuracy: 0.7707 - val_loss: 0.4898 - val_accuracy: 0.7865\n",
      "Epoch 414/1000\n",
      " - 0s - loss: 0.4921 - accuracy: 0.7862 - val_loss: 0.5032 - val_accuracy: 0.8034\n",
      "Epoch 415/1000\n",
      " - 0s - loss: 0.5057 - accuracy: 0.7722 - val_loss: 0.4845 - val_accuracy: 0.7809\n",
      "Epoch 416/1000\n",
      " - 0s - loss: 0.5136 - accuracy: 0.7412 - val_loss: 0.4842 - val_accuracy: 0.7809\n",
      "Epoch 417/1000\n",
      " - 0s - loss: 0.5040 - accuracy: 0.7750 - val_loss: 0.4810 - val_accuracy: 0.8202\n",
      "Epoch 418/1000\n",
      " - 0s - loss: 0.4819 - accuracy: 0.7876 - val_loss: 0.4840 - val_accuracy: 0.7978\n",
      "Epoch 419/1000\n",
      " - 0s - loss: 0.4964 - accuracy: 0.7876 - val_loss: 0.4853 - val_accuracy: 0.8034\n",
      "Epoch 420/1000\n",
      " - 0s - loss: 0.5166 - accuracy: 0.7482 - val_loss: 0.4803 - val_accuracy: 0.8371\n",
      "Epoch 421/1000\n",
      " - 0s - loss: 0.5160 - accuracy: 0.7609 - val_loss: 0.4875 - val_accuracy: 0.7921\n",
      "Epoch 422/1000\n",
      " - 0s - loss: 0.5197 - accuracy: 0.7623 - val_loss: 0.4806 - val_accuracy: 0.8202\n",
      "Epoch 423/1000\n",
      " - 0s - loss: 0.5174 - accuracy: 0.7736 - val_loss: 0.4803 - val_accuracy: 0.8202\n",
      "Epoch 424/1000\n",
      " - 0s - loss: 0.4934 - accuracy: 0.7806 - val_loss: 0.4962 - val_accuracy: 0.7978\n",
      "Epoch 425/1000\n",
      " - 0s - loss: 0.5150 - accuracy: 0.7567 - val_loss: 0.4849 - val_accuracy: 0.7809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 426/1000\n",
      " - 0s - loss: 0.4965 - accuracy: 0.7764 - val_loss: 0.4821 - val_accuracy: 0.8202\n",
      "Epoch 427/1000\n",
      " - 0s - loss: 0.4847 - accuracy: 0.7834 - val_loss: 0.4858 - val_accuracy: 0.7865\n",
      "Epoch 428/1000\n",
      " - 0s - loss: 0.4976 - accuracy: 0.7623 - val_loss: 0.4812 - val_accuracy: 0.8202\n",
      "Epoch 429/1000\n",
      " - 0s - loss: 0.5021 - accuracy: 0.7722 - val_loss: 0.4807 - val_accuracy: 0.8146\n",
      "Epoch 430/1000\n",
      " - 0s - loss: 0.4904 - accuracy: 0.7834 - val_loss: 0.4804 - val_accuracy: 0.8258\n",
      "Epoch 431/1000\n",
      " - 0s - loss: 0.5162 - accuracy: 0.7707 - val_loss: 0.4862 - val_accuracy: 0.8090\n",
      "Epoch 432/1000\n",
      " - 0s - loss: 0.4900 - accuracy: 0.7904 - val_loss: 0.4803 - val_accuracy: 0.8146\n",
      "Epoch 433/1000\n",
      " - 0s - loss: 0.5008 - accuracy: 0.7637 - val_loss: 0.4801 - val_accuracy: 0.8146\n",
      "Epoch 434/1000\n",
      " - 0s - loss: 0.5047 - accuracy: 0.7623 - val_loss: 0.4803 - val_accuracy: 0.8258\n",
      "Epoch 435/1000\n",
      " - 0s - loss: 0.5020 - accuracy: 0.7792 - val_loss: 0.4820 - val_accuracy: 0.8146\n",
      "Epoch 436/1000\n",
      " - 0s - loss: 0.5040 - accuracy: 0.7736 - val_loss: 0.4790 - val_accuracy: 0.8146\n",
      "Epoch 437/1000\n",
      " - 0s - loss: 0.5081 - accuracy: 0.7862 - val_loss: 0.4839 - val_accuracy: 0.8146\n",
      "Epoch 438/1000\n",
      " - 0s - loss: 0.5028 - accuracy: 0.7778 - val_loss: 0.4851 - val_accuracy: 0.8034\n",
      "Epoch 439/1000\n",
      " - 0s - loss: 0.5118 - accuracy: 0.7665 - val_loss: 0.4924 - val_accuracy: 0.7921\n",
      "Epoch 440/1000\n",
      " - 0s - loss: 0.5007 - accuracy: 0.7651 - val_loss: 0.4793 - val_accuracy: 0.8146\n",
      "Epoch 441/1000\n",
      " - 0s - loss: 0.4949 - accuracy: 0.7764 - val_loss: 0.4784 - val_accuracy: 0.8202\n",
      "Epoch 442/1000\n",
      " - 0s - loss: 0.4865 - accuracy: 0.7792 - val_loss: 0.4796 - val_accuracy: 0.8315\n",
      "Epoch 443/1000\n",
      " - 0s - loss: 0.5067 - accuracy: 0.7693 - val_loss: 0.4813 - val_accuracy: 0.7978\n",
      "Epoch 444/1000\n",
      " - 0s - loss: 0.5239 - accuracy: 0.7609 - val_loss: 0.4922 - val_accuracy: 0.7809\n",
      "Epoch 445/1000\n",
      " - 0s - loss: 0.4953 - accuracy: 0.7679 - val_loss: 0.4787 - val_accuracy: 0.8202\n",
      "Epoch 446/1000\n",
      " - 0s - loss: 0.5229 - accuracy: 0.7609 - val_loss: 0.4937 - val_accuracy: 0.7921\n",
      "Epoch 447/1000\n",
      " - 0s - loss: 0.5135 - accuracy: 0.7736 - val_loss: 0.4796 - val_accuracy: 0.8202\n",
      "Epoch 448/1000\n",
      " - 0s - loss: 0.4841 - accuracy: 0.7989 - val_loss: 0.4837 - val_accuracy: 0.8315\n",
      "Epoch 449/1000\n",
      " - 0s - loss: 0.4986 - accuracy: 0.7764 - val_loss: 0.4920 - val_accuracy: 0.7978\n",
      "Epoch 450/1000\n",
      " - 0s - loss: 0.5084 - accuracy: 0.7496 - val_loss: 0.4841 - val_accuracy: 0.8146\n",
      "Epoch 451/1000\n",
      " - 0s - loss: 0.5061 - accuracy: 0.7553 - val_loss: 0.4781 - val_accuracy: 0.8202\n",
      "Epoch 452/1000\n",
      " - 0s - loss: 0.5086 - accuracy: 0.7693 - val_loss: 0.4797 - val_accuracy: 0.8202\n",
      "Epoch 453/1000\n",
      " - 0s - loss: 0.5063 - accuracy: 0.7778 - val_loss: 0.4782 - val_accuracy: 0.8258\n",
      "Epoch 454/1000\n",
      " - 0s - loss: 0.4825 - accuracy: 0.7947 - val_loss: 0.4798 - val_accuracy: 0.8202\n",
      "Epoch 455/1000\n",
      " - 0s - loss: 0.4820 - accuracy: 0.7918 - val_loss: 0.4744 - val_accuracy: 0.8258\n",
      "Epoch 456/1000\n",
      " - 0s - loss: 0.4981 - accuracy: 0.7834 - val_loss: 0.4749 - val_accuracy: 0.8146\n",
      "Epoch 457/1000\n",
      " - 0s - loss: 0.5041 - accuracy: 0.7609 - val_loss: 0.4837 - val_accuracy: 0.7978\n",
      "Epoch 458/1000\n",
      " - 0s - loss: 0.4891 - accuracy: 0.7665 - val_loss: 0.4831 - val_accuracy: 0.8034\n",
      "Epoch 459/1000\n",
      " - 0s - loss: 0.5087 - accuracy: 0.7651 - val_loss: 0.4800 - val_accuracy: 0.8034\n",
      "Epoch 460/1000\n",
      " - 0s - loss: 0.4905 - accuracy: 0.7806 - val_loss: 0.4770 - val_accuracy: 0.8258\n",
      "Epoch 461/1000\n",
      " - 0s - loss: 0.4985 - accuracy: 0.7637 - val_loss: 0.4763 - val_accuracy: 0.8258\n",
      "Epoch 462/1000\n",
      " - 0s - loss: 0.4910 - accuracy: 0.7820 - val_loss: 0.4764 - val_accuracy: 0.8258\n",
      "Epoch 463/1000\n",
      " - 0s - loss: 0.4781 - accuracy: 0.7961 - val_loss: 0.4800 - val_accuracy: 0.8315\n",
      "Epoch 464/1000\n",
      " - 0s - loss: 0.5095 - accuracy: 0.7722 - val_loss: 0.4866 - val_accuracy: 0.7865\n",
      "Epoch 465/1000\n",
      " - 0s - loss: 0.4850 - accuracy: 0.7722 - val_loss: 0.4759 - val_accuracy: 0.8258\n",
      "Epoch 466/1000\n",
      " - 0s - loss: 0.5173 - accuracy: 0.7637 - val_loss: 0.4751 - val_accuracy: 0.8258\n",
      "Epoch 467/1000\n",
      " - 0s - loss: 0.4970 - accuracy: 0.7722 - val_loss: 0.4951 - val_accuracy: 0.7809\n",
      "Epoch 468/1000\n",
      " - 0s - loss: 0.5079 - accuracy: 0.7764 - val_loss: 0.4826 - val_accuracy: 0.8090\n",
      "Epoch 469/1000\n",
      " - 0s - loss: 0.4952 - accuracy: 0.7778 - val_loss: 0.4811 - val_accuracy: 0.8034\n",
      "Epoch 470/1000\n",
      " - 0s - loss: 0.5145 - accuracy: 0.7651 - val_loss: 0.4767 - val_accuracy: 0.8146\n",
      "Epoch 471/1000\n",
      " - 0s - loss: 0.5017 - accuracy: 0.7595 - val_loss: 0.4778 - val_accuracy: 0.8202\n",
      "Epoch 472/1000\n",
      " - 0s - loss: 0.5026 - accuracy: 0.7806 - val_loss: 0.4810 - val_accuracy: 0.7809\n",
      "Epoch 473/1000\n",
      " - 0s - loss: 0.4886 - accuracy: 0.7862 - val_loss: 0.4754 - val_accuracy: 0.8202\n",
      "Epoch 474/1000\n",
      " - 0s - loss: 0.5144 - accuracy: 0.7665 - val_loss: 0.4846 - val_accuracy: 0.8258\n",
      "Epoch 475/1000\n",
      " - 0s - loss: 0.4829 - accuracy: 0.7890 - val_loss: 0.4809 - val_accuracy: 0.8258\n",
      "Epoch 476/1000\n",
      " - 0s - loss: 0.4850 - accuracy: 0.7722 - val_loss: 0.4741 - val_accuracy: 0.8258\n",
      "Epoch 477/1000\n",
      " - 0s - loss: 0.4981 - accuracy: 0.7834 - val_loss: 0.4782 - val_accuracy: 0.8146\n",
      "Epoch 478/1000\n",
      " - 0s - loss: 0.4957 - accuracy: 0.7792 - val_loss: 0.4734 - val_accuracy: 0.8315\n",
      "Epoch 479/1000\n",
      " - 0s - loss: 0.4960 - accuracy: 0.7750 - val_loss: 0.4767 - val_accuracy: 0.7809\n",
      "Epoch 480/1000\n",
      " - 0s - loss: 0.5196 - accuracy: 0.7693 - val_loss: 0.4737 - val_accuracy: 0.8258\n",
      "Epoch 481/1000\n",
      " - 0s - loss: 0.5008 - accuracy: 0.7651 - val_loss: 0.4809 - val_accuracy: 0.8090\n",
      "Epoch 482/1000\n",
      " - 0s - loss: 0.5121 - accuracy: 0.7581 - val_loss: 0.4892 - val_accuracy: 0.7865\n",
      "Epoch 483/1000\n",
      " - 0s - loss: 0.4934 - accuracy: 0.7637 - val_loss: 0.4784 - val_accuracy: 0.8146\n",
      "Epoch 484/1000\n",
      " - 0s - loss: 0.4826 - accuracy: 0.7792 - val_loss: 0.4737 - val_accuracy: 0.8315\n",
      "Epoch 485/1000\n",
      " - 0s - loss: 0.4798 - accuracy: 0.7806 - val_loss: 0.4792 - val_accuracy: 0.7809\n",
      "Epoch 486/1000\n",
      " - 0s - loss: 0.4876 - accuracy: 0.7609 - val_loss: 0.4783 - val_accuracy: 0.8202\n",
      "Epoch 487/1000\n",
      " - 0s - loss: 0.4951 - accuracy: 0.7764 - val_loss: 0.4814 - val_accuracy: 0.8090\n",
      "Epoch 488/1000\n",
      " - 0s - loss: 0.4751 - accuracy: 0.8129 - val_loss: 0.4767 - val_accuracy: 0.7865\n",
      "Epoch 489/1000\n",
      " - 0s - loss: 0.5014 - accuracy: 0.7581 - val_loss: 0.4707 - val_accuracy: 0.8315\n",
      "Epoch 490/1000\n",
      " - 0s - loss: 0.5240 - accuracy: 0.7567 - val_loss: 0.4743 - val_accuracy: 0.8258\n",
      "Epoch 491/1000\n",
      " - 0s - loss: 0.4949 - accuracy: 0.7848 - val_loss: 0.4837 - val_accuracy: 0.7753\n",
      "Epoch 492/1000\n",
      " - 0s - loss: 0.5084 - accuracy: 0.7609 - val_loss: 0.4765 - val_accuracy: 0.8146\n",
      "Epoch 493/1000\n",
      " - 0s - loss: 0.4933 - accuracy: 0.7722 - val_loss: 0.4674 - val_accuracy: 0.8202\n",
      "Epoch 494/1000\n",
      " - 0s - loss: 0.4779 - accuracy: 0.7904 - val_loss: 0.4658 - val_accuracy: 0.8315\n",
      "Epoch 495/1000\n",
      " - 0s - loss: 0.5031 - accuracy: 0.7693 - val_loss: 0.4692 - val_accuracy: 0.8146\n",
      "Epoch 496/1000\n",
      " - 0s - loss: 0.5032 - accuracy: 0.7637 - val_loss: 0.4772 - val_accuracy: 0.8258\n",
      "Epoch 497/1000\n",
      " - 0s - loss: 0.4912 - accuracy: 0.7806 - val_loss: 0.4711 - val_accuracy: 0.8315\n",
      "Epoch 498/1000\n",
      " - 0s - loss: 0.4747 - accuracy: 0.7961 - val_loss: 0.4703 - val_accuracy: 0.7978\n",
      "Epoch 499/1000\n",
      " - 0s - loss: 0.4840 - accuracy: 0.7862 - val_loss: 0.4651 - val_accuracy: 0.8202\n",
      "Epoch 500/1000\n",
      " - 0s - loss: 0.5079 - accuracy: 0.7707 - val_loss: 0.4660 - val_accuracy: 0.8315\n",
      "Epoch 501/1000\n",
      " - 0s - loss: 0.4754 - accuracy: 0.7806 - val_loss: 0.4727 - val_accuracy: 0.8090\n",
      "Epoch 502/1000\n",
      " - 0s - loss: 0.4727 - accuracy: 0.7918 - val_loss: 0.4735 - val_accuracy: 0.8090\n",
      "Epoch 503/1000\n",
      " - 0s - loss: 0.4783 - accuracy: 0.7904 - val_loss: 0.4688 - val_accuracy: 0.8258\n",
      "Epoch 504/1000\n",
      " - 0s - loss: 0.4938 - accuracy: 0.7764 - val_loss: 0.4709 - val_accuracy: 0.8258\n",
      "Epoch 505/1000\n",
      " - 0s - loss: 0.4808 - accuracy: 0.7736 - val_loss: 0.4753 - val_accuracy: 0.7921\n",
      "Epoch 506/1000\n",
      " - 0s - loss: 0.4746 - accuracy: 0.7961 - val_loss: 0.4732 - val_accuracy: 0.8202\n",
      "Epoch 507/1000\n",
      " - 0s - loss: 0.5013 - accuracy: 0.7722 - val_loss: 0.4783 - val_accuracy: 0.8146\n",
      "Epoch 508/1000\n",
      " - 0s - loss: 0.4864 - accuracy: 0.7764 - val_loss: 0.4753 - val_accuracy: 0.8258\n",
      "Epoch 509/1000\n",
      " - 0s - loss: 0.4830 - accuracy: 0.7890 - val_loss: 0.4791 - val_accuracy: 0.7978\n",
      "Epoch 510/1000\n",
      " - 0s - loss: 0.4731 - accuracy: 0.7961 - val_loss: 0.4694 - val_accuracy: 0.8258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 511/1000\n",
      " - 0s - loss: 0.4771 - accuracy: 0.7750 - val_loss: 0.4714 - val_accuracy: 0.8202\n",
      "Epoch 512/1000\n",
      " - 0s - loss: 0.5017 - accuracy: 0.7722 - val_loss: 0.4756 - val_accuracy: 0.8146\n",
      "Epoch 513/1000\n",
      " - 0s - loss: 0.4857 - accuracy: 0.7862 - val_loss: 0.4788 - val_accuracy: 0.8258\n",
      "Epoch 514/1000\n",
      " - 0s - loss: 0.5028 - accuracy: 0.7876 - val_loss: 0.4763 - val_accuracy: 0.8258\n",
      "Epoch 515/1000\n",
      " - 0s - loss: 0.4869 - accuracy: 0.7876 - val_loss: 0.4736 - val_accuracy: 0.8090\n",
      "Epoch 516/1000\n",
      " - 0s - loss: 0.4723 - accuracy: 0.7904 - val_loss: 0.4696 - val_accuracy: 0.7978\n",
      "Epoch 517/1000\n",
      " - 0s - loss: 0.4951 - accuracy: 0.7975 - val_loss: 0.4679 - val_accuracy: 0.8315\n",
      "Epoch 518/1000\n",
      " - 0s - loss: 0.4927 - accuracy: 0.7581 - val_loss: 0.4720 - val_accuracy: 0.8034\n",
      "Epoch 519/1000\n",
      " - 0s - loss: 0.4918 - accuracy: 0.7637 - val_loss: 0.4697 - val_accuracy: 0.8258\n",
      "Epoch 520/1000\n",
      " - 0s - loss: 0.5022 - accuracy: 0.7736 - val_loss: 0.4713 - val_accuracy: 0.8258\n",
      "Epoch 521/1000\n",
      " - 0s - loss: 0.4752 - accuracy: 0.7989 - val_loss: 0.4729 - val_accuracy: 0.8090\n",
      "Epoch 522/1000\n",
      " - 0s - loss: 0.4878 - accuracy: 0.7834 - val_loss: 0.4737 - val_accuracy: 0.8090\n",
      "Epoch 523/1000\n",
      " - 0s - loss: 0.4684 - accuracy: 0.7862 - val_loss: 0.4710 - val_accuracy: 0.8258\n",
      "Epoch 524/1000\n",
      " - 0s - loss: 0.4697 - accuracy: 0.7862 - val_loss: 0.4655 - val_accuracy: 0.8315\n",
      "Epoch 525/1000\n",
      " - 0s - loss: 0.4933 - accuracy: 0.7989 - val_loss: 0.4712 - val_accuracy: 0.8146\n",
      "Epoch 526/1000\n",
      " - 0s - loss: 0.5064 - accuracy: 0.7707 - val_loss: 0.4696 - val_accuracy: 0.8034\n",
      "Epoch 527/1000\n",
      " - 0s - loss: 0.4856 - accuracy: 0.7651 - val_loss: 0.4685 - val_accuracy: 0.8090\n",
      "Epoch 528/1000\n",
      " - 0s - loss: 0.4744 - accuracy: 0.7876 - val_loss: 0.4695 - val_accuracy: 0.8258\n",
      "Epoch 529/1000\n",
      " - 0s - loss: 0.4809 - accuracy: 0.8031 - val_loss: 0.4728 - val_accuracy: 0.7921\n",
      "Epoch 530/1000\n",
      " - 0s - loss: 0.4976 - accuracy: 0.7637 - val_loss: 0.4701 - val_accuracy: 0.8034\n",
      "Epoch 531/1000\n",
      " - 0s - loss: 0.4734 - accuracy: 0.7975 - val_loss: 0.4693 - val_accuracy: 0.8258\n",
      "Epoch 532/1000\n",
      " - 0s - loss: 0.4920 - accuracy: 0.7862 - val_loss: 0.4717 - val_accuracy: 0.8146\n",
      "Epoch 533/1000\n",
      " - 0s - loss: 0.5044 - accuracy: 0.7848 - val_loss: 0.4787 - val_accuracy: 0.8202\n",
      "Epoch 534/1000\n",
      " - 0s - loss: 0.4865 - accuracy: 0.7947 - val_loss: 0.4746 - val_accuracy: 0.8090\n",
      "Epoch 535/1000\n",
      " - 0s - loss: 0.4871 - accuracy: 0.7848 - val_loss: 0.4713 - val_accuracy: 0.7978\n",
      "Epoch 536/1000\n",
      " - 0s - loss: 0.4977 - accuracy: 0.7707 - val_loss: 0.4706 - val_accuracy: 0.8090\n",
      "Epoch 537/1000\n",
      " - 0s - loss: 0.5074 - accuracy: 0.7750 - val_loss: 0.4726 - val_accuracy: 0.8034\n",
      "Epoch 538/1000\n",
      " - 0s - loss: 0.4973 - accuracy: 0.7637 - val_loss: 0.4720 - val_accuracy: 0.8202\n",
      "Epoch 539/1000\n",
      " - 0s - loss: 0.4834 - accuracy: 0.7904 - val_loss: 0.4696 - val_accuracy: 0.8090\n",
      "Epoch 540/1000\n",
      " - 0s - loss: 0.4855 - accuracy: 0.7764 - val_loss: 0.4698 - val_accuracy: 0.8090\n",
      "Epoch 541/1000\n",
      " - 0s - loss: 0.5025 - accuracy: 0.7764 - val_loss: 0.4700 - val_accuracy: 0.8315\n",
      "Epoch 542/1000\n",
      " - 0s - loss: 0.4776 - accuracy: 0.7806 - val_loss: 0.4692 - val_accuracy: 0.8258\n",
      "Epoch 543/1000\n",
      " - 0s - loss: 0.4955 - accuracy: 0.7778 - val_loss: 0.4723 - val_accuracy: 0.8090\n",
      "Epoch 544/1000\n",
      " - 0s - loss: 0.4903 - accuracy: 0.7693 - val_loss: 0.4677 - val_accuracy: 0.8371\n",
      "Epoch 545/1000\n",
      " - 0s - loss: 0.4748 - accuracy: 0.7890 - val_loss: 0.4698 - val_accuracy: 0.8258\n",
      "Epoch 546/1000\n",
      " - 0s - loss: 0.4657 - accuracy: 0.7975 - val_loss: 0.4678 - val_accuracy: 0.8202\n",
      "Epoch 547/1000\n",
      " - 0s - loss: 0.4806 - accuracy: 0.7834 - val_loss: 0.4732 - val_accuracy: 0.7753\n",
      "Epoch 548/1000\n",
      " - 0s - loss: 0.5041 - accuracy: 0.7665 - val_loss: 0.4694 - val_accuracy: 0.7865\n",
      "Epoch 549/1000\n",
      " - 0s - loss: 0.5118 - accuracy: 0.7722 - val_loss: 0.4725 - val_accuracy: 0.8202\n",
      "Epoch 550/1000\n",
      " - 0s - loss: 0.4830 - accuracy: 0.7778 - val_loss: 0.4697 - val_accuracy: 0.8315\n",
      "Epoch 551/1000\n",
      " - 0s - loss: 0.4695 - accuracy: 0.7904 - val_loss: 0.4681 - val_accuracy: 0.8315\n",
      "Epoch 552/1000\n",
      " - 0s - loss: 0.4765 - accuracy: 0.7904 - val_loss: 0.4687 - val_accuracy: 0.8315\n",
      "Epoch 553/1000\n",
      " - 0s - loss: 0.4880 - accuracy: 0.7722 - val_loss: 0.4746 - val_accuracy: 0.8258\n",
      "Epoch 554/1000\n",
      " - 0s - loss: 0.4755 - accuracy: 0.7904 - val_loss: 0.4692 - val_accuracy: 0.8258\n",
      "Epoch 555/1000\n",
      " - 0s - loss: 0.4819 - accuracy: 0.7918 - val_loss: 0.4692 - val_accuracy: 0.8258\n",
      "Epoch 556/1000\n",
      " - 0s - loss: 0.4876 - accuracy: 0.7722 - val_loss: 0.4693 - val_accuracy: 0.8202\n",
      "Epoch 557/1000\n",
      " - 0s - loss: 0.5082 - accuracy: 0.7750 - val_loss: 0.4692 - val_accuracy: 0.8315\n",
      "Epoch 558/1000\n",
      " - 0s - loss: 0.4908 - accuracy: 0.7750 - val_loss: 0.4711 - val_accuracy: 0.8202\n",
      "Epoch 559/1000\n",
      " - 0s - loss: 0.4976 - accuracy: 0.7707 - val_loss: 0.4665 - val_accuracy: 0.8315\n",
      "Epoch 560/1000\n",
      " - 0s - loss: 0.4861 - accuracy: 0.7820 - val_loss: 0.4665 - val_accuracy: 0.8371\n",
      "Epoch 561/1000\n",
      " - 0s - loss: 0.4700 - accuracy: 0.7736 - val_loss: 0.4666 - val_accuracy: 0.8371\n",
      "Epoch 562/1000\n",
      " - 0s - loss: 0.4903 - accuracy: 0.7707 - val_loss: 0.4705 - val_accuracy: 0.8315\n",
      "Epoch 563/1000\n",
      " - 0s - loss: 0.4683 - accuracy: 0.7947 - val_loss: 0.4667 - val_accuracy: 0.8371\n",
      "Epoch 564/1000\n",
      " - 0s - loss: 0.4807 - accuracy: 0.7764 - val_loss: 0.4659 - val_accuracy: 0.8315\n",
      "Epoch 565/1000\n",
      " - 0s - loss: 0.4819 - accuracy: 0.7707 - val_loss: 0.4666 - val_accuracy: 0.8258\n",
      "Epoch 566/1000\n",
      " - 0s - loss: 0.4827 - accuracy: 0.7792 - val_loss: 0.4652 - val_accuracy: 0.8202\n",
      "Epoch 567/1000\n",
      " - 0s - loss: 0.4750 - accuracy: 0.7989 - val_loss: 0.4634 - val_accuracy: 0.8371\n",
      "Epoch 568/1000\n",
      " - 0s - loss: 0.4893 - accuracy: 0.7707 - val_loss: 0.4665 - val_accuracy: 0.8146\n",
      "Epoch 569/1000\n",
      " - 0s - loss: 0.4872 - accuracy: 0.7693 - val_loss: 0.4783 - val_accuracy: 0.7809\n",
      "Epoch 570/1000\n",
      " - 0s - loss: 0.5070 - accuracy: 0.7539 - val_loss: 0.4713 - val_accuracy: 0.8090\n",
      "Epoch 571/1000\n",
      " - 0s - loss: 0.5003 - accuracy: 0.7820 - val_loss: 0.4725 - val_accuracy: 0.8371\n",
      "Epoch 572/1000\n",
      " - 0s - loss: 0.4896 - accuracy: 0.7764 - val_loss: 0.4723 - val_accuracy: 0.8202\n",
      "Epoch 573/1000\n",
      " - 0s - loss: 0.4798 - accuracy: 0.7947 - val_loss: 0.4678 - val_accuracy: 0.8258\n",
      "Epoch 574/1000\n",
      " - 0s - loss: 0.4886 - accuracy: 0.7707 - val_loss: 0.4677 - val_accuracy: 0.8090\n",
      "Epoch 575/1000\n",
      " - 0s - loss: 0.4934 - accuracy: 0.7806 - val_loss: 0.4651 - val_accuracy: 0.8258\n",
      "Epoch 576/1000\n",
      " - 0s - loss: 0.4606 - accuracy: 0.7989 - val_loss: 0.4645 - val_accuracy: 0.8202\n",
      "Epoch 577/1000\n",
      " - 0s - loss: 0.4950 - accuracy: 0.7651 - val_loss: 0.4649 - val_accuracy: 0.8258\n",
      "Epoch 578/1000\n",
      " - 0s - loss: 0.4866 - accuracy: 0.7764 - val_loss: 0.4682 - val_accuracy: 0.8202\n",
      "Epoch 579/1000\n",
      " - 0s - loss: 0.5182 - accuracy: 0.7567 - val_loss: 0.4666 - val_accuracy: 0.8202\n",
      "Epoch 580/1000\n",
      " - 0s - loss: 0.4762 - accuracy: 0.7890 - val_loss: 0.4680 - val_accuracy: 0.8202\n",
      "Epoch 581/1000\n",
      " - 0s - loss: 0.4827 - accuracy: 0.7961 - val_loss: 0.4682 - val_accuracy: 0.8258\n",
      "Epoch 582/1000\n",
      " - 0s - loss: 0.4842 - accuracy: 0.7679 - val_loss: 0.4720 - val_accuracy: 0.8202\n",
      "Epoch 583/1000\n",
      " - 0s - loss: 0.4909 - accuracy: 0.7693 - val_loss: 0.4688 - val_accuracy: 0.8315\n",
      "Epoch 584/1000\n",
      " - 0s - loss: 0.4721 - accuracy: 0.7890 - val_loss: 0.4680 - val_accuracy: 0.8315\n",
      "Epoch 585/1000\n",
      " - 0s - loss: 0.4782 - accuracy: 0.7806 - val_loss: 0.4676 - val_accuracy: 0.8258\n",
      "Epoch 586/1000\n",
      " - 0s - loss: 0.4787 - accuracy: 0.7890 - val_loss: 0.4652 - val_accuracy: 0.8315\n",
      "Epoch 587/1000\n",
      " - 0s - loss: 0.4715 - accuracy: 0.7848 - val_loss: 0.4662 - val_accuracy: 0.8146\n",
      "Epoch 588/1000\n",
      " - 0s - loss: 0.4923 - accuracy: 0.7736 - val_loss: 0.4738 - val_accuracy: 0.8090\n",
      "Epoch 589/1000\n",
      " - 0s - loss: 0.5134 - accuracy: 0.7609 - val_loss: 0.4644 - val_accuracy: 0.8202\n",
      "Epoch 590/1000\n",
      " - 0s - loss: 0.4882 - accuracy: 0.7693 - val_loss: 0.4633 - val_accuracy: 0.8258\n",
      "Epoch 591/1000\n",
      " - 0s - loss: 0.4837 - accuracy: 0.7707 - val_loss: 0.4661 - val_accuracy: 0.8090\n",
      "Epoch 592/1000\n",
      " - 0s - loss: 0.4876 - accuracy: 0.7792 - val_loss: 0.4650 - val_accuracy: 0.8315\n",
      "Epoch 593/1000\n",
      " - 0s - loss: 0.4766 - accuracy: 0.7806 - val_loss: 0.4631 - val_accuracy: 0.8258\n",
      "Epoch 594/1000\n",
      " - 0s - loss: 0.4700 - accuracy: 0.7989 - val_loss: 0.4664 - val_accuracy: 0.8146\n",
      "Epoch 595/1000\n",
      " - 0s - loss: 0.4833 - accuracy: 0.7764 - val_loss: 0.4620 - val_accuracy: 0.8315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 596/1000\n",
      " - 0s - loss: 0.4787 - accuracy: 0.7792 - val_loss: 0.4636 - val_accuracy: 0.8371\n",
      "Epoch 597/1000\n",
      " - 0s - loss: 0.4768 - accuracy: 0.7862 - val_loss: 0.4690 - val_accuracy: 0.8315\n",
      "Epoch 598/1000\n",
      " - 0s - loss: 0.4868 - accuracy: 0.7848 - val_loss: 0.4629 - val_accuracy: 0.8258\n",
      "Epoch 599/1000\n",
      " - 0s - loss: 0.4940 - accuracy: 0.7778 - val_loss: 0.4616 - val_accuracy: 0.8258\n",
      "Epoch 600/1000\n",
      " - 0s - loss: 0.4850 - accuracy: 0.7834 - val_loss: 0.4621 - val_accuracy: 0.8202\n",
      "Epoch 601/1000\n",
      " - 0s - loss: 0.4964 - accuracy: 0.7792 - val_loss: 0.4665 - val_accuracy: 0.8202\n",
      "Epoch 602/1000\n",
      " - 0s - loss: 0.4908 - accuracy: 0.7665 - val_loss: 0.4655 - val_accuracy: 0.8202\n",
      "Epoch 603/1000\n",
      " - 0s - loss: 0.4923 - accuracy: 0.7932 - val_loss: 0.4632 - val_accuracy: 0.8315\n",
      "Epoch 604/1000\n",
      " - 0s - loss: 0.4806 - accuracy: 0.7806 - val_loss: 0.4683 - val_accuracy: 0.8258\n",
      "Epoch 605/1000\n",
      " - 0s - loss: 0.4821 - accuracy: 0.7862 - val_loss: 0.4691 - val_accuracy: 0.8315\n",
      "Epoch 606/1000\n",
      " - 0s - loss: 0.4840 - accuracy: 0.7820 - val_loss: 0.4662 - val_accuracy: 0.8258\n",
      "Epoch 607/1000\n",
      " - 0s - loss: 0.5094 - accuracy: 0.7539 - val_loss: 0.4656 - val_accuracy: 0.8371\n",
      "Epoch 608/1000\n",
      " - 0s - loss: 0.4898 - accuracy: 0.7848 - val_loss: 0.4687 - val_accuracy: 0.8258\n",
      "Epoch 609/1000\n",
      " - 0s - loss: 0.4765 - accuracy: 0.7876 - val_loss: 0.4643 - val_accuracy: 0.8315\n",
      "Epoch 610/1000\n",
      " - 0s - loss: 0.4888 - accuracy: 0.7764 - val_loss: 0.4674 - val_accuracy: 0.8258\n",
      "Epoch 611/1000\n",
      " - 0s - loss: 0.4788 - accuracy: 0.7918 - val_loss: 0.4646 - val_accuracy: 0.8315\n",
      "Epoch 612/1000\n",
      " - 0s - loss: 0.4757 - accuracy: 0.7989 - val_loss: 0.4653 - val_accuracy: 0.8315\n",
      "Epoch 613/1000\n",
      " - 0s - loss: 0.4843 - accuracy: 0.7890 - val_loss: 0.4674 - val_accuracy: 0.8202\n",
      "Epoch 614/1000\n",
      " - 0s - loss: 0.5000 - accuracy: 0.7834 - val_loss: 0.4670 - val_accuracy: 0.8146\n",
      "Epoch 615/1000\n",
      " - 0s - loss: 0.4776 - accuracy: 0.7947 - val_loss: 0.4686 - val_accuracy: 0.8258\n",
      "Epoch 616/1000\n",
      " - 0s - loss: 0.4834 - accuracy: 0.7961 - val_loss: 0.4650 - val_accuracy: 0.8315\n",
      "Epoch 617/1000\n",
      " - 0s - loss: 0.4911 - accuracy: 0.7918 - val_loss: 0.4666 - val_accuracy: 0.8371\n",
      "Epoch 618/1000\n",
      " - 0s - loss: 0.4814 - accuracy: 0.7679 - val_loss: 0.4649 - val_accuracy: 0.8315\n",
      "Epoch 619/1000\n",
      " - 0s - loss: 0.4940 - accuracy: 0.7778 - val_loss: 0.4664 - val_accuracy: 0.8371\n",
      "Epoch 620/1000\n",
      " - 0s - loss: 0.4899 - accuracy: 0.7834 - val_loss: 0.4711 - val_accuracy: 0.8258\n",
      "Epoch 621/1000\n",
      " - 0s - loss: 0.4842 - accuracy: 0.7890 - val_loss: 0.4725 - val_accuracy: 0.8146\n",
      "Epoch 622/1000\n",
      " - 0s - loss: 0.4642 - accuracy: 0.8045 - val_loss: 0.4697 - val_accuracy: 0.8258\n",
      "Epoch 623/1000\n",
      " - 0s - loss: 0.4661 - accuracy: 0.8059 - val_loss: 0.4658 - val_accuracy: 0.8202\n",
      "Epoch 624/1000\n",
      " - 0s - loss: 0.4959 - accuracy: 0.7651 - val_loss: 0.4653 - val_accuracy: 0.8258\n",
      "Epoch 625/1000\n",
      " - 0s - loss: 0.4877 - accuracy: 0.7932 - val_loss: 0.4733 - val_accuracy: 0.8146\n",
      "Epoch 626/1000\n",
      " - 0s - loss: 0.4845 - accuracy: 0.7820 - val_loss: 0.4730 - val_accuracy: 0.8034\n",
      "Epoch 627/1000\n",
      " - 0s - loss: 0.4650 - accuracy: 0.7947 - val_loss: 0.4693 - val_accuracy: 0.8315\n",
      "Epoch 628/1000\n",
      " - 0s - loss: 0.4762 - accuracy: 0.7834 - val_loss: 0.4676 - val_accuracy: 0.8315\n",
      "Epoch 629/1000\n",
      " - 0s - loss: 0.4960 - accuracy: 0.7778 - val_loss: 0.4699 - val_accuracy: 0.8090\n",
      "Epoch 630/1000\n",
      " - 0s - loss: 0.4765 - accuracy: 0.7904 - val_loss: 0.4718 - val_accuracy: 0.8427\n",
      "Epoch 631/1000\n",
      " - 0s - loss: 0.4942 - accuracy: 0.7947 - val_loss: 0.4719 - val_accuracy: 0.8146\n",
      "Epoch 632/1000\n",
      " - 0s - loss: 0.4780 - accuracy: 0.7890 - val_loss: 0.4699 - val_accuracy: 0.8034\n",
      "Epoch 633/1000\n",
      " - 0s - loss: 0.4886 - accuracy: 0.7848 - val_loss: 0.4680 - val_accuracy: 0.8034\n",
      "Epoch 634/1000\n",
      " - 0s - loss: 0.4807 - accuracy: 0.7834 - val_loss: 0.4659 - val_accuracy: 0.8146\n",
      "Epoch 635/1000\n",
      " - 0s - loss: 0.4533 - accuracy: 0.7947 - val_loss: 0.4662 - val_accuracy: 0.8258\n",
      "Epoch 636/1000\n",
      " - 0s - loss: 0.4770 - accuracy: 0.7904 - val_loss: 0.4661 - val_accuracy: 0.8146\n",
      "Epoch 637/1000\n",
      " - 0s - loss: 0.4797 - accuracy: 0.7750 - val_loss: 0.4671 - val_accuracy: 0.8202\n",
      "Epoch 638/1000\n",
      " - 0s - loss: 0.4937 - accuracy: 0.7764 - val_loss: 0.4663 - val_accuracy: 0.8146\n",
      "Epoch 639/1000\n",
      " - 0s - loss: 0.4589 - accuracy: 0.8017 - val_loss: 0.4641 - val_accuracy: 0.8258\n",
      "Epoch 640/1000\n",
      " - 0s - loss: 0.4735 - accuracy: 0.7806 - val_loss: 0.4656 - val_accuracy: 0.7978\n",
      "Epoch 641/1000\n",
      " - 0s - loss: 0.4652 - accuracy: 0.8003 - val_loss: 0.4658 - val_accuracy: 0.8258\n",
      "Epoch 642/1000\n",
      " - 0s - loss: 0.4933 - accuracy: 0.7806 - val_loss: 0.4685 - val_accuracy: 0.8146\n",
      "Epoch 643/1000\n",
      " - 0s - loss: 0.4981 - accuracy: 0.7862 - val_loss: 0.4690 - val_accuracy: 0.8258\n",
      "Epoch 644/1000\n",
      " - 0s - loss: 0.4694 - accuracy: 0.8003 - val_loss: 0.4656 - val_accuracy: 0.8202\n",
      "Epoch 645/1000\n",
      " - 0s - loss: 0.4859 - accuracy: 0.7904 - val_loss: 0.4681 - val_accuracy: 0.8090\n",
      "Epoch 646/1000\n",
      " - 0s - loss: 0.4769 - accuracy: 0.7778 - val_loss: 0.4638 - val_accuracy: 0.8371\n",
      "Epoch 647/1000\n",
      " - 0s - loss: 0.4792 - accuracy: 0.7848 - val_loss: 0.4640 - val_accuracy: 0.8315\n",
      "Epoch 648/1000\n",
      " - 0s - loss: 0.4925 - accuracy: 0.7750 - val_loss: 0.4691 - val_accuracy: 0.8090\n",
      "Epoch 649/1000\n",
      " - 0s - loss: 0.4978 - accuracy: 0.7707 - val_loss: 0.4651 - val_accuracy: 0.8034\n",
      "Epoch 650/1000\n",
      " - 0s - loss: 0.4911 - accuracy: 0.7862 - val_loss: 0.4644 - val_accuracy: 0.8146\n",
      "Epoch 651/1000\n",
      " - 0s - loss: 0.4863 - accuracy: 0.7750 - val_loss: 0.4629 - val_accuracy: 0.8090\n",
      "Epoch 652/1000\n",
      " - 0s - loss: 0.4735 - accuracy: 0.7947 - val_loss: 0.4623 - val_accuracy: 0.8371\n",
      "Epoch 653/1000\n",
      " - 0s - loss: 0.4642 - accuracy: 0.7918 - val_loss: 0.4635 - val_accuracy: 0.8371\n",
      "Epoch 654/1000\n",
      " - 0s - loss: 0.4519 - accuracy: 0.8087 - val_loss: 0.4614 - val_accuracy: 0.8371\n",
      "Epoch 655/1000\n",
      " - 0s - loss: 0.4530 - accuracy: 0.8115 - val_loss: 0.4631 - val_accuracy: 0.8258\n",
      "Epoch 656/1000\n",
      " - 0s - loss: 0.4612 - accuracy: 0.8059 - val_loss: 0.4649 - val_accuracy: 0.8202\n",
      "Epoch 657/1000\n",
      " - 0s - loss: 0.4887 - accuracy: 0.7876 - val_loss: 0.4637 - val_accuracy: 0.8090\n",
      "Epoch 658/1000\n",
      " - 0s - loss: 0.4730 - accuracy: 0.7792 - val_loss: 0.4644 - val_accuracy: 0.8146\n",
      "Epoch 659/1000\n",
      " - 0s - loss: 0.4904 - accuracy: 0.7707 - val_loss: 0.4636 - val_accuracy: 0.8202\n",
      "Epoch 660/1000\n",
      " - 0s - loss: 0.4831 - accuracy: 0.7904 - val_loss: 0.4664 - val_accuracy: 0.8090\n",
      "Epoch 661/1000\n",
      " - 0s - loss: 0.4884 - accuracy: 0.7707 - val_loss: 0.4678 - val_accuracy: 0.8146\n",
      "Epoch 662/1000\n",
      " - 0s - loss: 0.4605 - accuracy: 0.7834 - val_loss: 0.4669 - val_accuracy: 0.8146\n",
      "Epoch 663/1000\n",
      " - 0s - loss: 0.4721 - accuracy: 0.8017 - val_loss: 0.4684 - val_accuracy: 0.8258\n",
      "Epoch 664/1000\n",
      " - 0s - loss: 0.4835 - accuracy: 0.7862 - val_loss: 0.4704 - val_accuracy: 0.8315\n",
      "Epoch 665/1000\n",
      " - 0s - loss: 0.4687 - accuracy: 0.7947 - val_loss: 0.4717 - val_accuracy: 0.8146\n",
      "Epoch 666/1000\n",
      " - 0s - loss: 0.4647 - accuracy: 0.7989 - val_loss: 0.4666 - val_accuracy: 0.8258\n",
      "Epoch 667/1000\n",
      " - 0s - loss: 0.4867 - accuracy: 0.7778 - val_loss: 0.4718 - val_accuracy: 0.8146\n",
      "Epoch 668/1000\n",
      " - 0s - loss: 0.4882 - accuracy: 0.7567 - val_loss: 0.4697 - val_accuracy: 0.8146\n",
      "Epoch 669/1000\n",
      " - 0s - loss: 0.4800 - accuracy: 0.7665 - val_loss: 0.4670 - val_accuracy: 0.8258\n",
      "Epoch 670/1000\n",
      " - 0s - loss: 0.4805 - accuracy: 0.7932 - val_loss: 0.4669 - val_accuracy: 0.8258\n",
      "Epoch 671/1000\n",
      " - 0s - loss: 0.4714 - accuracy: 0.7848 - val_loss: 0.4669 - val_accuracy: 0.8202\n",
      "Epoch 672/1000\n",
      " - 0s - loss: 0.4480 - accuracy: 0.8003 - val_loss: 0.4648 - val_accuracy: 0.8090\n",
      "Epoch 673/1000\n",
      " - 0s - loss: 0.4778 - accuracy: 0.7975 - val_loss: 0.4613 - val_accuracy: 0.8315\n",
      "Epoch 674/1000\n",
      " - 0s - loss: 0.4934 - accuracy: 0.7862 - val_loss: 0.4638 - val_accuracy: 0.8146\n",
      "Epoch 675/1000\n",
      " - 0s - loss: 0.4644 - accuracy: 0.8003 - val_loss: 0.4604 - val_accuracy: 0.8258\n",
      "Epoch 676/1000\n",
      " - 0s - loss: 0.4760 - accuracy: 0.7637 - val_loss: 0.4638 - val_accuracy: 0.8202\n",
      "Epoch 677/1000\n",
      " - 0s - loss: 0.4692 - accuracy: 0.7947 - val_loss: 0.4631 - val_accuracy: 0.8315\n",
      "Epoch 678/1000\n",
      " - 0s - loss: 0.4636 - accuracy: 0.7989 - val_loss: 0.4626 - val_accuracy: 0.8202\n",
      "Epoch 679/1000\n",
      " - 0s - loss: 0.4630 - accuracy: 0.7947 - val_loss: 0.4629 - val_accuracy: 0.8315\n",
      "Epoch 680/1000\n",
      " - 0s - loss: 0.4766 - accuracy: 0.7834 - val_loss: 0.4612 - val_accuracy: 0.8258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 681/1000\n",
      " - 0s - loss: 0.4877 - accuracy: 0.7947 - val_loss: 0.4643 - val_accuracy: 0.8146\n",
      "Epoch 682/1000\n",
      " - 0s - loss: 0.4831 - accuracy: 0.7918 - val_loss: 0.4650 - val_accuracy: 0.7921\n",
      "Epoch 683/1000\n",
      " - 0s - loss: 0.4806 - accuracy: 0.7862 - val_loss: 0.4695 - val_accuracy: 0.7865\n",
      "Epoch 684/1000\n",
      " - 0s - loss: 0.4864 - accuracy: 0.7890 - val_loss: 0.4630 - val_accuracy: 0.8202\n",
      "Epoch 685/1000\n",
      " - 0s - loss: 0.4680 - accuracy: 0.7862 - val_loss: 0.4641 - val_accuracy: 0.8371\n",
      "Epoch 686/1000\n",
      " - 0s - loss: 0.4876 - accuracy: 0.7764 - val_loss: 0.4671 - val_accuracy: 0.8146\n",
      "Epoch 687/1000\n",
      " - 0s - loss: 0.4898 - accuracy: 0.7693 - val_loss: 0.4629 - val_accuracy: 0.8315\n",
      "Epoch 688/1000\n",
      " - 0s - loss: 0.4934 - accuracy: 0.7722 - val_loss: 0.4650 - val_accuracy: 0.8258\n",
      "Epoch 689/1000\n",
      " - 0s - loss: 0.4760 - accuracy: 0.7918 - val_loss: 0.4654 - val_accuracy: 0.8034\n",
      "Epoch 690/1000\n",
      " - 0s - loss: 0.4778 - accuracy: 0.7890 - val_loss: 0.4614 - val_accuracy: 0.8315\n",
      "Epoch 691/1000\n",
      " - 0s - loss: 0.4459 - accuracy: 0.8158 - val_loss: 0.4582 - val_accuracy: 0.8371\n",
      "Epoch 692/1000\n",
      " - 0s - loss: 0.4789 - accuracy: 0.7947 - val_loss: 0.4609 - val_accuracy: 0.8090\n",
      "Epoch 693/1000\n",
      " - 0s - loss: 0.4737 - accuracy: 0.7918 - val_loss: 0.4567 - val_accuracy: 0.8315\n",
      "Epoch 694/1000\n",
      " - 0s - loss: 0.4730 - accuracy: 0.7778 - val_loss: 0.4610 - val_accuracy: 0.8090\n",
      "Epoch 695/1000\n",
      " - 0s - loss: 0.4740 - accuracy: 0.7890 - val_loss: 0.4570 - val_accuracy: 0.8202\n",
      "Epoch 696/1000\n",
      " - 0s - loss: 0.4653 - accuracy: 0.8003 - val_loss: 0.4572 - val_accuracy: 0.8315\n",
      "Epoch 697/1000\n",
      " - 0s - loss: 0.5007 - accuracy: 0.7736 - val_loss: 0.4574 - val_accuracy: 0.8315\n",
      "Epoch 698/1000\n",
      " - 0s - loss: 0.4620 - accuracy: 0.8087 - val_loss: 0.4599 - val_accuracy: 0.8202\n",
      "Epoch 699/1000\n",
      " - 0s - loss: 0.4652 - accuracy: 0.7876 - val_loss: 0.4673 - val_accuracy: 0.8034\n",
      "Epoch 700/1000\n",
      " - 0s - loss: 0.4966 - accuracy: 0.7623 - val_loss: 0.4581 - val_accuracy: 0.8371\n",
      "Epoch 701/1000\n",
      " - 0s - loss: 0.4871 - accuracy: 0.7862 - val_loss: 0.4709 - val_accuracy: 0.8202\n",
      "Epoch 702/1000\n",
      " - 0s - loss: 0.4761 - accuracy: 0.7904 - val_loss: 0.4634 - val_accuracy: 0.8202\n",
      "Epoch 703/1000\n",
      " - 0s - loss: 0.4792 - accuracy: 0.7778 - val_loss: 0.4612 - val_accuracy: 0.8427\n",
      "Epoch 704/1000\n",
      " - 0s - loss: 0.4731 - accuracy: 0.7848 - val_loss: 0.4637 - val_accuracy: 0.8258\n",
      "Epoch 705/1000\n",
      " - 0s - loss: 0.4731 - accuracy: 0.7862 - val_loss: 0.4653 - val_accuracy: 0.8315\n",
      "Epoch 706/1000\n",
      " - 0s - loss: 0.4721 - accuracy: 0.7989 - val_loss: 0.4660 - val_accuracy: 0.8315\n",
      "Epoch 707/1000\n",
      " - 0s - loss: 0.4687 - accuracy: 0.7862 - val_loss: 0.4610 - val_accuracy: 0.8315\n",
      "Epoch 708/1000\n",
      " - 0s - loss: 0.4813 - accuracy: 0.7750 - val_loss: 0.4580 - val_accuracy: 0.8258\n",
      "Epoch 709/1000\n",
      " - 0s - loss: 0.4768 - accuracy: 0.7932 - val_loss: 0.4599 - val_accuracy: 0.8371\n",
      "Epoch 710/1000\n",
      " - 0s - loss: 0.4736 - accuracy: 0.7736 - val_loss: 0.4640 - val_accuracy: 0.8315\n",
      "Epoch 711/1000\n",
      " - 0s - loss: 0.4812 - accuracy: 0.7820 - val_loss: 0.4664 - val_accuracy: 0.8146\n",
      "Epoch 712/1000\n",
      " - 0s - loss: 0.4832 - accuracy: 0.7890 - val_loss: 0.4624 - val_accuracy: 0.8258\n",
      "Epoch 713/1000\n",
      " - 0s - loss: 0.4651 - accuracy: 0.7918 - val_loss: 0.4618 - val_accuracy: 0.8258\n",
      "Epoch 714/1000\n",
      " - 0s - loss: 0.4759 - accuracy: 0.7932 - val_loss: 0.4704 - val_accuracy: 0.8090\n",
      "Epoch 715/1000\n",
      " - 0s - loss: 0.4764 - accuracy: 0.7918 - val_loss: 0.4666 - val_accuracy: 0.8258\n",
      "Epoch 716/1000\n",
      " - 0s - loss: 0.4867 - accuracy: 0.7890 - val_loss: 0.4588 - val_accuracy: 0.8315\n",
      "Epoch 717/1000\n",
      " - 0s - loss: 0.4858 - accuracy: 0.7707 - val_loss: 0.4588 - val_accuracy: 0.8371\n",
      "Epoch 718/1000\n",
      " - 0s - loss: 0.4674 - accuracy: 0.7778 - val_loss: 0.4581 - val_accuracy: 0.8371\n",
      "Epoch 719/1000\n",
      " - 0s - loss: 0.4799 - accuracy: 0.7848 - val_loss: 0.4594 - val_accuracy: 0.8371\n",
      "Epoch 720/1000\n",
      " - 0s - loss: 0.4715 - accuracy: 0.7792 - val_loss: 0.4625 - val_accuracy: 0.8090\n",
      "Epoch 721/1000\n",
      " - 0s - loss: 0.4734 - accuracy: 0.7834 - val_loss: 0.4628 - val_accuracy: 0.8315\n",
      "Epoch 722/1000\n",
      " - 0s - loss: 0.4682 - accuracy: 0.7876 - val_loss: 0.4619 - val_accuracy: 0.8202\n",
      "Epoch 723/1000\n",
      " - 0s - loss: 0.4895 - accuracy: 0.7693 - val_loss: 0.4619 - val_accuracy: 0.8202\n",
      "Epoch 724/1000\n",
      " - 0s - loss: 0.4770 - accuracy: 0.7876 - val_loss: 0.4666 - val_accuracy: 0.8146\n",
      "Epoch 725/1000\n",
      " - 0s - loss: 0.4774 - accuracy: 0.7792 - val_loss: 0.4642 - val_accuracy: 0.8315\n",
      "Epoch 726/1000\n",
      " - 0s - loss: 0.4861 - accuracy: 0.7693 - val_loss: 0.4631 - val_accuracy: 0.8315\n",
      "Epoch 727/1000\n",
      " - 0s - loss: 0.4749 - accuracy: 0.7989 - val_loss: 0.4607 - val_accuracy: 0.8315\n",
      "Epoch 728/1000\n",
      " - 0s - loss: 0.4738 - accuracy: 0.7947 - val_loss: 0.4663 - val_accuracy: 0.8315\n",
      "Epoch 729/1000\n",
      " - 0s - loss: 0.4777 - accuracy: 0.7848 - val_loss: 0.4570 - val_accuracy: 0.8258\n",
      "Epoch 730/1000\n",
      " - 0s - loss: 0.4721 - accuracy: 0.8101 - val_loss: 0.4674 - val_accuracy: 0.8315\n",
      "Epoch 731/1000\n",
      " - 0s - loss: 0.5060 - accuracy: 0.7764 - val_loss: 0.4664 - val_accuracy: 0.8146\n",
      "Epoch 732/1000\n",
      " - 0s - loss: 0.4844 - accuracy: 0.7848 - val_loss: 0.4645 - val_accuracy: 0.8146\n",
      "Epoch 733/1000\n",
      " - 0s - loss: 0.4769 - accuracy: 0.7918 - val_loss: 0.4604 - val_accuracy: 0.8146\n",
      "Epoch 734/1000\n",
      " - 0s - loss: 0.4872 - accuracy: 0.7764 - val_loss: 0.4589 - val_accuracy: 0.8146\n",
      "Epoch 735/1000\n",
      " - 0s - loss: 0.4905 - accuracy: 0.7679 - val_loss: 0.4586 - val_accuracy: 0.8258\n",
      "Epoch 736/1000\n",
      " - 0s - loss: 0.4914 - accuracy: 0.7876 - val_loss: 0.4577 - val_accuracy: 0.8315\n",
      "Epoch 737/1000\n",
      " - 0s - loss: 0.4702 - accuracy: 0.7975 - val_loss: 0.4580 - val_accuracy: 0.8202\n",
      "Epoch 738/1000\n",
      " - 0s - loss: 0.4809 - accuracy: 0.7862 - val_loss: 0.4562 - val_accuracy: 0.8371\n",
      "Epoch 739/1000\n",
      " - 0s - loss: 0.4728 - accuracy: 0.8017 - val_loss: 0.4556 - val_accuracy: 0.8315\n",
      "Epoch 740/1000\n",
      " - 0s - loss: 0.4539 - accuracy: 0.7989 - val_loss: 0.4538 - val_accuracy: 0.8258\n",
      "Epoch 741/1000\n",
      " - 0s - loss: 0.4580 - accuracy: 0.7876 - val_loss: 0.4557 - val_accuracy: 0.8315\n",
      "Epoch 742/1000\n",
      " - 0s - loss: 0.4872 - accuracy: 0.7792 - val_loss: 0.4590 - val_accuracy: 0.8258\n",
      "Epoch 743/1000\n",
      " - 0s - loss: 0.4750 - accuracy: 0.7918 - val_loss: 0.4618 - val_accuracy: 0.8202\n",
      "Epoch 744/1000\n",
      " - 0s - loss: 0.4712 - accuracy: 0.8045 - val_loss: 0.4622 - val_accuracy: 0.8090\n",
      "Epoch 745/1000\n",
      " - 0s - loss: 0.4672 - accuracy: 0.7834 - val_loss: 0.4595 - val_accuracy: 0.8371\n",
      "Epoch 746/1000\n",
      " - 0s - loss: 0.4851 - accuracy: 0.7778 - val_loss: 0.4603 - val_accuracy: 0.8315\n",
      "Epoch 747/1000\n",
      " - 0s - loss: 0.4591 - accuracy: 0.7947 - val_loss: 0.4598 - val_accuracy: 0.8427\n",
      "Epoch 748/1000\n",
      " - 0s - loss: 0.4523 - accuracy: 0.7989 - val_loss: 0.4606 - val_accuracy: 0.8258\n",
      "Epoch 749/1000\n",
      " - 0s - loss: 0.4725 - accuracy: 0.7975 - val_loss: 0.4606 - val_accuracy: 0.8202\n",
      "Epoch 750/1000\n",
      " - 0s - loss: 0.4786 - accuracy: 0.7806 - val_loss: 0.4601 - val_accuracy: 0.8146\n",
      "Epoch 751/1000\n",
      " - 0s - loss: 0.4771 - accuracy: 0.7848 - val_loss: 0.4553 - val_accuracy: 0.8315\n",
      "Epoch 752/1000\n",
      " - 0s - loss: 0.4562 - accuracy: 0.7975 - val_loss: 0.4563 - val_accuracy: 0.8315\n",
      "Epoch 753/1000\n",
      " - 0s - loss: 0.4651 - accuracy: 0.7750 - val_loss: 0.4612 - val_accuracy: 0.8146\n",
      "Epoch 754/1000\n",
      " - 0s - loss: 0.4638 - accuracy: 0.7918 - val_loss: 0.4587 - val_accuracy: 0.8146\n",
      "Epoch 755/1000\n",
      " - 0s - loss: 0.4774 - accuracy: 0.7904 - val_loss: 0.4632 - val_accuracy: 0.8202\n",
      "Epoch 756/1000\n",
      " - 0s - loss: 0.4705 - accuracy: 0.8017 - val_loss: 0.4631 - val_accuracy: 0.8090\n",
      "Epoch 757/1000\n",
      " - 0s - loss: 0.4910 - accuracy: 0.7792 - val_loss: 0.4620 - val_accuracy: 0.8146\n",
      "Epoch 758/1000\n",
      " - 0s - loss: 0.4655 - accuracy: 0.8087 - val_loss: 0.4661 - val_accuracy: 0.8090\n",
      "Epoch 759/1000\n",
      " - 0s - loss: 0.4610 - accuracy: 0.7975 - val_loss: 0.4744 - val_accuracy: 0.8090\n",
      "Epoch 760/1000\n",
      " - 0s - loss: 0.4628 - accuracy: 0.7862 - val_loss: 0.4613 - val_accuracy: 0.8090\n",
      "Epoch 761/1000\n",
      " - 0s - loss: 0.4618 - accuracy: 0.8073 - val_loss: 0.4560 - val_accuracy: 0.8371\n",
      "Epoch 762/1000\n",
      " - 0s - loss: 0.4867 - accuracy: 0.7693 - val_loss: 0.4575 - val_accuracy: 0.8315\n",
      "Epoch 763/1000\n",
      " - 0s - loss: 0.4676 - accuracy: 0.7961 - val_loss: 0.4589 - val_accuracy: 0.8034\n",
      "Epoch 764/1000\n",
      " - 0s - loss: 0.4855 - accuracy: 0.7707 - val_loss: 0.4646 - val_accuracy: 0.8034\n",
      "Epoch 765/1000\n",
      " - 0s - loss: 0.4762 - accuracy: 0.7820 - val_loss: 0.4573 - val_accuracy: 0.8034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 766/1000\n",
      " - 0s - loss: 0.4840 - accuracy: 0.7862 - val_loss: 0.4838 - val_accuracy: 0.8090\n",
      "Epoch 767/1000\n",
      " - 0s - loss: 0.4921 - accuracy: 0.7778 - val_loss: 0.4579 - val_accuracy: 0.8034\n",
      "Epoch 768/1000\n",
      " - 0s - loss: 0.4601 - accuracy: 0.7890 - val_loss: 0.4569 - val_accuracy: 0.8090\n",
      "Epoch 769/1000\n",
      " - 0s - loss: 0.4693 - accuracy: 0.7975 - val_loss: 0.4521 - val_accuracy: 0.8202\n",
      "Epoch 770/1000\n",
      " - 0s - loss: 0.4773 - accuracy: 0.7932 - val_loss: 0.4514 - val_accuracy: 0.8258\n",
      "Epoch 771/1000\n",
      " - 0s - loss: 0.4697 - accuracy: 0.7848 - val_loss: 0.4539 - val_accuracy: 0.8258\n",
      "Epoch 772/1000\n",
      " - 0s - loss: 0.4816 - accuracy: 0.7947 - val_loss: 0.4539 - val_accuracy: 0.8371\n",
      "Epoch 773/1000\n",
      " - 0s - loss: 0.4767 - accuracy: 0.7750 - val_loss: 0.4542 - val_accuracy: 0.8371\n",
      "Epoch 774/1000\n",
      " - 0s - loss: 0.4873 - accuracy: 0.7932 - val_loss: 0.4571 - val_accuracy: 0.8090\n",
      "Epoch 775/1000\n",
      " - 0s - loss: 0.4739 - accuracy: 0.7989 - val_loss: 0.4588 - val_accuracy: 0.8146\n",
      "Epoch 776/1000\n",
      " - 0s - loss: 0.4787 - accuracy: 0.7764 - val_loss: 0.4586 - val_accuracy: 0.8258\n",
      "Epoch 777/1000\n",
      " - 0s - loss: 0.4713 - accuracy: 0.7848 - val_loss: 0.4596 - val_accuracy: 0.8371\n",
      "Epoch 778/1000\n",
      " - 0s - loss: 0.4761 - accuracy: 0.7806 - val_loss: 0.4589 - val_accuracy: 0.8315\n",
      "Epoch 779/1000\n",
      " - 0s - loss: 0.4892 - accuracy: 0.7736 - val_loss: 0.4589 - val_accuracy: 0.8371\n",
      "Epoch 780/1000\n",
      " - 0s - loss: 0.4791 - accuracy: 0.7890 - val_loss: 0.4595 - val_accuracy: 0.8315\n",
      "Epoch 781/1000\n",
      " - 0s - loss: 0.4515 - accuracy: 0.7961 - val_loss: 0.4579 - val_accuracy: 0.8371\n",
      "Epoch 782/1000\n",
      " - 0s - loss: 0.5012 - accuracy: 0.7679 - val_loss: 0.4735 - val_accuracy: 0.8202\n",
      "Epoch 783/1000\n",
      " - 0s - loss: 0.4897 - accuracy: 0.7792 - val_loss: 0.4550 - val_accuracy: 0.8315\n",
      "Epoch 784/1000\n",
      " - 0s - loss: 0.4752 - accuracy: 0.7890 - val_loss: 0.4575 - val_accuracy: 0.8146\n",
      "Epoch 785/1000\n",
      " - 0s - loss: 0.5022 - accuracy: 0.7707 - val_loss: 0.4543 - val_accuracy: 0.8146\n",
      "Epoch 786/1000\n",
      " - 0s - loss: 0.4750 - accuracy: 0.7890 - val_loss: 0.4559 - val_accuracy: 0.8371\n",
      "Epoch 787/1000\n",
      " - 0s - loss: 0.4767 - accuracy: 0.7876 - val_loss: 0.4563 - val_accuracy: 0.8315\n",
      "Epoch 788/1000\n",
      " - 0s - loss: 0.4764 - accuracy: 0.7904 - val_loss: 0.4565 - val_accuracy: 0.8090\n",
      "Epoch 789/1000\n",
      " - 0s - loss: 0.4773 - accuracy: 0.7862 - val_loss: 0.4575 - val_accuracy: 0.8315\n",
      "Epoch 790/1000\n",
      " - 0s - loss: 0.4637 - accuracy: 0.7961 - val_loss: 0.4554 - val_accuracy: 0.8427\n",
      "Epoch 791/1000\n",
      " - 0s - loss: 0.4543 - accuracy: 0.8017 - val_loss: 0.4547 - val_accuracy: 0.8371\n",
      "Epoch 792/1000\n",
      " - 0s - loss: 0.4746 - accuracy: 0.7918 - val_loss: 0.4562 - val_accuracy: 0.8146\n",
      "Epoch 793/1000\n",
      " - 0s - loss: 0.4569 - accuracy: 0.7989 - val_loss: 0.4582 - val_accuracy: 0.8315\n",
      "Epoch 794/1000\n",
      " - 0s - loss: 0.4701 - accuracy: 0.7989 - val_loss: 0.4626 - val_accuracy: 0.8371\n",
      "Epoch 795/1000\n",
      " - 0s - loss: 0.4573 - accuracy: 0.8017 - val_loss: 0.4611 - val_accuracy: 0.8315\n",
      "Epoch 796/1000\n",
      " - 0s - loss: 0.4856 - accuracy: 0.7778 - val_loss: 0.4575 - val_accuracy: 0.8146\n",
      "Epoch 797/1000\n",
      " - 0s - loss: 0.4813 - accuracy: 0.7932 - val_loss: 0.4569 - val_accuracy: 0.8315\n",
      "Epoch 798/1000\n",
      " - 0s - loss: 0.4661 - accuracy: 0.7764 - val_loss: 0.4569 - val_accuracy: 0.8427\n",
      "Epoch 799/1000\n",
      " - 0s - loss: 0.4751 - accuracy: 0.7904 - val_loss: 0.4575 - val_accuracy: 0.8146\n",
      "Epoch 800/1000\n",
      " - 0s - loss: 0.4750 - accuracy: 0.7722 - val_loss: 0.4605 - val_accuracy: 0.8427\n",
      "Epoch 801/1000\n",
      " - 0s - loss: 0.4589 - accuracy: 0.7947 - val_loss: 0.4581 - val_accuracy: 0.8258\n",
      "Epoch 802/1000\n",
      " - 0s - loss: 0.4521 - accuracy: 0.7961 - val_loss: 0.4567 - val_accuracy: 0.8202\n",
      "Epoch 803/1000\n",
      " - 0s - loss: 0.4670 - accuracy: 0.7918 - val_loss: 0.4546 - val_accuracy: 0.8146\n",
      "Epoch 804/1000\n",
      " - 0s - loss: 0.4892 - accuracy: 0.7862 - val_loss: 0.4749 - val_accuracy: 0.8202\n",
      "Epoch 805/1000\n",
      " - 0s - loss: 0.4986 - accuracy: 0.7750 - val_loss: 0.4585 - val_accuracy: 0.8427\n",
      "Epoch 806/1000\n",
      " - 0s - loss: 0.4926 - accuracy: 0.7820 - val_loss: 0.4579 - val_accuracy: 0.8090\n",
      "Epoch 807/1000\n",
      " - 0s - loss: 0.4872 - accuracy: 0.7792 - val_loss: 0.4567 - val_accuracy: 0.8034\n",
      "Epoch 808/1000\n",
      " - 0s - loss: 0.4836 - accuracy: 0.7834 - val_loss: 0.4544 - val_accuracy: 0.8371\n",
      "Epoch 809/1000\n",
      " - 0s - loss: 0.4754 - accuracy: 0.7862 - val_loss: 0.4620 - val_accuracy: 0.8258\n",
      "Epoch 810/1000\n",
      " - 0s - loss: 0.4943 - accuracy: 0.7792 - val_loss: 0.4566 - val_accuracy: 0.8315\n",
      "Epoch 811/1000\n",
      " - 0s - loss: 0.4844 - accuracy: 0.7693 - val_loss: 0.4572 - val_accuracy: 0.8034\n",
      "Epoch 812/1000\n",
      " - 0s - loss: 0.4678 - accuracy: 0.7989 - val_loss: 0.4603 - val_accuracy: 0.7978\n",
      "Epoch 813/1000\n",
      " - 0s - loss: 0.4614 - accuracy: 0.8031 - val_loss: 0.4566 - val_accuracy: 0.8202\n",
      "Epoch 814/1000\n",
      " - 0s - loss: 0.4713 - accuracy: 0.7947 - val_loss: 0.4574 - val_accuracy: 0.8258\n",
      "Epoch 815/1000\n",
      " - 0s - loss: 0.4699 - accuracy: 0.7932 - val_loss: 0.4563 - val_accuracy: 0.8202\n",
      "Epoch 816/1000\n",
      " - 0s - loss: 0.4827 - accuracy: 0.7750 - val_loss: 0.4555 - val_accuracy: 0.8202\n",
      "Epoch 817/1000\n",
      " - 0s - loss: 0.4675 - accuracy: 0.8003 - val_loss: 0.4609 - val_accuracy: 0.8258\n",
      "Epoch 818/1000\n",
      " - 0s - loss: 0.4724 - accuracy: 0.8059 - val_loss: 0.4554 - val_accuracy: 0.8258\n",
      "Epoch 819/1000\n",
      " - 0s - loss: 0.4763 - accuracy: 0.7834 - val_loss: 0.4559 - val_accuracy: 0.8090\n",
      "Epoch 820/1000\n",
      " - 0s - loss: 0.4611 - accuracy: 0.7862 - val_loss: 0.4517 - val_accuracy: 0.8202\n",
      "Epoch 821/1000\n",
      " - 0s - loss: 0.4708 - accuracy: 0.7947 - val_loss: 0.4510 - val_accuracy: 0.8315\n",
      "Epoch 822/1000\n",
      " - 0s - loss: 0.4851 - accuracy: 0.7820 - val_loss: 0.4568 - val_accuracy: 0.8146\n",
      "Epoch 823/1000\n",
      " - 0s - loss: 0.4647 - accuracy: 0.8115 - val_loss: 0.4570 - val_accuracy: 0.8371\n",
      "Epoch 824/1000\n",
      " - 0s - loss: 0.4783 - accuracy: 0.7792 - val_loss: 0.4556 - val_accuracy: 0.8315\n",
      "Epoch 825/1000\n",
      " - 0s - loss: 0.4625 - accuracy: 0.7975 - val_loss: 0.4547 - val_accuracy: 0.8202\n",
      "Epoch 826/1000\n",
      " - 0s - loss: 0.4778 - accuracy: 0.7918 - val_loss: 0.4597 - val_accuracy: 0.8258\n",
      "Epoch 827/1000\n",
      " - 0s - loss: 0.4849 - accuracy: 0.7778 - val_loss: 0.4542 - val_accuracy: 0.8202\n",
      "Epoch 828/1000\n",
      " - 0s - loss: 0.4638 - accuracy: 0.7932 - val_loss: 0.4558 - val_accuracy: 0.8034\n",
      "Epoch 829/1000\n",
      " - 0s - loss: 0.4655 - accuracy: 0.7820 - val_loss: 0.4559 - val_accuracy: 0.7978\n",
      "Epoch 830/1000\n",
      " - 0s - loss: 0.4761 - accuracy: 0.7975 - val_loss: 0.4530 - val_accuracy: 0.8427\n",
      "Epoch 831/1000\n",
      " - 0s - loss: 0.4867 - accuracy: 0.7750 - val_loss: 0.4524 - val_accuracy: 0.8258\n",
      "Epoch 832/1000\n",
      " - 0s - loss: 0.4557 - accuracy: 0.8017 - val_loss: 0.4500 - val_accuracy: 0.8258\n",
      "Epoch 833/1000\n",
      " - 0s - loss: 0.4814 - accuracy: 0.7848 - val_loss: 0.4526 - val_accuracy: 0.8258\n",
      "Epoch 834/1000\n",
      " - 0s - loss: 0.4806 - accuracy: 0.7722 - val_loss: 0.4587 - val_accuracy: 0.8202\n",
      "Epoch 835/1000\n",
      " - 0s - loss: 0.4846 - accuracy: 0.7890 - val_loss: 0.4590 - val_accuracy: 0.8371\n",
      "Epoch 836/1000\n",
      " - 0s - loss: 0.4741 - accuracy: 0.7918 - val_loss: 0.4598 - val_accuracy: 0.8146\n",
      "Epoch 837/1000\n",
      " - 0s - loss: 0.4814 - accuracy: 0.7834 - val_loss: 0.4583 - val_accuracy: 0.8202\n",
      "Epoch 838/1000\n",
      " - 0s - loss: 0.4730 - accuracy: 0.7806 - val_loss: 0.4566 - val_accuracy: 0.8258\n",
      "Epoch 839/1000\n",
      " - 0s - loss: 0.4496 - accuracy: 0.8158 - val_loss: 0.4548 - val_accuracy: 0.8258\n",
      "Epoch 840/1000\n",
      " - 0s - loss: 0.4542 - accuracy: 0.8087 - val_loss: 0.4551 - val_accuracy: 0.8090\n",
      "Epoch 841/1000\n",
      " - 0s - loss: 0.4518 - accuracy: 0.7989 - val_loss: 0.4572 - val_accuracy: 0.8258\n",
      "Epoch 842/1000\n",
      " - 0s - loss: 0.4699 - accuracy: 0.7862 - val_loss: 0.4585 - val_accuracy: 0.8146\n",
      "Epoch 843/1000\n",
      " - 0s - loss: 0.4731 - accuracy: 0.7820 - val_loss: 0.4596 - val_accuracy: 0.7978\n",
      "Epoch 844/1000\n",
      " - 0s - loss: 0.4475 - accuracy: 0.7947 - val_loss: 0.4570 - val_accuracy: 0.8427\n",
      "Epoch 845/1000\n",
      " - 0s - loss: 0.4674 - accuracy: 0.7876 - val_loss: 0.4599 - val_accuracy: 0.8258\n",
      "Epoch 846/1000\n",
      " - 0s - loss: 0.4678 - accuracy: 0.7890 - val_loss: 0.4619 - val_accuracy: 0.8258\n",
      "Epoch 847/1000\n",
      " - 0s - loss: 0.4625 - accuracy: 0.8045 - val_loss: 0.4627 - val_accuracy: 0.8202\n",
      "Epoch 848/1000\n",
      " - 0s - loss: 0.4653 - accuracy: 0.7848 - val_loss: 0.4604 - val_accuracy: 0.8146\n",
      "Epoch 849/1000\n",
      " - 0s - loss: 0.4595 - accuracy: 0.7890 - val_loss: 0.4562 - val_accuracy: 0.8427\n",
      "Epoch 850/1000\n",
      " - 0s - loss: 0.4811 - accuracy: 0.7904 - val_loss: 0.4548 - val_accuracy: 0.8427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 851/1000\n",
      " - 0s - loss: 0.4773 - accuracy: 0.7862 - val_loss: 0.4554 - val_accuracy: 0.8371\n",
      "Epoch 852/1000\n",
      " - 0s - loss: 0.4375 - accuracy: 0.8129 - val_loss: 0.4512 - val_accuracy: 0.8202\n",
      "Epoch 853/1000\n",
      " - 0s - loss: 0.4842 - accuracy: 0.7806 - val_loss: 0.4509 - val_accuracy: 0.8258\n",
      "Epoch 854/1000\n",
      " - 0s - loss: 0.4832 - accuracy: 0.7862 - val_loss: 0.4540 - val_accuracy: 0.8258\n",
      "Epoch 855/1000\n",
      " - 0s - loss: 0.4754 - accuracy: 0.7806 - val_loss: 0.4603 - val_accuracy: 0.8371\n",
      "Epoch 856/1000\n",
      " - 0s - loss: 0.4773 - accuracy: 0.7876 - val_loss: 0.4601 - val_accuracy: 0.8315\n",
      "Epoch 857/1000\n",
      " - 0s - loss: 0.4767 - accuracy: 0.7722 - val_loss: 0.4604 - val_accuracy: 0.8258\n",
      "Epoch 858/1000\n",
      " - 0s - loss: 0.4707 - accuracy: 0.7961 - val_loss: 0.4664 - val_accuracy: 0.8034\n",
      "Epoch 859/1000\n",
      " - 0s - loss: 0.4643 - accuracy: 0.8017 - val_loss: 0.4647 - val_accuracy: 0.8146\n",
      "Epoch 860/1000\n",
      " - 0s - loss: 0.4447 - accuracy: 0.8172 - val_loss: 0.4592 - val_accuracy: 0.8146\n",
      "Epoch 861/1000\n",
      " - 0s - loss: 0.4622 - accuracy: 0.8003 - val_loss: 0.4631 - val_accuracy: 0.8090\n",
      "Epoch 862/1000\n",
      " - 0s - loss: 0.4700 - accuracy: 0.7862 - val_loss: 0.4628 - val_accuracy: 0.8258\n",
      "Epoch 863/1000\n",
      " - 0s - loss: 0.4825 - accuracy: 0.7876 - val_loss: 0.4598 - val_accuracy: 0.8146\n",
      "Epoch 864/1000\n",
      " - 0s - loss: 0.4745 - accuracy: 0.7876 - val_loss: 0.4580 - val_accuracy: 0.8315\n",
      "Epoch 865/1000\n",
      " - 0s - loss: 0.4836 - accuracy: 0.7834 - val_loss: 0.4604 - val_accuracy: 0.8146\n",
      "Epoch 866/1000\n",
      " - 0s - loss: 0.4625 - accuracy: 0.8003 - val_loss: 0.4622 - val_accuracy: 0.8202\n",
      "Epoch 867/1000\n",
      " - 0s - loss: 0.4827 - accuracy: 0.7764 - val_loss: 0.4590 - val_accuracy: 0.8258\n",
      "Epoch 868/1000\n",
      " - 0s - loss: 0.4651 - accuracy: 0.7947 - val_loss: 0.4612 - val_accuracy: 0.8258\n",
      "Epoch 869/1000\n",
      " - 0s - loss: 0.4712 - accuracy: 0.7975 - val_loss: 0.4618 - val_accuracy: 0.8315\n",
      "Epoch 870/1000\n",
      " - 0s - loss: 0.4624 - accuracy: 0.7947 - val_loss: 0.4623 - val_accuracy: 0.8427\n",
      "Epoch 871/1000\n",
      " - 0s - loss: 0.4779 - accuracy: 0.7904 - val_loss: 0.4598 - val_accuracy: 0.8427\n",
      "Epoch 872/1000\n",
      " - 0s - loss: 0.4660 - accuracy: 0.7947 - val_loss: 0.4580 - val_accuracy: 0.8258\n",
      "Epoch 873/1000\n",
      " - 0s - loss: 0.4612 - accuracy: 0.7862 - val_loss: 0.4575 - val_accuracy: 0.8202\n",
      "Epoch 874/1000\n",
      " - 0s - loss: 0.4882 - accuracy: 0.7778 - val_loss: 0.4557 - val_accuracy: 0.8090\n",
      "Epoch 875/1000\n",
      " - 0s - loss: 0.4894 - accuracy: 0.7820 - val_loss: 0.4546 - val_accuracy: 0.8202\n",
      "Epoch 876/1000\n",
      " - 0s - loss: 0.4722 - accuracy: 0.7989 - val_loss: 0.4599 - val_accuracy: 0.8258\n",
      "Epoch 877/1000\n",
      " - 0s - loss: 0.4915 - accuracy: 0.7764 - val_loss: 0.4561 - val_accuracy: 0.8315\n",
      "Epoch 878/1000\n",
      " - 0s - loss: 0.4828 - accuracy: 0.7862 - val_loss: 0.4552 - val_accuracy: 0.8315\n",
      "Epoch 879/1000\n",
      " - 0s - loss: 0.5043 - accuracy: 0.7750 - val_loss: 0.4567 - val_accuracy: 0.8202\n",
      "Epoch 880/1000\n",
      " - 0s - loss: 0.4720 - accuracy: 0.7862 - val_loss: 0.4580 - val_accuracy: 0.8258\n",
      "Epoch 881/1000\n",
      " - 0s - loss: 0.4850 - accuracy: 0.7862 - val_loss: 0.4609 - val_accuracy: 0.8258\n",
      "Epoch 882/1000\n",
      " - 0s - loss: 0.4675 - accuracy: 0.7961 - val_loss: 0.4601 - val_accuracy: 0.8371\n",
      "Epoch 883/1000\n",
      " - 0s - loss: 0.4801 - accuracy: 0.7890 - val_loss: 0.4575 - val_accuracy: 0.8315\n",
      "Epoch 884/1000\n",
      " - 0s - loss: 0.4467 - accuracy: 0.8242 - val_loss: 0.4559 - val_accuracy: 0.8202\n",
      "Epoch 885/1000\n",
      " - 0s - loss: 0.4781 - accuracy: 0.7806 - val_loss: 0.4563 - val_accuracy: 0.8315\n",
      "Epoch 886/1000\n",
      " - 0s - loss: 0.4685 - accuracy: 0.7820 - val_loss: 0.4579 - val_accuracy: 0.8258\n",
      "Epoch 887/1000\n",
      " - 0s - loss: 0.4587 - accuracy: 0.7947 - val_loss: 0.4573 - val_accuracy: 0.8202\n",
      "Epoch 888/1000\n",
      " - 0s - loss: 0.4702 - accuracy: 0.8059 - val_loss: 0.4558 - val_accuracy: 0.8258\n",
      "Epoch 889/1000\n",
      " - 0s - loss: 0.4862 - accuracy: 0.7862 - val_loss: 0.4557 - val_accuracy: 0.8371\n",
      "Epoch 890/1000\n",
      " - 0s - loss: 0.4633 - accuracy: 0.8059 - val_loss: 0.4552 - val_accuracy: 0.8315\n",
      "Epoch 891/1000\n",
      " - 0s - loss: 0.4654 - accuracy: 0.7820 - val_loss: 0.4533 - val_accuracy: 0.8315\n",
      "Epoch 892/1000\n",
      " - 0s - loss: 0.4642 - accuracy: 0.8017 - val_loss: 0.4534 - val_accuracy: 0.8202\n",
      "Epoch 893/1000\n",
      " - 0s - loss: 0.4809 - accuracy: 0.7764 - val_loss: 0.4518 - val_accuracy: 0.8258\n",
      "Epoch 894/1000\n",
      " - 0s - loss: 0.4370 - accuracy: 0.8186 - val_loss: 0.4518 - val_accuracy: 0.8371\n",
      "Epoch 895/1000\n",
      " - 0s - loss: 0.4913 - accuracy: 0.7736 - val_loss: 0.4532 - val_accuracy: 0.8371\n",
      "Epoch 896/1000\n",
      " - 0s - loss: 0.4642 - accuracy: 0.7989 - val_loss: 0.4548 - val_accuracy: 0.8315\n",
      "Epoch 897/1000\n",
      " - 0s - loss: 0.4794 - accuracy: 0.7918 - val_loss: 0.4567 - val_accuracy: 0.8315\n",
      "Epoch 898/1000\n",
      " - 0s - loss: 0.4607 - accuracy: 0.7947 - val_loss: 0.4600 - val_accuracy: 0.8258\n",
      "Epoch 899/1000\n",
      " - 0s - loss: 0.4796 - accuracy: 0.7975 - val_loss: 0.4642 - val_accuracy: 0.8146\n",
      "Epoch 900/1000\n",
      " - 0s - loss: 0.4660 - accuracy: 0.8003 - val_loss: 0.4651 - val_accuracy: 0.8034\n",
      "Epoch 901/1000\n",
      " - 0s - loss: 0.4628 - accuracy: 0.7904 - val_loss: 0.4589 - val_accuracy: 0.8202\n",
      "Epoch 902/1000\n",
      " - 0s - loss: 0.4527 - accuracy: 0.8003 - val_loss: 0.4581 - val_accuracy: 0.8258\n",
      "Epoch 903/1000\n",
      " - 0s - loss: 0.4583 - accuracy: 0.7961 - val_loss: 0.4625 - val_accuracy: 0.8034\n",
      "Epoch 904/1000\n",
      " - 0s - loss: 0.4762 - accuracy: 0.7834 - val_loss: 0.4680 - val_accuracy: 0.7865\n",
      "Epoch 905/1000\n",
      " - 0s - loss: 0.4701 - accuracy: 0.7947 - val_loss: 0.4593 - val_accuracy: 0.8034\n",
      "Epoch 906/1000\n",
      " - 0s - loss: 0.4928 - accuracy: 0.7736 - val_loss: 0.4556 - val_accuracy: 0.8258\n",
      "Epoch 907/1000\n",
      " - 0s - loss: 0.4766 - accuracy: 0.7876 - val_loss: 0.4567 - val_accuracy: 0.8315\n",
      "Epoch 908/1000\n",
      " - 0s - loss: 0.4584 - accuracy: 0.8101 - val_loss: 0.4592 - val_accuracy: 0.8315\n",
      "Epoch 909/1000\n",
      " - 0s - loss: 0.4961 - accuracy: 0.7722 - val_loss: 0.4618 - val_accuracy: 0.8146\n",
      "Epoch 910/1000\n",
      " - 0s - loss: 0.4751 - accuracy: 0.7989 - val_loss: 0.4616 - val_accuracy: 0.8258\n",
      "Epoch 911/1000\n",
      " - 0s - loss: 0.4733 - accuracy: 0.7876 - val_loss: 0.4608 - val_accuracy: 0.8315\n",
      "Epoch 912/1000\n",
      " - 0s - loss: 0.4658 - accuracy: 0.7848 - val_loss: 0.4608 - val_accuracy: 0.8202\n",
      "Epoch 913/1000\n",
      " - 0s - loss: 0.4642 - accuracy: 0.8045 - val_loss: 0.4638 - val_accuracy: 0.8371\n",
      "Epoch 914/1000\n",
      " - 0s - loss: 0.4791 - accuracy: 0.7918 - val_loss: 0.4625 - val_accuracy: 0.8258\n",
      "Epoch 915/1000\n",
      " - 0s - loss: 0.4750 - accuracy: 0.7947 - val_loss: 0.4617 - val_accuracy: 0.8315\n",
      "Epoch 916/1000\n",
      " - 0s - loss: 0.4741 - accuracy: 0.7961 - val_loss: 0.4611 - val_accuracy: 0.8315\n",
      "Epoch 917/1000\n",
      " - 0s - loss: 0.4777 - accuracy: 0.7876 - val_loss: 0.4618 - val_accuracy: 0.8258\n",
      "Epoch 918/1000\n",
      " - 0s - loss: 0.4791 - accuracy: 0.7806 - val_loss: 0.4587 - val_accuracy: 0.8315\n",
      "Epoch 919/1000\n",
      " - 0s - loss: 0.4517 - accuracy: 0.8059 - val_loss: 0.4624 - val_accuracy: 0.8146\n",
      "Epoch 920/1000\n",
      " - 0s - loss: 0.4458 - accuracy: 0.8143 - val_loss: 0.4586 - val_accuracy: 0.7978\n",
      "Epoch 921/1000\n",
      " - 0s - loss: 0.4685 - accuracy: 0.7890 - val_loss: 0.4631 - val_accuracy: 0.7921\n",
      "Epoch 922/1000\n",
      " - 0s - loss: 0.4648 - accuracy: 0.7918 - val_loss: 0.4530 - val_accuracy: 0.8258\n",
      "Epoch 923/1000\n",
      " - 0s - loss: 0.4681 - accuracy: 0.8045 - val_loss: 0.4581 - val_accuracy: 0.8146\n",
      "Epoch 924/1000\n",
      " - 0s - loss: 0.4656 - accuracy: 0.7961 - val_loss: 0.4581 - val_accuracy: 0.8146\n",
      "Epoch 925/1000\n",
      " - 0s - loss: 0.4698 - accuracy: 0.7989 - val_loss: 0.4544 - val_accuracy: 0.8315\n",
      "Epoch 926/1000\n",
      " - 0s - loss: 0.4732 - accuracy: 0.7932 - val_loss: 0.4566 - val_accuracy: 0.8258\n",
      "Epoch 927/1000\n",
      " - 0s - loss: 0.4617 - accuracy: 0.7961 - val_loss: 0.4570 - val_accuracy: 0.8315\n",
      "Epoch 928/1000\n",
      " - 0s - loss: 0.4795 - accuracy: 0.7904 - val_loss: 0.4567 - val_accuracy: 0.8258\n",
      "Epoch 929/1000\n",
      " - 0s - loss: 0.4635 - accuracy: 0.7890 - val_loss: 0.4598 - val_accuracy: 0.8258\n",
      "Epoch 930/1000\n",
      " - 0s - loss: 0.4591 - accuracy: 0.7947 - val_loss: 0.4578 - val_accuracy: 0.8258\n",
      "Epoch 931/1000\n",
      " - 0s - loss: 0.4769 - accuracy: 0.7834 - val_loss: 0.4597 - val_accuracy: 0.8258\n",
      "Epoch 932/1000\n",
      " - 0s - loss: 0.4674 - accuracy: 0.7806 - val_loss: 0.4593 - val_accuracy: 0.8258\n",
      "Epoch 933/1000\n",
      " - 0s - loss: 0.4580 - accuracy: 0.7947 - val_loss: 0.4681 - val_accuracy: 0.8090\n",
      "Epoch 934/1000\n",
      " - 0s - loss: 0.4827 - accuracy: 0.7778 - val_loss: 0.4546 - val_accuracy: 0.8258\n",
      "Epoch 935/1000\n",
      " - 0s - loss: 0.4881 - accuracy: 0.7736 - val_loss: 0.4647 - val_accuracy: 0.8258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 936/1000\n",
      " - 0s - loss: 0.4693 - accuracy: 0.7876 - val_loss: 0.4596 - val_accuracy: 0.8202\n",
      "Epoch 937/1000\n",
      " - 0s - loss: 0.4813 - accuracy: 0.7918 - val_loss: 0.4597 - val_accuracy: 0.8315\n",
      "Epoch 938/1000\n",
      " - 0s - loss: 0.4670 - accuracy: 0.7876 - val_loss: 0.4591 - val_accuracy: 0.8315\n",
      "Epoch 939/1000\n",
      " - 0s - loss: 0.4734 - accuracy: 0.7862 - val_loss: 0.4620 - val_accuracy: 0.8315\n",
      "Epoch 940/1000\n",
      " - 0s - loss: 0.4873 - accuracy: 0.7792 - val_loss: 0.4617 - val_accuracy: 0.8202\n",
      "Epoch 941/1000\n",
      " - 0s - loss: 0.4843 - accuracy: 0.7792 - val_loss: 0.4599 - val_accuracy: 0.8202\n",
      "Epoch 942/1000\n",
      " - 0s - loss: 0.4718 - accuracy: 0.7989 - val_loss: 0.4556 - val_accuracy: 0.8315\n",
      "Epoch 943/1000\n",
      " - 0s - loss: 0.4788 - accuracy: 0.7848 - val_loss: 0.4566 - val_accuracy: 0.8202\n",
      "Epoch 944/1000\n",
      " - 0s - loss: 0.4610 - accuracy: 0.8059 - val_loss: 0.4526 - val_accuracy: 0.8090\n",
      "Epoch 945/1000\n",
      " - 0s - loss: 0.4749 - accuracy: 0.8017 - val_loss: 0.4499 - val_accuracy: 0.8315\n",
      "Epoch 946/1000\n",
      " - 0s - loss: 0.4664 - accuracy: 0.8003 - val_loss: 0.4492 - val_accuracy: 0.8258\n",
      "Epoch 947/1000\n",
      " - 0s - loss: 0.4804 - accuracy: 0.7989 - val_loss: 0.4504 - val_accuracy: 0.8371\n",
      "Epoch 948/1000\n",
      " - 0s - loss: 0.4943 - accuracy: 0.7750 - val_loss: 0.4576 - val_accuracy: 0.8258\n",
      "Epoch 949/1000\n",
      " - 0s - loss: 0.4584 - accuracy: 0.8003 - val_loss: 0.4595 - val_accuracy: 0.8315\n",
      "Epoch 950/1000\n",
      " - 0s - loss: 0.4733 - accuracy: 0.7904 - val_loss: 0.4567 - val_accuracy: 0.8315\n",
      "Epoch 951/1000\n",
      " - 0s - loss: 0.4609 - accuracy: 0.8003 - val_loss: 0.4534 - val_accuracy: 0.8258\n",
      "Epoch 952/1000\n",
      " - 0s - loss: 0.4886 - accuracy: 0.7820 - val_loss: 0.4543 - val_accuracy: 0.8258\n",
      "Epoch 953/1000\n",
      " - 0s - loss: 0.4591 - accuracy: 0.7947 - val_loss: 0.4532 - val_accuracy: 0.8258\n",
      "Epoch 954/1000\n",
      " - 0s - loss: 0.4536 - accuracy: 0.8003 - val_loss: 0.4540 - val_accuracy: 0.8315\n",
      "Epoch 955/1000\n",
      " - 0s - loss: 0.4703 - accuracy: 0.7862 - val_loss: 0.4573 - val_accuracy: 0.8202\n",
      "Epoch 956/1000\n",
      " - 0s - loss: 0.4984 - accuracy: 0.7764 - val_loss: 0.4565 - val_accuracy: 0.8315\n",
      "Epoch 957/1000\n",
      " - 0s - loss: 0.4522 - accuracy: 0.8228 - val_loss: 0.4582 - val_accuracy: 0.8258\n",
      "Epoch 958/1000\n",
      " - 0s - loss: 0.4724 - accuracy: 0.8059 - val_loss: 0.4542 - val_accuracy: 0.8371\n",
      "Epoch 959/1000\n",
      " - 0s - loss: 0.4765 - accuracy: 0.7876 - val_loss: 0.4524 - val_accuracy: 0.8315\n",
      "Epoch 960/1000\n",
      " - 0s - loss: 0.4499 - accuracy: 0.8101 - val_loss: 0.4505 - val_accuracy: 0.8315\n",
      "Epoch 961/1000\n",
      " - 0s - loss: 0.4615 - accuracy: 0.7848 - val_loss: 0.4523 - val_accuracy: 0.8258\n",
      "Epoch 962/1000\n",
      " - 0s - loss: 0.4610 - accuracy: 0.8003 - val_loss: 0.4510 - val_accuracy: 0.8315\n",
      "Epoch 963/1000\n",
      " - 0s - loss: 0.4551 - accuracy: 0.8073 - val_loss: 0.4547 - val_accuracy: 0.8258\n",
      "Epoch 964/1000\n",
      " - 0s - loss: 0.4697 - accuracy: 0.7890 - val_loss: 0.4506 - val_accuracy: 0.8315\n",
      "Epoch 965/1000\n",
      " - 0s - loss: 0.4594 - accuracy: 0.7989 - val_loss: 0.4530 - val_accuracy: 0.8315\n",
      "Epoch 966/1000\n",
      " - 0s - loss: 0.4624 - accuracy: 0.7862 - val_loss: 0.4510 - val_accuracy: 0.8258\n",
      "Epoch 967/1000\n",
      " - 0s - loss: 0.4691 - accuracy: 0.7918 - val_loss: 0.4541 - val_accuracy: 0.8315\n",
      "Epoch 968/1000\n",
      " - 0s - loss: 0.4689 - accuracy: 0.7932 - val_loss: 0.4553 - val_accuracy: 0.8315\n",
      "Epoch 969/1000\n",
      " - 0s - loss: 0.4627 - accuracy: 0.8003 - val_loss: 0.4559 - val_accuracy: 0.8146\n",
      "Epoch 970/1000\n",
      " - 0s - loss: 0.4858 - accuracy: 0.7792 - val_loss: 0.4558 - val_accuracy: 0.8146\n",
      "Epoch 971/1000\n",
      " - 0s - loss: 0.4610 - accuracy: 0.7932 - val_loss: 0.4545 - val_accuracy: 0.8146\n",
      "Epoch 972/1000\n",
      " - 0s - loss: 0.4626 - accuracy: 0.8073 - val_loss: 0.4552 - val_accuracy: 0.8202\n",
      "Epoch 973/1000\n",
      " - 0s - loss: 0.4713 - accuracy: 0.7750 - val_loss: 0.4559 - val_accuracy: 0.8258\n",
      "Epoch 974/1000\n",
      " - 0s - loss: 0.4722 - accuracy: 0.7890 - val_loss: 0.4573 - val_accuracy: 0.8146\n",
      "Epoch 975/1000\n",
      " - 0s - loss: 0.4738 - accuracy: 0.7890 - val_loss: 0.4570 - val_accuracy: 0.8146\n",
      "Epoch 976/1000\n",
      " - 0s - loss: 0.4732 - accuracy: 0.7918 - val_loss: 0.4582 - val_accuracy: 0.8315\n",
      "Epoch 977/1000\n",
      " - 0s - loss: 0.4893 - accuracy: 0.7876 - val_loss: 0.4554 - val_accuracy: 0.8258\n",
      "Epoch 978/1000\n",
      " - 0s - loss: 0.4682 - accuracy: 0.7961 - val_loss: 0.4536 - val_accuracy: 0.8258\n",
      "Epoch 979/1000\n",
      " - 0s - loss: 0.4842 - accuracy: 0.8003 - val_loss: 0.4577 - val_accuracy: 0.8146\n",
      "Epoch 980/1000\n",
      " - 0s - loss: 0.4783 - accuracy: 0.7848 - val_loss: 0.4566 - val_accuracy: 0.8258\n",
      "Epoch 981/1000\n",
      " - 0s - loss: 0.4681 - accuracy: 0.7820 - val_loss: 0.4557 - val_accuracy: 0.8202\n",
      "Epoch 982/1000\n",
      " - 0s - loss: 0.4725 - accuracy: 0.7961 - val_loss: 0.4620 - val_accuracy: 0.8202\n",
      "Epoch 983/1000\n",
      " - 0s - loss: 0.4573 - accuracy: 0.8073 - val_loss: 0.4531 - val_accuracy: 0.8258\n",
      "Epoch 984/1000\n",
      " - 0s - loss: 0.4767 - accuracy: 0.7890 - val_loss: 0.4545 - val_accuracy: 0.8202\n",
      "Epoch 985/1000\n",
      " - 0s - loss: 0.4837 - accuracy: 0.7806 - val_loss: 0.4542 - val_accuracy: 0.8258\n",
      "Epoch 986/1000\n",
      " - 0s - loss: 0.4664 - accuracy: 0.7932 - val_loss: 0.4560 - val_accuracy: 0.8315\n",
      "Epoch 987/1000\n",
      " - 0s - loss: 0.4842 - accuracy: 0.7904 - val_loss: 0.4564 - val_accuracy: 0.8146\n",
      "Epoch 988/1000\n",
      " - 0s - loss: 0.4709 - accuracy: 0.7989 - val_loss: 0.4537 - val_accuracy: 0.8202\n",
      "Epoch 989/1000\n",
      " - 0s - loss: 0.4661 - accuracy: 0.7989 - val_loss: 0.4536 - val_accuracy: 0.8202\n",
      "Epoch 990/1000\n",
      " - 0s - loss: 0.4648 - accuracy: 0.7975 - val_loss: 0.4560 - val_accuracy: 0.8202\n",
      "Epoch 991/1000\n",
      " - 0s - loss: 0.4446 - accuracy: 0.8172 - val_loss: 0.4573 - val_accuracy: 0.8258\n",
      "Epoch 992/1000\n",
      " - 0s - loss: 0.4597 - accuracy: 0.7947 - val_loss: 0.4569 - val_accuracy: 0.8315\n",
      "Epoch 993/1000\n",
      " - 0s - loss: 0.4676 - accuracy: 0.7904 - val_loss: 0.4572 - val_accuracy: 0.8202\n",
      "Epoch 994/1000\n",
      " - 0s - loss: 0.4668 - accuracy: 0.7876 - val_loss: 0.4610 - val_accuracy: 0.8202\n",
      "Epoch 995/1000\n",
      " - 0s - loss: 0.4652 - accuracy: 0.8017 - val_loss: 0.4554 - val_accuracy: 0.8371\n",
      "Epoch 996/1000\n",
      " - 0s - loss: 0.4711 - accuracy: 0.7932 - val_loss: 0.4528 - val_accuracy: 0.8371\n",
      "Epoch 997/1000\n",
      " - 0s - loss: 0.4635 - accuracy: 0.7989 - val_loss: 0.4574 - val_accuracy: 0.8090\n",
      "Epoch 998/1000\n",
      " - 0s - loss: 0.4662 - accuracy: 0.7806 - val_loss: 0.4517 - val_accuracy: 0.8202\n",
      "Epoch 999/1000\n",
      " - 0s - loss: 0.4408 - accuracy: 0.8073 - val_loss: 0.4517 - val_accuracy: 0.8315\n",
      "Epoch 1000/1000\n",
      " - 0s - loss: 0.4504 - accuracy: 0.8115 - val_loss: 0.4587 - val_accuracy: 0.8090\n"
     ]
    }
   ],
   "source": [
    "# Train neural network\n",
    "history = network.fit(X_train, # Features\n",
    "                      y_train, # Target\n",
    "                      epochs=1000, # Number of epochs\n",
    "                      verbose=2, # Some output\n",
    "                      batch_size=100, # Number of observations per batch\n",
    "                      validation_data=(X_test, y_test)) # Data for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-03T18:56:46.794519Z",
     "start_time": "2020-01-03T18:56:46.786911Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178, 9)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-03T18:56:46.819279Z",
     "start_time": "2020-01-03T18:56:46.798449Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178/178 [==============================] - 0s 48us/step\n"
     ]
    }
   ],
   "source": [
    "score = network.evaluate(X_test, y_test, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-03T18:56:46.832547Z",
     "start_time": "2020-01-03T18:56:46.822677Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "accuracy: 80.90%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n%s: %.2f%%\" % (network.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-03T18:56:46.924874Z",
     "start_time": "2020-01-03T18:56:46.836520Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# calculate predictions\n",
    "predictions = network.predict(X_test)\n",
    "# round predictions\n",
    "rounded = [round(x[0]) for x in predictions]\n",
    "print(rounded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-03T18:56:46.931964Z",
     "start_time": "2020-01-03T18:56:46.927495Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_accuracy', 'loss', 'accuracy'])\n"
     ]
    }
   ],
   "source": [
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-03T18:56:47.407350Z",
     "start_time": "2020-01-03T18:56:46.934444Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get training and test loss histories\n",
    "training_loss = history.history['loss']\n",
    "test_loss = history.history['val_loss']\n",
    "\n",
    "# Create count of the number of epochs\n",
    "epoch_count = range(1, len(training_loss) + 1)\n",
    "\n",
    "# Visualize loss history\n",
    "plt.plot(epoch_count, training_loss, 'r--')\n",
    "plt.plot(epoch_count, test_loss, 'b-')\n",
    "plt.legend(['Training Loss', 'Test Loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://chrisalbon.com/deep_learning/keras/visualize_loss_history/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-03T18:56:47.702383Z",
     "start_time": "2020-01-03T18:56:47.409697Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get training and test accuracy histories\n",
    "training_accuracy = history.history['accuracy']\n",
    "test_accuracy = history.history['val_accuracy']\n",
    "\n",
    "# Create count of the number of epochs\n",
    "epoch_count = range(1, len(training_accuracy) + 1)\n",
    "\n",
    "# Visualize accuracy history\n",
    "plt.plot(epoch_count, training_accuracy, 'r--')\n",
    "plt.plot(epoch_count, test_accuracy, 'b-')\n",
    "plt.legend(['Training Accuracy', 'Test Accuracy'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy Score')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://chrisalbon.com/deep_learning/keras/visualize_performance_history/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-03T18:56:47.744888Z",
     "start_time": "2020-01-03T18:56:47.704850Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# calculate predictions\n",
    "predictions = model.predict(X_test)\n",
    "# round predictions\n",
    "rounded = [round(x[0]) for x in predictions]\n",
    "print(rounded)\n",
    "\n",
    "\n",
    "\n",
    "# # calculate predictions\n",
    "# predictions = model.predict(X)\n",
    "# # round predictions\n",
    "# rounded = [round(x[0]) for x in predictions]\n",
    "# print(rounded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources \n",
    "\n",
    "http://neuralnetworksanddeeplearning.com/\n",
    "    \n",
    "http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/\n",
    "\n",
    "https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://chrisalbon.com/deep_learning/keras/visualize_neural_network_architecture/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
